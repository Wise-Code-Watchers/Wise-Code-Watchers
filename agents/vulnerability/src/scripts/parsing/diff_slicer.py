"""
Diff Slicer - 将PR diff拆分为可审计最小单元
目标：控输入、立边界、降误报
"""

import re
import os
import hashlib
import logging
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path

from ..core.types import (
    AuditUnit, SymbolType, ImpactAnalysis, FileAnalysis, SymbolInfo
)

# 配置日志 - 避免重复handler
logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)
    logger.propagate = False


class DiffSlicer:
    """Diff切片器 - 将PR diff拆分为可审计单元"""

    def __init__(self):
        # 语言检测映射
        self.language_extensions = {
            '.py': 'python',
            '.js': 'javascript',
            '.ts': 'typescript',
            '.jsx': 'javascript',
            '.tsx': 'typescript',
            '.java': 'java',
            '.go': 'go',
            '.rs': 'rust',
            '.c': 'c',
            '.cpp': 'cpp',
            '.h': 'c',
            '.hpp': 'cpp',
            '.cs': 'csharp',
            '.php': 'php',
            '.rb': 'ruby',
            '.swift': 'swift',
            '.kt': 'kotlin',
            '.scala': 'scala'
        }

        # 测试文件模式
        self.test_patterns = [
            r'/test[s]?/',  # test/tests
            r'_test\.',     # _test.py
            r'\.test\.',     # .test.py
            r'/spec[s]?/',   # spec/specs
            r'_spec\.',      # _spec.py
            r'\.spec\.',      # .spec.py
            r'__tests__/',   # Python tests package
        ]

        # 生成文件模式
        self.generated_patterns = [
            r'/node_modules/',
            r'/vendor/',
            r'/target/',
            r'/build/',
            r'/dist/',
            r'/\.git/',
            r'/__pycache__/',
            r'\.min\.js',   # minified JS
            r'\.bundle\.js', # bundled JS
            r'\.css\.map',   # CSS source maps
            r'\.js\.map',    # JS source maps
        ]

    def detect_language(self, file_path: str) -> str:
        """检测文件语言"""
        file_ext = os.path.splitext(file_path)[1].lower()
        return self.language_extensions.get(file_ext, 'unknown')

    def is_test_file(self, file_path: str) -> bool:
        """判断是否为测试文件"""
        file_path_lower = file_path.lower()
        return any(re.search(pattern, file_path_lower) for pattern in self.test_patterns)

    def is_generated_file(self, file_path: str) -> bool:
        """判断是否为生成文件"""
        return any(re.search(pattern, file_path) for pattern in self.generated_patterns)

    def extract_symbol_from_lines(self, lines: List[str], start_line: int) -> SymbolInfo:
        """从代码行中提取符号信息"""
        symbol_info: SymbolInfo = {
            'type': SymbolType.UNKNOWN.value,
            'name': '',
            'signature': '',
            'line_number': start_line,
            'column_number': 1
        }

        # 在给定行附近查找符号定义
        context_lines = min(10, len(lines))
        search_range = max(0, start_line - 5)
        search_end = min(len(lines), start_line + 5)

        for i in range(search_range, search_end):
            line = lines[i].strip()

            # 函数定义模式
            if line.startswith('def '):
                match = re.match(r'def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\(', line)
                if match:
                    symbol_info['type'] = SymbolType.FUNCTION.value
                    symbol_info['name'] = match.group(1)
                    symbol_info['line_number'] = i + 1
                    symbol_info['signature'] = line
                    break

            # 类定义模式
            elif line.startswith('class '):
                match = re.match(r'class\s+([a-zA-Z_][a-zA-Z0-9_]*)', line)
                if match:
                    symbol_info['type'] = SymbolType.CLASS.value
                    symbol_info['name'] = match.group(1)
                    symbol_info['line_number'] = i + 1
                    symbol_info['signature'] = line
                    break

            # 方法定义模式（简单识别）
            elif any(keyword in line for keyword in ['async def ', 'def ']) and '(' in line:
                # 在类中寻找方法
                for keyword in ['async def ', 'def ']:
                    if keyword in line:
                        match = re.match(r'.*' + keyword + r'([a-zA-Z_][a-zA-Z0-9_]*)\s*\(', line)
                        if match:
                            symbol_info['type'] = SymbolType.METHOD.value
                            symbol_info['name'] = match.group(1)
                            symbol_info['line_number'] = i + 1
                            symbol_info['signature'] = line
                            break

            # 配置定义模式
            elif any(keyword in line for keyword in ['CONFIG_', 'Setting', 'config = ', 'settings = ']):
                symbol_info['type'] = SymbolType.CONFIG.value
                symbol_info['name'] = 'config'
                symbol_info['line_number'] = i + 1
                break

        return symbol_info

    def extract_symbol_with_backtrack(self, file_lines: List[str], target_line: int) -> SymbolInfo:
        """
        向上回溯找最近的def/class符号

        Args:
            file_lines: 文件的所有行
            target_line: hunk开始的行号（1-indexed）

        Returns:
            符号信息
        """
        symbol_info: SymbolInfo = {
            'type': SymbolType.UNKNOWN.value,
            'name': '',
            'signature': '',
            'line_number': target_line,
            'column_number': 1
        }

        # 安全检查
        if not file_lines or target_line < 1:
            logger.warning(f"[符号回溯] 无效输入: file_lines={len(file_lines) if file_lines else 0}, target_line={target_line}")
            symbol_info['type'] = 'module'
            symbol_info['name'] = '<module>'
            symbol_info['signature'] = 'module-level code'
            return symbol_info

        # 转换为0-indexed
        target_idx = min(len(file_lines) - 1, max(0, target_line - 1))
        search_start = max(0, target_idx - 200)  # 向上最多回溯200行

        logger.info(f"[符号回溯] 从行{target_line}向上回溯到{search_start+1}, file_lines总数: {len(file_lines)}")

        # 从target行向上搜索
        try:
            for i in range(target_idx, search_start - 1, -1):
                if i >= len(file_lines):
                    logger.warning(f"[符号回溯] 索引越界: i={i}, len(file_lines)={len(file_lines)}")
                    continue  # 防止索引越界
                line = file_lines[i].strip()
                if not line:
                    continue

                # 跳过注释和字符串（简化处理）
                if line.startswith('#') or line.startswith('"""') or line.startswith("'''"):
                    continue

                # 函数定义模式
                if line.startswith('def '):
                    match = re.match(r'def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\(', line)
                    if match:
                        symbol_info['type'] = SymbolType.FUNCTION.value
                        symbol_info['name'] = match.group(1)
                        symbol_info['line_number'] = i + 1
                        symbol_info['signature'] = line
                        logger.info(f"[符号回溯] 找到函数: {match.group(1)} at line {i+1}")
                        return symbol_info

                # 异步函数定义模式
                if line.startswith('async def '):
                    match = re.match(r'async def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\(', line)
                    if match:
                        symbol_info['type'] = SymbolType.FUNCTION.value
                        symbol_info['name'] = match.group(1)
                        symbol_info['line_number'] = i + 1
                        symbol_info['signature'] = line
                        logger.info(f"[符号回溯] 找到异步函数: {match.group(1)} at line {i+1}")
                        return symbol_info

                # 类定义模式
                if line.startswith('class '):
                    match = re.match(r'class\s+([a-zA-Z_][a-zA-Z0-9_]*)', line)
                    if match:
                        symbol_info['type'] = SymbolType.CLASS.value
                        symbol_info['name'] = match.group(1)
                        symbol_info['line_number'] = i + 1
                        symbol_info['signature'] = line
                        logger.info(f"[符号回溯] 找到类: {match.group(1)} at line {i+1}")
                        return symbol_info

                # 方法定定（如果前面已找到类）
                if (line.startswith('def ') or line.startswith('async def ')) and '(' in line:
                    for keyword in ['async def ', 'def ']:
                        if keyword in line:
                            match = re.match(r'.*' + re.escape(keyword) + r'([a-zA-Z_][a-zA-Z0-9_]*)\s*\(', line)
                            if match:
                                symbol_info['type'] = SymbolType.METHOD.value
                                symbol_info['name'] = match.group(1)
                                symbol_info['line_number'] = i + 1
                                symbol_info['signature'] = line
                                logger.info(f"[符号回溯] 找到方法: {match.group(1)} at line {i+1}")
                                return symbol_info

        except Exception as e:
            logger.error(f"[符号回溯] 搜索过程中发生异常: {str(e)}")
            # 如果出错，标记为模块级
            symbol_info['type'] = 'module'
            symbol_info['name'] = '<module>'
            symbol_info['signature'] = 'module-level code (search error)'
            return symbol_info

        # 如果没找到任何符号，标记为模块级
        symbol_info['type'] = 'module'
        symbol_info['name'] = '<module>'
        symbol_info['signature'] = 'module-level code'
        logger.info(f"[符号回溯] 未找到符号，标记为模块级: line {target_line}")

        return symbol_info

    def infer_impact_from_lines(self, lines: List[str]) -> ImpactAnalysis:
        """从代码行推断影响范围 - 只做事实抽取，不做判断"""
        content = '\n'.join(lines)

        return {
            'control_flow': bool(re.search(r'\b(if|for|while|switch|try|catch|finally|break|continue|return)\b', content)),
            'data_flow': bool(re.search(r'[=+\-*/]|\.[a-zA-Z_][a-zA-Z0-9_]*|\->[a-zA-Z_][a-zA-Z0-9_]*', content)),
            'security_surface': bool(re.search(r'\b(http|https|sql|exec|eval|deserialize|open|read|write|delete)\b', content, re.IGNORECASE)),
            'model_prompt': bool(re.search(r'\b(prompt|system|messages|role|user|assistant|chatgpt|gpt|claude)\b', content, re.IGNORECASE)),
            'file_system': bool(re.search(r'\b(open|read|write|delete|mkdir|chdir|os\.path|pathlib)\b', content, re.IGNORECASE)),
            'network_io': bool(re.search(r'\b(requests|urllib|socket|http|https|fetch|axios)\b', content, re.IGNORECASE)),
            'database_access': bool(re.search(r'\b(select|insert|update|delete|create|drop|cursor|execute|query)\b', content, re.IGNORECASE))
        }

    def analyze_file(self, file_path: str, content: str) -> FileAnalysis:
        """分析文件基本信息"""
        lines = content.split('\n')

        return {
            'file_path': file_path,
            'language': self.detect_language(file_path),
            'is_test_file': self.is_test_file(file_path),
            'is_generated': self.is_generated_file(file_path),
            'total_lines': len(lines),
            'has_imports': any('import ' in line for line in lines),
            'has_exports': any(('export ' in line or 'def __export__' in line) for line in lines),
            'complexity_metrics': {
                'functions': len([line for line in lines if re.match(r'^\s*(def\s+|async\s+def\s+)', line)]),
                'classes': len([line for line in lines if re.match(r'^\s*class\s+', line)]),
                'conditionals': len([line for line in lines if re.search(r'\bif\s+', line)]),
                'loops': len([line for line in lines if re.search(r'\b(for\s+|while\s+)', line)])
            }
        }

    def parse_diff_hunk(self, diff_line: str, file_analysis: FileAnalysis) -> Dict[str, Any]:
        """解析单个diff hunk"""
        # 简化的diff解析 - 假设格式相对标准
        if not diff_line.startswith('@@'):
            return None

        # 提取行号信息
        hunk_header_match = re.match(r'@@\s+-(\d+)(?:,(\d+))?\s+\+(\d+)(?:,(\d+))?\s+@@', diff_line)
        if not hunk_header_match:
            return None

        old_start = int(hunk_header_match.group(1))
        new_start = int(hunk_header_match.group(3))
        old_count = int(hunk_header_match.group(2) or 0)
        new_count = int(hunk_header_match.group(4) or 0)

        return {
            'hunk_id': hashlib.md5(diff_line.encode()).hexdigest()[:8],
            'old_start': old_start,
            'new_start': new_start,
            'old_count': old_count,
            'new_count': new_count,
            'line_range': f"{new_start}-{new_start + new_count - 1}" if new_count > 0 else f"{new_start}"
        }

    def slice_diff_content(self, diff_content: str, file_path: str) -> List[AuditUnit]:
        """将单个文件的diff内容切片为审计单元"""
        units: List[AuditUnit] = []
        lines = diff_content.split('\n')
        current_hunk = None
        hunk_lines: List[str] = []

        # 分析文件
        filtered_lines = []
        for line in lines:
            if not line.startswith('diff --git') and not line.startswith('index'):
                if line.startswith('+') or line.startswith('-') or line.startswith(' '):
                    filtered_lines.append(line[1:])
                else:
                    filtered_lines.append(line)

        file_content = '\n'.join(filtered_lines)
        file_analysis = self.analyze_file(file_path, file_content)

        i = 0
        while i < len(lines):
            line = lines[i]

            # 开始新的hunk
            if line.startswith('@@'):
                # 保存前一个hunk
                if current_hunk and hunk_lines:
                    unit = self._create_audit_unit_from_hunk(
                        current_hunk, hunk_lines, file_analysis, file_content
                    )
                    if unit:
                        units.append(unit)

                # 解析新hunk
                current_hunk = self.parse_diff_hunk(line, file_analysis)
                hunk_lines = []

            # 处理hunk内容行
            elif current_hunk and (line.startswith('+') or line.startswith('-') or line.startswith(' ')):
                hunk_lines.append(line)

            i += 1

        # 处理最后一个hunk
        if current_hunk and hunk_lines:
            unit = self._create_audit_unit_from_hunk(current_hunk, hunk_lines, file_analysis, file_content)
            if unit:
                units.append(unit)

        return units

    def _create_audit_unit_from_hunk(self, hunk: Dict[str, Any], hunk_lines: List[str], file_analysis: FileAnalysis, full_file_content: str = None) -> Optional[AuditUnit]:
        """从hunk信息创建审计单元"""
        # 过滤纯上下文行
        code_lines = [line[1:] for line in hunk_lines if line.startswith('+')]
        if not code_lines:
            return None

        # 获取代码内容用于分析
        code_content = '\n'.join(code_lines)

        # 提取符号信息 - 改进：如果有完整文件内容，向上回溯找符号
        if full_file_content:
            file_lines = full_file_content.split('\n')
            new_start = hunk.get('new_start', 1)  # 确保至少是1
            if new_start < 1:
                new_start = 1  # 如果解析失败，默认从第1行开始
            symbol_info = self.extract_symbol_with_backtrack(file_lines, new_start)
        else:
            new_start = hunk.get('new_start', 1)
            if new_start < 1:
                new_start = 1
            symbol_info = self.extract_symbol_from_lines(code_lines, new_start)

        # 推断影响范围
        impact_analysis = self.infer_impact_from_lines(code_lines)

        # 创建审计单元
        audit_unit: AuditUnit = {
            'id': hashlib.md5(f"{hunk['hunk_id']}_{file_analysis['file_path']}".encode()).hexdigest(),
            'file_path': file_analysis['file_path'],
            'language': file_analysis['language'],
            'diff_hunk': code_content,
            'symbol': {
                'type': symbol_info['type'],
                'name': symbol_info['name'],
                'signature': symbol_info['signature'],
                'line_number': symbol_info['line_number'],
                'column_number': symbol_info['column_number']
            },
            'impact': impact_analysis,
            'is_test_code': file_analysis['is_test_file'],
            'is_generated': file_analysis['is_generated'],
            'hunk_id': hunk['hunk_id'],
            'line_start': hunk['new_start'],
            'line_end': hunk['new_start'] + hunk.get('new_count', 0) - 1 if hunk.get('new_count', 0) > 0 else hunk['new_start']
        }

        return audit_unit

    def slice_diff(self, diff_content: str, file_paths: List[str] = None) -> List[AuditUnit]:
        """
        将整个diff切片为审计单元

        Args:
            diff_content: diff内容
            file_paths: 指定文件路径列表（可选，用于限制范围）

        Returns:
            审计单元列表
        """
        logger.info(f"[Diff切片器] 开始处理diff内容，指定文件数: {len(file_paths) if file_paths else '无限制'}")

        all_units: List[AuditUnit] = []

        if not diff_content:
            logger.warning(f"[Diff切片器] diff内容为空")
            return all_units

        lines = diff_content.split('\n')
        current_file = None
        current_diff = []
        file_count = 0

        i = 0
        while i < len(lines):
            line = lines[i]

            # 检测文件开始
            if line.startswith('diff --git'):
                # 保存前一个文件的单元
                if current_file and current_diff:
                    logger.info(f"[Diff切片器] 处理文件: {current_file}")
                    units = self.slice_diff_content('\n'.join(current_diff), current_file)
                    logger.info(f"[Diff切片器] 文件 {current_file} 生成 {len(units)} 个审计单元")
                    all_units.extend(units)
                    file_count += 1

                # 提取文件路径
                parts = line.split()
                if len(parts) >= 4:
                    current_file = parts[3][2:]  # 移除 b/ 前缀
                    logger.info(f"[Diff切片器] 发现文件: {current_file}")

                    # 如果指定了文件路径，检查是否在范围内
                    if file_paths and current_file not in file_paths:
                        logger.info(f"[Diff切片器] 跳过文件 (不在范围内): {current_file}")
                        # 跳过这个文件，直到下一个文件
                        current_file = None
                        current_diff = []
                        while i < len(lines) and not lines[i].startswith('diff --git'):
                            i += 1
                        continue

                current_diff = []

            # 收集diff内容
            elif current_file:
                current_diff.append(line)

            i += 1

        # 处理最后一个文件
        if current_file and current_diff:
            logger.info(f"[Diff切片器] 处理最后文件: {current_file}")
            units = self.slice_diff_content('\n'.join(current_diff), current_file)
            logger.info(f"[Diff切片器] 最后文件 {current_file} 生成 {len(units)} 个审计单元")
            all_units.extend(units)
            file_count += 1

        logger.info(f"[Diff切片器] 处理完成: {file_count} 个文件, 总计 {len(all_units)} 个审计单元")
        return all_units

    def get_units_summary(self, units: List[AuditUnit]) -> Dict[str, Any]:
        """获取审计单元汇总统计"""
        if not units:
            return {
                'total_units': 0,
                'languages': {},
                'impact_distribution': {},
                'symbol_types': {},
                'test_units': 0,
                'generated_units': 0
            }

        languages = {}
        impact_stats = {'control_flow': 0, 'data_flow': 0, 'security_surface': 0, 'model_prompt': 0}
        symbol_stats = {}
        test_count = 0
        generated_count = 0

        for unit in units:
            # 统计语言
            lang = unit.get('language', 'unknown')
            languages[lang] = languages.get(lang, 0) + 1

            # 统计影响
            impact = unit.get('impact', {})
            for key in impact_stats:
                if impact.get(key, False):
                    impact_stats[key] += 1

            # 统计符号类型
            symbol_type = unit.get('symbol', {}).get('type', 'unknown')
            symbol_stats[symbol_type] = symbol_stats.get(symbol_type, 0) + 1

            # 统计测试和生成文件
            if unit.get('is_test_code', False):
                test_count += 1
            if unit.get('is_generated', False):
                generated_count += 1

        return {
            'total_units': len(units),
            'languages': languages,
            'impact_distribution': impact_stats,
            'symbol_types': symbol_stats,
            'test_units': test_count,
            'generated_units': generated_count
        }