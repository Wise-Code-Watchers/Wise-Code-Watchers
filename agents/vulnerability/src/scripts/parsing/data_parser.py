"""
数据解析模块 - 对应1.py功能
解析PR metadata和diff文件，生成结构化数据
"""

import argparse
import json
import os
import re
from dataclasses import dataclass, asdict
from typing import Any, Dict, List, Optional, Tuple


def deep_get(obj: Any, path: str, default=None):
    """
    Safely get nested values from dict using dot-path, e.g. "base.sha".
    Returns default if missing.
    """
    cur = obj
    for part in path.split("."):
        if isinstance(cur, dict) and part in cur:
            cur = cur[part]
        else:
            return default
    return cur


def parse_metadata(metadata_path: str) -> Dict[str, Any]:
    """解析PR metadata.json"""
    with open(metadata_path, "r", encoding="utf-8") as f:
        raw = json.load(f)

    # Try to normalize common GitHub/GitLab-ish fields
    parsed = {
        "source_file": os.path.basename(metadata_path),
        "extracted": {
            "number": deep_get(raw, "number", deep_get(raw, "pull_request.number")),
            "title": deep_get(raw, "title", deep_get(raw, "pull_request.title")),
            "body": deep_get(raw, "body", deep_get(raw, "pull_request.body")),
            "state": deep_get(raw, "state", deep_get(raw, "pull_request.state")),
            "html_url": deep_get(raw, "html_url", deep_get(raw, "pull_request.html_url")),
            "user": deep_get(raw, "user.login", deep_get(raw, "author.username", deep_get(raw, "pull_request.user.login"))),
            "created_at": deep_get(raw, "created_at", deep_get(raw, "pull_request.created_at")),
            "updated_at": deep_get(raw, "updated_at", deep_get(raw, "pull_request.updated_at")),
            "base_ref": deep_get(raw, "base.ref", deep_get(raw, "pull_request.base.ref")),
            "base_sha": deep_get(raw, "base.sha", deep_get(raw, "pull_request.base.sha")),
            "head_ref": deep_get(raw, "head.ref", deep_get(raw, "pull_request.head.ref")),
            "head_sha": deep_get(raw, "head.sha", deep_get(raw, "pull_request.head.sha")),
            "repo_full_name": deep_get(raw, "base.repo.full_name", deep_get(raw, "repository.full_name", deep_get(raw, "pull_request.base.repo.full_name"))),
        },
        "raw": raw,
    }

    return parsed


# Diff parsing
HUNK_RE = re.compile(r"^@@\s*-(\d+)(?:,(\d+))?\s+\+(\d+)(?:,(\d+))?\s*@@(?:\s*(.*))?$")

def normalize_diff_path(p: str) -> str:
    """
    Normalize 'a/foo' or 'b/foo' -> 'foo'. Keep special '/dev/null' as is.
    """
    if p == "/dev/null":
        return p
    if p.startswith("a/") or p.startswith("b/"):
        return p[2:]
    return p

def guess_language(file_path: str) -> str:
    ext = os.path.splitext(file_path)[1].lower()
    mapping = {
        ".py": "Python",
        ".go": "Go",
        ".js": "JavaScript",
        ".ts": "TypeScript",
        ".tsx": "TypeScript",
        ".jsx": "JavaScript",
        ".java": "Java",
        ".rs": "Rust",
        ".c": "C",
        ".cc": "C++",
        ".cpp": "C++",
        ".h": "C/C++ Header",
        ".hpp": "C++ Header",
        ".cs": "C#",
        ".php": "PHP",
        ".rb": "Ruby",
        ".kt": "Kotlin",
        ".swift": "Swift",
        ".scala": "Scala",
        ".sql": "SQL",
        ".yaml": "YAML",
        ".yml": "YAML",
        ".json": "JSON",
        ".toml": "TOML",
        ".md": "Markdown",
        ".sh": "Shell",
        ".ps1": "PowerShell",
        ".dockerfile": "Docker",
    }
    if file_path.endswith("Dockerfile") or os.path.basename(file_path).lower() == "dockerfile":
        return "Docker"
    return mapping.get(ext, "Unknown")


@dataclass
class DiffLine:
    type: str  # "add" | "del" | "context"
    content: str
    old_lineno: Optional[int] = None
    new_lineno: Optional[int] = None


@dataclass
class Hunk:
    old_start: int
    old_count: int
    new_start: int
    new_count: int
    header: str
    lines: List[DiffLine]


@dataclass
class FileDiff:
    file_path: str
    old_path: str
    new_path: str
    change_type: str  # "modified" | "added" | "deleted" | "renamed" | "unknown"
    hunks: List[Hunk]
    additions: int
    deletions: int
    language: str
    is_binary: bool = False
    rename_from: Optional[str] = None
    rename_to: Optional[str] = None


def parse_unified_diff(diff_text: str) -> Dict[str, Any]:
    """解析PR diff文件"""
    lines = diff_text.splitlines()

    file_diffs: List[FileDiff] = []
    cur_file: Optional[FileDiff] = None
    cur_hunk: Optional[Hunk] = None

    # For line number tracking inside hunks
    old_ln = None
    new_ln = None

    def flush_hunk():
        nonlocal cur_file, cur_hunk
        if cur_file and cur_hunk:
            cur_file.hunks.append(cur_hunk)
            cur_hunk = None

    def flush_file():
        nonlocal cur_file, cur_hunk
        flush_hunk()
        if cur_file:
            file_diffs.append(cur_file)
            cur_file = None

    i = 0
    while i < len(lines):
        s = lines[i]

        # Start of a file diff
        if s.startswith("diff --git "):
            flush_file()
            # Example: diff --git a/foo b/foo
            parts = s.split()
            old_path = normalize_diff_path(parts[2]) if len(parts) >= 4 else ""
            new_path = normalize_diff_path(parts[3]) if len(parts) >= 4 else ""
            cur_file = FileDiff(
                file_path=new_path if new_path != "/dev/null" else old_path,
                old_path=old_path,
                new_path=new_path,
                change_type="unknown",
                hunks=[],
                additions=0,
                deletions=0,
                language=guess_language(new_path if new_path != "/dev/null" else old_path),
                is_binary=False,
            )
            old_ln, new_ln = None, None
            i += 1
            continue

        if cur_file is None:
            i += 1
            continue

        # Detect rename / new / delete
        if s.startswith("new file mode"):
            cur_file.change_type = "added"
        elif s.startswith("deleted file mode"):
            cur_file.change_type = "deleted"
        elif s.startswith("rename from "):
            cur_file.change_type = "renamed"
            cur_file.rename_from = normalize_diff_path(s[len("rename from "):].strip())
        elif s.startswith("rename to "):
            cur_file.change_type = "renamed"
            cur_file.rename_to = normalize_diff_path(s[len("rename to "):].strip())
        elif s.startswith("Binary files "):
            cur_file.is_binary = True

        # Old/New file markers
        if s.startswith("--- "):
            p = s[4:].strip()
            cur_file.old_path = normalize_diff_path(p)
            if cur_file.new_path == "":
                cur_file.file_path = cur_file.old_path
                cur_file.language = guess_language(cur_file.file_path)
            i += 1
            continue

        if s.startswith("+++ "):
            p = s[4:].strip()
            cur_file.new_path = normalize_diff_path(p)
            cur_file.file_path = cur_file.new_path if cur_file.new_path != "/dev/null" else cur_file.old_path
            cur_file.language = guess_language(cur_file.file_path)
            if cur_file.change_type == "unknown":
                # If one side is /dev/null, infer add/delete
                if cur_file.old_path == "/dev/null":
                    cur_file.change_type = "added"
                elif cur_file.new_path == "/dev/null":
                    cur_file.change_type = "deleted"
                else:
                    cur_file.change_type = "modified"
            i += 1
            continue

        # Hunk header
        m = HUNK_RE.match(s)
        if m:
            flush_hunk()
            old_start = int(m.group(1))
            old_count = int(m.group(2) or "1")
            new_start = int(m.group(3))
            new_count = int(m.group(4) or "1")
            header = (m.group(5) or "").strip()

            cur_hunk = Hunk(
                old_start=old_start,
                old_count=old_count,
                new_start=new_start,
                new_count=new_count,
                header=header,
                lines=[],
            )
            old_ln = old_start
            new_ln = new_start
            i += 1
            continue

        # Lines inside hunks
        if cur_hunk is not None:
            # Note: do not count \ No newline at end of file
            if s.startswith("\\"):
                i += 1
                continue

            if s.startswith("+") and not s.startswith("+++"):
                cur_file.additions += 1
                cur_hunk.lines.append(DiffLine(type="add", content=s[1:], old_lineno=None, new_lineno=new_ln))
                new_ln = (new_ln + 1) if new_ln is not None else None

            elif s.startswith("-") and not s.startswith("---"):
                cur_file.deletions += 1
                cur_hunk.lines.append(DiffLine(type="del", content=s[1:], old_lineno=old_ln, new_lineno=None))
                old_ln = (old_ln + 1) if old_ln is not None else None

            else:
                # context line (usually starts with space, but be tolerant)
                content = s[1:] if s.startswith(" ") else s
                cur_hunk.lines.append(DiffLine(type="context", content=content, old_lineno=old_ln, new_lineno=new_ln))
                old_ln = (old_ln + 1) if old_ln is not None else None
                new_ln = (new_ln + 1) if new_ln is not None else None

            i += 1
            continue

        i += 1

    flush_file()

    total_add = sum(fd.additions for fd in file_diffs)
    total_del = sum(fd.deletions for fd in file_diffs)
    lang_stats: Dict[str, int] = {}
    for fd in file_diffs:
        lang_stats[fd.language] = lang_stats.get(fd.language, 0) + 1

    result = {
        "summary": {
            "files": len(file_diffs),
            "total_additions": total_add,
            "total_deletions": total_del,
            "languages_by_filecount": dict(sorted(lang_stats.items(), key=lambda x: (-x[1], x[0]))),
        },
        "files": [asdict(fd) for fd in file_diffs],
    }
    return result


def ensure_dir(p: str):
    """确保目录存在"""
    os.makedirs(p, exist_ok=True)


def parse_pr_data(pr_dir: str, out_dir: Optional[str] = None) -> Dict[str, str]:
    """
    解析PR数据的主入口

    Args:
        pr_dir: PR数据目录
        out_dir: 输出目录，默认为pr_dir/out

    Returns:
        包含输出文件路径的字典
    """
    base_dir = os.path.abspath(pr_dir)
    out_dir = os.path.abspath(out_dir) if out_dir else os.path.join(base_dir, "out")
    ensure_dir(out_dir)

    metadata_path = os.path.join(base_dir, "metadata.json")
    diff_path = os.path.join(base_dir, "pr.diff")

    if not os.path.exists(metadata_path):
        raise FileNotFoundError(f"Missing metadata.json at: {metadata_path}")
    if not os.path.exists(diff_path):
        raise FileNotFoundError(f"Missing pr.diff at: {diff_path}")

    # Parse metadata
    meta = parse_metadata(metadata_path)
    meta_out = os.path.join(out_dir, "pr_meta_parsed.json")
    with open(meta_out, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

    # Parse diff
    with open(diff_path, "r", encoding="utf-8", errors="replace") as f:
        diff_text = f.read()
    diff_ir = parse_unified_diff(diff_text)
    diff_out = os.path.join(out_dir, "diff_ir.json")
    with open(diff_out, "w", encoding="utf-8") as f:
        json.dump(diff_ir, f, ensure_ascii=False, indent=2)

    return {
        "pr_meta_parsed": meta_out,
        "diff_ir": diff_out
    }