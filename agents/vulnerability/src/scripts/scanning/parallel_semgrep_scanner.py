"""å¹¶è¡Œ Semgrep æ‰«æå™¨

æ”¯æŒå¹¶å‘æ‰§è¡Œå¤šä¸ªæ‰«æä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡ç‹¬ç«‹è¿è¡Œå¹¶ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ã€‚
æ‰€æœ‰ä»»åŠ¡å®Œæˆååˆå¹¶ç»“æœã€‚

ä¼˜åŠ¿ï¼š
- å¹¶è¡Œæ‰§è¡Œï¼Œå¤§å¹…æå‡æ‰«æé€Ÿåº¦
- ä»»åŠ¡éš”ç¦»ï¼Œå•ä¸ªä»»åŠ¡å¤±è´¥ä¸å½±å“å…¶ä»–ä»»åŠ¡
- å®æ—¶ä¿å­˜ä¸´æ—¶ç»“æœï¼Œé¿å…è¿›åº¦ä¸¢å¤±
- è‡ªåŠ¨èµ„æºç®¡ç†å’Œæ¸…ç†
"""

from __future__ import annotations

import json
import os
import tempfile
import uuid
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from .template_semgrep_scanner import _build_config_from_tasks


@dataclass
class ScanTask:
    """æ‰«æä»»åŠ¡å®šä¹‰"""
    task_id: int
    template: str
    params: Dict[str, Any]
    feature_name: str
    reason: str
    priority: str = "medium"


@dataclass
class TaskResult:
    """å•ä¸ªä»»åŠ¡çš„æ‰«æç»“æœ"""
    task_id: int
    success: bool
    template: str
    feature_name: str
    findings_count: int
    evidence: List[Dict[str, Any]]
    raw_results: Dict[str, Any]
    grouped_results: Dict[str, Any]
    error: Optional[str] = None
    execution_time: float = 0.0


def _get_timeout_for_template(template: str, default_timeout: int = 60) -> int:
    """æ ¹æ®æ¨¡æ¿ç±»å‹è¿”å›åˆé€‚çš„è¶…æ—¶æ—¶é—´"""
    complex_templates = {
        "PY_UPSTREAM_CALLGRAPH": 180,
        "PY_ARGUMENT_SOURCES_AT_CALLSITE": 180,
    }
    return complex_templates.get(template, default_timeout)


def _run_single_task(
    task: ScanTask,
    pr_dir: str,
    tmp_dir: Path,
    timeout_seconds: int = 60,
    max_findings: int = 300
) -> TaskResult:
    """æ‰§è¡Œå•ä¸ªæ‰«æä»»åŠ¡ï¼Œä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶"""

    task_tmp_dir = tmp_dir / f"task_{task.task_id}"
    task_tmp_dir.mkdir(parents=True, exist_ok=True)

    actual_timeout = _get_timeout_for_template(task.template, timeout_seconds)

    start_time = datetime.now()

    try:
        task_dict = {
            "template": task.template,
            "params": task.params,
        }

        cfg_text = _build_config_from_tasks([task_dict])

        with tempfile.NamedTemporaryFile("w", suffix=".yaml", delete=False, encoding="utf-8") as tmp:
            tmp.write(cfg_text)
            tmp_path = tmp.name

        try:
            import subprocess
            cmd = [
                "semgrep",
                f"--config={tmp_path}",
                "--json",
                "--quiet",
                f"--timeout={max(30, actual_timeout // 3)}",
                pr_dir,
            ]

            try:
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=actual_timeout
                )
            except subprocess.TimeoutExpired:
                _kill_semgrep_processes(tmp_path)

                return TaskResult(
                    task_id=task.task_id,
                    success=False,
                    template=task.template,
                    feature_name=task.feature_name,
                    findings_count=0,
                    evidence=[],
                    raw_results={},
                    grouped_results={},
                    error=f"Timeout after {actual_timeout}s",
                    execution_time=(datetime.now() - start_time).total_seconds()
                )

            if result.returncode not in [0, 1]:
                return TaskResult(
                    task_id=task.task_id,
                    success=False,
                    template=task.template,
                    feature_name=task.feature_name,
                    findings_count=0,
                    evidence=[],
                    raw_results={},
                    grouped_results={},
                    error=f"Semgrep failed: {result.stderr}",
                    execution_time=(datetime.now() - start_time).total_seconds()
                )

            data = json.loads(result.stdout) if result.stdout else {"results": []}
            results = data.get("results", []) or []

            if max_findings and isinstance(max_findings, int) and max_findings > 0:
                results = results[:max_findings]

            raw_file = task_tmp_dir / "raw_results.json"
            with open(raw_file, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

            evidence = []
            for it in results:
                evidence.append({
                    "engine": "semgrep_template",
                    "rule_id": it.get("check_id", ""),
                    "severity": (it.get("extra", {}).get("severity") or "INFO"),
                    "file_path": it.get("path", ""),
                    "line_start": it.get("start", {}).get("line", 0),
                    "line_end": it.get("end", {}).get("line", 0),
                    "message": it.get("extra", {}).get("message") or it.get("message", ""),
                    "snippet": it.get("extra", {}).get("lines", ""),
                    "metavars": it.get("extra", {}).get("metavars", {}),
                })

            grouped_results = {
                "features": [{
                    "feature_id": f"temp_{task.task_id}",
                    "feature_name": task.feature_name,
                    "risk_level": "unknown",
                    "total_tasks": 1,
                    "tasks_with_results": 1 if len(evidence) > 0 else 0,
                    "total_findings": len(evidence),
                    "tasks": [{
                        'task_id': task.task_id,
                        'template': task.template,
                        'priority': task.priority,
                        'params': task.params,
                        'reason': task.reason,
                        'findings': evidence,
                        'finding_count': len(evidence)
                    }]
                }]
            }

            grouped_file = task_tmp_dir / "grouped_results.json"
            with open(grouped_file, "w", encoding="utf-8") as f:
                json.dump(grouped_results, f, indent=2, ensure_ascii=False)

            execution_time = (datetime.now() - start_time).total_seconds()

            return TaskResult(
                task_id=task.task_id,
                success=True,
                template=task.template,
                feature_name=task.feature_name,
                findings_count=len(evidence),
                evidence=evidence,
                raw_results=data,
                grouped_results=grouped_results,
                execution_time=execution_time
            )

        finally:
            try:
                os.remove(tmp_path)
            except Exception:
                pass

    except Exception as e:
        import traceback
        return TaskResult(
            task_id=task.task_id,
            success=False,
            template=task.template,
            feature_name=task.feature_name,
            findings_count=0,
            evidence=[],
            raw_results={},
            grouped_results={},
            error=f"{str(e)}\n{traceback.format_exc()}",
            execution_time=(datetime.now() - start_time).total_seconds()
        )


def _kill_semgrep_processes(config_path: str) -> None:
    """æ€æ­»ä¸æŒ‡å®šé…ç½®æ–‡ä»¶ç›¸å…³çš„ semgrep è¿›ç¨‹"""
    try:
        import psutil
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                if proc.info['name'] and 'semgrep' in proc.info['name'].lower():
                    cmdline = proc.info['cmdline'] or []
                    if any(config_path in arg for arg in cmdline):
                        proc.kill()
            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
                pass
    except ImportError:
        import subprocess
        try:
            subprocess.run(['pkill', '-9', '-f', f'semgrep.*{config_path}'],
                          capture_output=True, timeout=5)
        except:
            pass


def run_parallel_semgrep_tasks(
    pr_dir: str,
    feature_risk_plan: Dict[str, Any],
    max_workers: int = 4,
    max_findings: int = 300,
    timeout_seconds: int = 60,
    output_dir: Optional[str] = None
) -> Dict[str, Any]:
    """å¹¶è¡Œæ‰§è¡Œ Semgrep æ‰«æä»»åŠ¡

    Args:
        pr_dir: æºä»£ç ç›®å½•
        feature_risk_plan: åŠŸèƒ½é£é™©è®¡åˆ’ï¼ˆåŒ…å«æ‰«æä»»åŠ¡ï¼‰
        max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
        max_findings: æ¯ä¸ªä»»åŠ¡æœ€å¤§å‘ç°æ•°
        timeout_seconds: æ¯ä¸ªä»»åŠ¡è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        output_dir: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä¸º pr_dir/out/tmp_parallelï¼‰

    Returns:
        {
            "success": bool,
            "summary": {...},
            "evidence": [...],
            "task_results": [...],
            "output_files": {...}
        }
    """

    # 1. å‡†å¤‡è¾“å‡ºç›®å½•
    # output_dir: å¦‚æœä¼ å…¥ï¼Œåˆ™æ˜¯ PR å¯¼å‡ºç›®å½•çš„ out æ–‡ä»¶å¤¹
    #            å¦‚æœä¸º Noneï¼Œåˆ™ä½¿ç”¨ pr_dir çš„ out æ–‡ä»¶å¤¹
    if output_dir is None:
        pr_path = Path(pr_dir)
        final_output_dir = pr_path / "out"
    else:
        final_output_dir = Path(output_dir)

    final_output_dir.mkdir(parents=True, exist_ok=True)

    # ä¸´æ—¶æ–‡ä»¶ä¿å­˜åœ¨ out/tmp_parallel/ ä¸‹
    tmp_base = final_output_dir / "tmp_parallel"
    tmp_base.mkdir(parents=True, exist_ok=True)

    # ä¸ºæ­¤æ¬¡æ‰«æåˆ›å»ºå”¯ä¸€å­ç›®å½•
    scan_id = f"scan_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
    tmp_dir = tmp_base / scan_id
    tmp_dir.mkdir(parents=True, exist_ok=True)

    # 2. æå–æ‰€æœ‰æ‰«æä»»åŠ¡
    tasks = []
    task_idx = 0
    for feature in feature_risk_plan.get("features", []) or []:
        feature_name = feature.get("feature_name", "")
        for scan_task in feature.get("scan_tasks", []) or []:
            task_idx += 1
            tasks.append(ScanTask(
                task_id=task_idx,
                template=scan_task.get("template", ""),
                params=scan_task.get("params", {}) or {},
                feature_name=feature_name,
                reason=scan_task.get("reason", ""),
                priority=scan_task.get("priority", "medium")
            ))

    if not tasks:
        return {
            "success": True,
            "summary": {
                "total_tasks": 0,
                "successful_tasks": 0,
                "failed_tasks": 0,
                "total_findings": 0,
                "execution_time": 0.0
            },
            "evidence": [],
            "task_results": [],
            "output_files": {}
        }

    print(f"\n{'='*80}")
    print(f"ğŸš€ å¹¶è¡Œæ‰«æå¯åŠ¨")
    print(f"{'='*80}")
    print(f"æ€»ä»»åŠ¡æ•°: {len(tasks)}")
    print(f"å¹¶è¡Œå·¥ä½œçº¿ç¨‹: {max_workers}")
    print(f"è¶…æ—¶è®¾ç½®: {timeout_seconds}s/ä»»åŠ¡")
    print(f"ä¸´æ—¶ç›®å½•: {tmp_dir}")

    # 3. å¹¶è¡Œæ‰§è¡Œä»»åŠ¡
    task_results: List[TaskResult] = []
    start_time = datetime.now()

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_task = {
            executor.submit(
                _run_single_task,
                task,
                pr_dir,
                tmp_dir,
                timeout_seconds,
                max_findings
            ): task for task in tasks
        }

        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_task):
            task = future_to_task[future]
            try:
                result = future.result()
                task_results.append(result)

                status = "âœ…" if result.success else "âŒ"
                findings = result.findings_count
                time_str = f"{result.execution_time:.1f}s"

                print(f"{status} [{task.task_id}/{len(tasks)}] {task.template} "
                      f"({task.feature_name}) - {findings} findings, {time_str}")

                if not result.success and result.error:
                    print(f"   é”™è¯¯: {result.error[:100]}")

            except Exception as exc:
                print(f"âŒ [{task.task_id}/{len(tasks)}] {task.template} å¼‚å¸¸: {exc}")
                task_results.append(TaskResult(
                    task_id=task.task_id,
                    success=False,
                    template=task.template,
                    feature_name=task.feature_name,
                    findings_count=0,
                    evidence=[],
                    raw_results={},
                    grouped_results={},
                    error=str(exc),
                    execution_time=0.0
                ))

    execution_time = (datetime.now() - start_time).total_seconds()

    # 4. åˆå¹¶ç»“æœ
    print(f"\n{'='*80}")
    print(f"ğŸ“Š åˆå¹¶ç»“æœ...")
    print(f"{'='*80}")

    all_evidence = []
    all_grouped_features = []
    successful_count = 0
    failed_count = 0
    total_findings = 0

    for result in task_results:
        if result.success:
            successful_count += 1
            all_evidence.extend(result.evidence)
            total_findings += result.findings_count

            # åˆå¹¶åˆ†ç»„ç»“æœ
            if result.grouped_results.get("features"):
                all_grouped_features.extend(result.grouped_results["features"])
        else:
            failed_count += 1

    # 5. ä¿å­˜åˆå¹¶åçš„ç»“æœ
    scan_metadata = {
        "scan_id": scan_id,
        "scan_time": datetime.now().isoformat(),
        "execution_time": execution_time,
        "total_tasks": len(tasks),
        "successful_tasks": successful_count,
        "failed_tasks": failed_count,
        "total_findings": total_findings,
        "max_workers": max_workers,
        "timeout_seconds": timeout_seconds
    }

    # ä¿å­˜æœ€ç»ˆæ±‡æ€»ç»“æœ
    merged_grouped = {
        "scan_metadata": scan_metadata,
        "features": all_grouped_features
    }

    final_grouped_file = tmp_dir / "final_grouped_results.json"
    with open(final_grouped_file, "w", encoding="utf-8") as f:
        json.dump(merged_grouped, f, indent=2, ensure_ascii=False)

    # ä¿å­˜åŸå§‹æ±‡æ€»ç»“æœ
    merged_raw = {
        "scan_metadata": scan_metadata,
        "task_results": [asdict(r) for r in task_results]
    }

    final_raw_file = tmp_dir / "final_raw_results.json"
    with open(final_raw_file, "w", encoding="utf-8") as f:
        json.dump(merged_raw, f, indent=2, ensure_ascii=False)

    # ä¿å­˜è¯æ®åˆ—è¡¨
    evidence_file = tmp_dir / "all_evidence.json"
    with open(evidence_file, "w", encoding="utf-8") as f:
        json.dump(all_evidence, f, indent=2, ensure_ascii=False)

    # 6. å¤åˆ¶åˆ°æœ€ç»ˆè¾“å‡ºä½ç½®
    # æœ€ç»ˆæ–‡ä»¶ä¿å­˜åœ¨ final_output_dirï¼ˆPR å¯¼å‡ºç›®å½•çš„ out æ–‡ä»¶å¤¹ï¼‰
    final_parsed_target = final_output_dir / "semgrep_template_parsed_results.json"
    final_raw_target = final_output_dir / "semgrep_template_raw_results.json"

    import shutil
    shutil.copy(final_grouped_file, final_parsed_target)
    shutil.copy(final_raw_file, final_raw_target)

    print(f"\n{'='*80}")
    print(f"âœ… å¹¶è¡Œæ‰«æå®Œæˆ")
    print(f"{'='*80}")
    print(f"æ€»ä»»åŠ¡æ•°: {len(tasks)}")
    print(f"æˆåŠŸ: {successful_count}")
    print(f"å¤±è´¥: {failed_count}")
    print(f"æ€»å‘ç°: {total_findings}")
    print(f"æ€»è€—æ—¶: {execution_time:.1f}s")
    print(f"å¹³å‡æ¯ä»»åŠ¡: {execution_time/len(tasks):.1f}s")
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"   åˆ†ç±»ç»“æœ: {final_parsed_target}")
    print(f"   åŸå§‹ç»“æœ: {final_raw_target}")
    print(f"   ä¸´æ—¶ç›®å½•: {tmp_dir}")

    return {
        "success": failed_count == 0,
        "summary": scan_metadata,
        "evidence": all_evidence,
        "task_results": [asdict(r) for r in task_results],
        "grouped_results": merged_grouped,
        "output_files": {
            "parsed_results": str(final_parsed_target),
            "raw_results": str(final_raw_target),
            "tmp_dir": str(tmp_dir)
        }
    }
