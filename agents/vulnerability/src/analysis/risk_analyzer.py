"""
风险分析模块 - 对应2.py和3.py功能
基于LangGraph的风险评分和功能分类系统
"""

import json
import os
import re
import subprocess
from typing import Any, Dict, List, Optional, TypedDict, Annotated

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage
from langchain_openai import ChatOpenAI


class RiskState(TypedDict, total=False):
    messages: Annotated[List[BaseMessage], add_messages]

    pr_dir: str
    diff_ir_path: str
    top_n: int
    batch_size: int

    diff_ir: Dict[str, Any]
    pr_hunks: List[Dict[str, Any]]
    batches: List[List[Dict[str, Any]]]
    partial_items: List[Dict[str, Any]]

    final_output: Dict[str, Any]
    output_path: str


def _load_json(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def _hunk_to_text(hunk: Dict[str, Any], max_lines: int = 160) -> str:
    """将hunk转换为文本格式"""
    lines = hunk.get("lines", [])
    if len(lines) > max_lines:
        lines = lines[:max_lines] + [
            {
                "type": "context",
                "content": f"...(truncated, total_lines={len(hunk.get('lines', []))})",
                "old_lineno": None,
                "new_lineno": None,
            }
        ]

    out = []
    for ln in lines:
        t = ln.get("type")
        content = ln.get("content", "")
        old_no = ln.get("old_lineno")
        new_no = ln.get("new_lineno")

        if t == "add":
            prefix = "+"
        elif t == "del":
            prefix = "-"
        else:
            prefix = " "

        out.append(f"{prefix} (old:{old_no}, new:{new_no}) {content}")
    return "\n".join(out)


def _build_hunks(diff_ir: Dict[str, Any]) -> List[Dict[str, Any]]:
    """从diff_ir构建hunks列表"""
    hunks: List[Dict[str, Any]] = []
    files = diff_ir.get("files", [])
    for f_idx, f in enumerate(files):
        file_path = f.get("file_path")
        language = f.get("language")
        change_type = f.get("change_type")
        is_binary = f.get("is_binary", False)

        for h_idx, h in enumerate(f.get("hunks", [])):
            hid = f"{f_idx}:{h_idx}"

            # --- Line range helpers (for downstream planning / UI) ---
            old_start = h.get("old_start")
            old_count = h.get("old_count")
            new_start = h.get("new_start")
            new_count = h.get("new_count")

            def _end_line(start: Optional[int], count: Optional[int]) -> Optional[int]:
                if start is None or count is None:
                    return None
                try:
                    c = int(count)
                    return int(start) + max(c, 0) - 1 if c > 0 else int(start)
                except Exception:
                    return None

            hunks.append(
                {
                    "hunk_id": hid,
                    "file_path": file_path,
                    "language": language,
                    "change_type": change_type,
                    "is_binary": is_binary,
                    "old_start": old_start,
                    "old_count": old_count,
                    "new_start": new_start,
                    "new_count": new_count,

                    # Human-friendly ranges
                    "old_start_line": old_start,
                    "old_end_line": _end_line(old_start, old_count),
                    "new_start_line": new_start,
                    "new_end_line": _end_line(new_start, new_count),
                    "header": h.get("header", ""),
                    "hunk_text": _hunk_to_text(h, max_lines=160),
                }
            )
    return hunks


def _make_batches(items: List[Dict[str, Any]], batch_size: int) -> List[List[Dict[str, Any]]]:
    return [items[i : i + batch_size] for i in range(0, len(items), batch_size)]


def _gather_minimal_context(file_path: str, hunk_lines: List[str], pr_dir: str) -> Dict[str, Any]:
    """
    为verifier收集最小化的上下文信息

    Args:
        file_path: 文件路径
        hunk_lines: hunk中的代码行
        pr_dir: PR目录路径

    Returns:
        包含上下文信息的字典
    """
    context = {
        "imports": [],
        "function_calls": [],
        "class_usage": [],
        "variable_references": []
    }

    try:
        full_path = os.path.join(pr_dir, file_path)
        if not os.path.exists(full_path):
            return context

        # 从hunk中提取可能的函数名、变量名、类名
        potential_symbols = set()
        for line in hunk_lines:
            # 简单的正则匹配提取符号
            # 函数调用
            func_calls = re.findall(r'(\w+)\(', line)
            potential_symbols.update(func_calls)

            # 变量引用
            var_refs = re.findall(r'\b(\w+)\b', line)
            potential_symbols.update(var_refs)

            # 类引用 (大写开头)
            class_refs = re.findall(r'\b([A-Z]\w*)\b', line)
            potential_symbols.update(class_refs)

        # 过滤掉常见关键词
        filtered_symbols = [sym for sym in potential_symbols
                          if sym not in ['if', 'for', 'while', 'def', 'class', 'import', 'from',
                                        'return', 'else', 'elif', 'try', 'except', 'with', 'as',
                                        'and', 'or', 'not', 'in', 'is', 'None', 'True', 'False']
                          and len(sym) > 1]

        if not filtered_symbols:
            return context

        # 使用grep查找相关行（限制每个符号最多3个匹配）
        try:
            for symbol in filtered_symbols[:10]:  # 限制符号数量
                try:
                    # 查找函数定义
                    result = subprocess.run(
                        ['grep', '-n', f'def {symbol}\\(', full_path],
                        capture_output=True, text=True, timeout=5
                    )
                    if result.stdout.strip():
                        context["function_calls"].extend(result.stdout.strip().split('\n')[:3])

                    # 查找类定义
                    result = subprocess.run(
                        ['grep', '-n', f'class {symbol}\\b', full_path],
                        capture_output=True, text=True, timeout=5
                    )
                    if result.stdout.strip():
                        context["class_usage"].extend(result.stdout.strip().split('\n')[:3])

                    # 查找import语句
                    result = subprocess.run(
                        ['grep', '-n', f'import.*{symbol}\\b', full_path],
                        capture_output=True, text=True, timeout=5
                    )
                    if result.stdout.strip():
                        context["imports"].extend(result.stdout.strip().split('\n')[:3])

                except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
                    # grep失败或超时，跳过
                    continue

        except Exception as e:
            # 上下文收集失败不应该影响主流程
            print(f"Warning: Context gathering failed for {file_path}: {e}")

        # 去重并限制总长度
        for key in context:
            context[key] = list(set(context[key]))[:5]

    except Exception as e:
        print(f"Warning: Failed to gather context for {file_path}: {e}")

    return context


def _detect_api_breakage(hunk_text: str, file_path: str) -> Dict[str, Any]:
    """
    检测API变更风险，特别是可能破坏兼容性的变更

    Args:
        hunk_text: hunk的文本内容
        file_path: 文件路径

    Returns:
        包含API breakage检测结果的字典
    """
    result = {
        "is_api_breakage": False,
        "breakage_type": None,
        "evidence": [],
        "risk_level": "none"
    }

    try:
        lines = hunk_text.split('\n')
        for line in lines:
            line = line.strip()
            if not line or line.startswith(('---', '+++', '@@')):
                continue

            # 检测函数参数变更
            if line.startswith('+'):
                # 新增或修改的行
                if re.search(r'def \w+\([^)]*\)', line):
                    # 函数定义
                    if 'async def' in line:
                        match = re.search(r'async def (\w+)\(([^)]*)\)', line)
                    else:
                        match = re.search(r'def (\w+)\(([^)]*)\)', line)

                    if match:
                        func_name = match.group(1)
                        params = match.group(2)

                        # 检查是否移除了参数（这可能是API breakage）
                        if not params or params.strip() == 'self' or params.strip() == '':
                            result["is_api_breakage"] = True
                            result["breakage_type"] = "parameter_removal"
                            result["evidence"].append(f"Function {func_name} with no/minimal parameters")
                            result["risk_level"] = "high"

                # 检测返回值变更
                if 'return ' in line and not line.startswith('#'):
                    result["is_api_breakage"] = True
                    result["breakage_type"] = "return_value_change"
                    result["evidence"].append(f"Return statement modified: {line}")
                    result["risk_level"] = "medium"

                # 检测装饰器变更（可能影响行为）
                if line.startswith('@') and any(decorator in line for decorator in ['@app.route', '@bp.route', '@api.route']):
                    result["is_api_breakage"] = True
                    result["breakage_type"] = "route_change"
                    result["evidence"].append(f"Route decorator modified: {line}")
                    result["risk_level"] = "high"

            elif line.startswith('-'):
                # 删除的行
                if 'def ' in line:
                    result["is_api_breakage"] = True
                    result["breakage_type"] = "function_removal"
                    result["evidence"].append(f"Function removed: {line}")
                    result["risk_level"] = "critical"

                if '@' in line and any(decorator in line for decorator in ['@app.route', '@bp.route', '@api.route']):
                    result["is_api_breakage"] = True
                    result["breakage_type"] = "endpoint_removal"
                    result["evidence"].append(f"Endpoint removed: {line}")
                    result["risk_level"] = "critical"

                # 检测导入移除（可能破坏依赖）
                if line.strip().startswith('import ') or line.strip().startswith('from '):
                    result["is_api_breakage"] = True
                    result["breakage_type"] = "import_removal"
                    result["evidence"].append(f"Import removed: {line}")
                    result["risk_level"] = "medium"

        # 根据文件路径调整风险评估
        if 'tests/' in file_path or 'test_' in file_path:
            result["risk_level"] = "low"  # 测试文件的变更影响较小
        elif any(path in file_path for path in ['api/', 'routes/', 'views/', 'handlers/']):
            # API相关文件的风险更高
            if result["risk_level"] == "medium":
                result["risk_level"] = "high"

    except Exception as e:
        print(f"Warning: API breakage detection failed: {e}")

    return result


# LLM Prompts
RISK_JUDGE_SYSTEM_PROMPT = """你是代码安全风险"裁判"。你的任务是基于输入的证据进行判别式验证，输出三态结果。

输入可能包括：
1) Diff hunk文本（变更内容）
2) Semgrep扫描结果（如果有的话）
3) 代码上下文

输出必须是以下status之一：
- CONFIRMED: 有明确证据证明这是真实漏洞
- SUSPECTED: 有风险信号但证据不足，需要更多上下文/测试
- NONE: 不构成风险或证据不足

强约束规则：
1) 只有当能从输入证据中"直接证明"风险存在时，才输出 CONFIRMED
2) 对于CONFIRMED，必须提供具体的证据引用和可解释的攻击路径
3) 对于SUSPECTED，必须说明缺少什么证据或测试
4) 对于逻辑缺陷，必须能给出具体反例才能CONFIRMED
5) Semgrep高置信度结果倾向CONFIRMED，但仍需验证
6) 输出必须是严格JSON格式

JSON Schema：
{
  "status": "CONFIRMED|SUSPECTED|NONE",
  "risk_type": "SQL_INJECTION|COMMAND_INJECTION|AUTH_BYPASS|SSRF|PATH_TRAVERSAL|INSECURE_DESERIALIZATION|SENSITIVE_DATA_EXPOSURE|LOGIC_DEFECT|OTHER",
  "severity": "CRITICAL|HIGH|MEDIUM|LOW",
  "confidence": 0.0,
  "why": "详细解释为什么是这个状态",
  "evidence_refs": ["semgrep_finding_id|diff_line_refs", "..."],
  "attack_path": "对于CONFIRMED，描述攻击路径（对于其他状态为空）",
  "missing_context": "对于SUSPECTED，说明需要什么额外上下文或测试",
  "counter_example": "对于逻辑缺陷CONFIRMED，给出具体反例"
}
"""

RISK_VERIFY_SYSTEM_PROMPT = """你是代码安全风险"裁判"。你将收到一个 hunk 的 diff 文本，你需要做出三态判别：CONFIRMED/SUSPECTED/NONE。

输出必须是以下 status 之一，并附带对应的 risk_type：
- CONFIRMED: 有明确证据证明这是真实漏洞
- SUSPECTED: 有风险信号但证据不足，需要更多上下文/测试
- NONE: 能明确判定"不是问题/与本 PR 无关"

对应 risk_type 范围：
- SQL_INJECTION
- COMMAND_INJECTION
- AUTH_BYPASS
- SSRF
- PATH_TRAVERSAL
- INSECURE_DESERIALIZATION
- SENSITIVE_DATA_EXPOSURE
- API_BREAKAGE
- LOGIC_DEFECT
- CONCURRENCY_ISSUE
- CONFIGURATION_RISK
- NONE

强约束：
1) NONE 只用于能明确判定"不是问题/与本 PR 无关"的情况
2) 不确定但值得关注的，输出 SUSPECTED
3) 对于 CONFIRMED，必须提供具体的证据引用和可解释的攻击路径
4) 对于 SUSPECTED，必须说明缺少什么证据或测试，以及为什么值得关注
5) 特别关注：API 变更（参数/返回值变更）、并发逻辑、路由配置、认证授权变更
6) 输出必须是严格 JSON

JSON Schema：
{
  "status": "CONFIRMED|SUSPECTED|NONE",
  "risk_type": "SQL_INJECTION|COMMAND_INJECTION|AUTH_BYPASS|SSRF|PATH_TRAVERSAL|INSECURE_DESERIALIZATION|SENSITIVE_DATA_EXPOSURE|API_BREAKAGE|LOGIC_DEFECT|CONCURRENCY_ISSUE|CONFIGURATION_RISK|NONE",
  "confidence": 0.0,
  "why": "详细解释为什么是这个状态",
  "evidence": ["diff_evidence_line1", "diff_evidence_line2"],
  "attack_path": "对于CONFIRMED，描述攻击路径（其他为空）",
  "missing_context": "对于SUSPECTED，说明需要什么额外上下文或测试",
  "impact_assessment": "评估此变更的潜在影响范围"
}
"""

BATCH_SYSTEM_PROMPT = """你是"PR 变更分诊（Triage）Agent"。你只做分诊，不做漏洞定论。

你将看到多个 diff hunks。对每个 hunk 你要输出：
1) functional_change：这个变更做了什么（只描述事实，不推测业务）
2) risk_signals：风险信号是什么（必须能从 diff 里直接引用证据行）
3) risk_score：0~100（代表"值得深挖程度"，不是"漏洞确定性"）
4) key_questions：如果证据不足，需要什么上下文才能确认
5) suggested_agents：建议后续交给哪个 agent 深挖

强约束（非常重要）：
- 禁止输出"存在漏洞""攻击者可以""可被利用"等结论性措辞。
- 你只能基于 diff 文本做判断；没有证据就降低 risk_score，并写清楚缺什么证据。
- risk_signals 必须包含 1~3 条 diff 原样证据（<=80字/条）。
- 测试文件 / mock / 注释 / 格式化变更：risk_score 必须 <= 10。
- 输出必须是严格 JSON，符合 schema。

JSON Schema：
{
  "items": [
    {
      "hunk_id": "string",
      "file_path": "string",
      "new_start_line": 0,
      "new_end_line": 0,
      "old_start_line": 0,
      "old_end_line": 0,
      "functional_change": "string",
      "before_after": ["string", "..."],
      "feature_tags": ["string", "..."],
      "business_context_guess": "string",
      "risk_signals": ["diff_evidence_line1", "diff_evidence_line2"],
      "risk_score": 0,
      "severity": "blocker|high|medium|low|info",
      "risk_categories": ["auth","security","memory","concurrency","config","logic","tests","dependency","performance","data-integrity","other"],
      "confidence": 0.0,
      "rationale": "string",
      "key_questions": ["string", "..."],
      "suggested_agents": ["security_agent","logic_agent","memory_agent","static_agent","config_agent","performance_agent"]
    }
  ]
}
"""

FINAL_SYSTEM_PROMPT = """你是"按功能分类的 PR 变更总结 + 深挖计划生成器"。你将得到一组逐 hunk 的条目（包含 functional_change、feature_tags、风险分数、verified_risk等）。
你的任务是：
1) 按"功能"对 hunks 做分组分类（基于 feature_tags + functional_change 的语义聚类），生成 feature buckets。
2) 每个 feature bucket 需要输出：功能摘要、潜在影响、涉及的 hunks、建议深挖方向（agent）、以及该功能下最高风险点。
3) 同时输出一个全局的 Top N 深挖清单（基于 risk_score 和 verified_risk 状态），但最终展示以"功能分类"为主。

强约束：
- 不要编造不存在的 hunk_id/file_path。
- 不要更改已有条目的事实含义；若发现字段缺失/格式问题，仅做最小修补并记录 notes。
- 输出必须是严格 JSON（不要 Markdown，不要多余文本）。
- 功能分类要"业务逻辑导向"：比如"鉴权流程调整""输入校验/解析变更""数据写入路径变更""异常处理与返回码策略变更""配置/部署策略调整"等。
- verified_risk 处理规则：
  * CONFIRMED 状态：必须出现在 top_focus，优先深挖
  * SUSPECTED 状态：如果 risk_score >= 60 或 risk_type 为 API_BREAKAGE/CONCURRENCY_ISSUE，应该进入深挖计划
  * NONE 状态：排除在深挖计划之外，但可在 notes 中提到"已复核为无明确证据"
- stage3_plan 应该包含所有值得深挖的 features（包含 SUSPECTED 状态的高分项）

JSON Schema（必须严格遵循）：
{
  "summary": {
    "total_hunks": 0,
    "top_n": 0,
    "feature_count": 0,
    "verification_breakdown": {"confirmed":0,"suspected":0,"none":0},
    "notes": ["string", "..."]
  },
  "stage3_plan": {
    "features_to_deep_dive": ["feature_id1", "feature_id2"],
    "quick_check_queue": ["feature_id3", "feature_id4"],
    "deep_dive_queue": ["feature_id5", "feature_id6"],
    "reason": "包含 CONFIRMED 状态和高分 SUSPECTED 状态的 features"
  },
  "features": [
    {
      "feature_id": "string",
      "feature_name": "string",
      "feature_tags": ["string", "..."],
      "summary": "string",
      "business_impact": "string",
      "risk_overview": {
        "max_risk_score": 0,
        "max_verified_status": "CONFIRMED|SUSPECTED|NONE",
        "sev_breakdown": {"blocker":0,"high":0,"medium":0,"low":0,"info":0},
        "key_risks": ["string", "..."],
        "verified_risks": {"confirmed":0,"suspected":0,"none":0}
      },
      "suggested_agents": ["..."],
      "hunks": [
        {
          "hunk_id": "string",
          "file_path": "string",
          "new_start_line": 0,
          "new_end_line": 0,
          "old_start_line": 0,
          "old_end_line": 0,
          "functional_change": "string",
          "risk_score": 0,
          "severity": "blocker|high|medium|low|info",
          "confidence": 0.0,
          "verified_status": "CONFIRMED|SUSPECTED|NONE",
          "verified_risk_type": "string"
        }
      ]
    }
  ],
  "top_focus": [
    {
      "rank": 1,
      "hunk_id": "string",
      "file_path": "string",
      "verified_status": "CONFIRMED|SUSPECTED|NONE",
      "why_focus": "string",
      "suggested_agents": ["..."]
    }
  ]
}
"""


def prepare_node(state: RiskState) -> RiskState:
    """准备节点 - 加载数据并创建批次"""
    pr_dir = state["pr_dir"]
    diff_ir_path = state.get("diff_ir_path") or os.path.join(pr_dir, "out", "diff_ir.json")
    if not os.path.exists(diff_ir_path):
        raise FileNotFoundError(f"diff_ir.json not found at: {diff_ir_path}")

    diff_ir = _load_json(diff_ir_path)
    hunks = _build_hunks(diff_ir)
    batches = _make_batches(hunks, state.get("batch_size", 8))

    out_dir = os.path.join(pr_dir, "out")
    os.makedirs(out_dir, exist_ok=True)
    output_path = os.path.join(out_dir, "feature_risk_plan.json")

    return {
        "diff_ir_path": diff_ir_path,
        "diff_ir": diff_ir,
        "pr_hunks": hunks,
        "batches": batches,
        "partial_items": [],
        "output_path": output_path,
        "messages": [SystemMessage(content="Workflow started: prepared hunks & batches.")],
    }


def score_next_batch_node_factory(llm: ChatOpenAI):
    """评分批次节点工厂"""
    def score_next_batch_node(state: RiskState) -> RiskState:
        batches = state.get("batches", [])
        if not batches:
            return state

        batch = batches[0]
        remaining = batches[1:]

        payload = {
            "batch_size": len(batch),
            "hunks": [
                {
                    "hunk_id": h["hunk_id"],
                    "file_path": h["file_path"],
                    "language": h["language"],
                    "change_type": h["change_type"],
                    "old_range": {"start": h["old_start"], "count": h["old_count"]},
                    "new_range": {"start": h["new_start"], "count": h["new_count"]},

                    # Line ranges (for deterministic downstream usage)
                    "old_start_line": h.get("old_start_line"),
                    "old_end_line": h.get("old_end_line"),
                    "new_start_line": h.get("new_start_line"),
                    "new_end_line": h.get("new_end_line"),
                    "header": h["header"],
                    "is_binary": h["is_binary"],
                    "diff": h["hunk_text"],
                }
                for h in batch
            ],
        }

        msgs = [
            SystemMessage(content=BATCH_SYSTEM_PROMPT),
            HumanMessage(content=json.dumps(payload, ensure_ascii=False)),
        ]
        
        # Retry logic for LLM calls with null response handling
        max_retries = 3
        text = None
        last_error = None
        for attempt in range(max_retries):
            try:
                resp = llm.invoke(msgs)
                text = resp.content if hasattr(resp, "content") else str(resp)
                if text:
                    break
            except TypeError as e:
                if "null value for `choices`" in str(e):
                    last_error = e
                    import time
                    time.sleep(1)  # Wait before retry
                    continue
                raise
            except Exception as e:
                last_error = e
                import time
                time.sleep(1)
                continue
        
        if not text:
            # Fallback: create default scores for this batch
            items = []
            for h in batch:
                items.append({
                    "hunk_id": h["hunk_id"],
                    "file_path": h["file_path"],
                    "language": h["language"],
                    "risk_categories": ["unknown"],
                    "risk_labels": ["needs_review"],
                    "symbol_name": "",
                    "functional_change": "Unknown - LLM unavailable",
                    "risk_score": 50,  # Default medium risk
                    "severity": "medium",
                    "confidence": 0.3,
                    "suggested_agents": ["logic_agent", "security_agent"],
                    "hunk_text": h["hunk_text"],
                })
            partial_items = state.get("partial_items", []) + items
            return {
                "batches": remaining,
                "partial_items": partial_items,
                "messages": state.get("messages", []) + [
                    HumanMessage(content=f"LLM failed after {max_retries} retries, using default scores for {len(items)} items.")
                ],
            }

        try:
            # Clean up the response - remove markdown code blocks if present
            cleaned_text = text.strip()
            if cleaned_text.startswith("```json"):
                cleaned_text = cleaned_text[7:]  # Remove ```json
            elif cleaned_text.startswith("```"):
                cleaned_text = cleaned_text[3:]   # Remove ```

            if cleaned_text.endswith("```"):
                cleaned_text = cleaned_text[:-3]  # Remove trailing ```

            cleaned_text = cleaned_text.strip()

            # Find JSON boundaries
            if not cleaned_text.startswith("{"):
                # Try to find the start of JSON
                start_idx = cleaned_text.find("{")
                if start_idx != -1:
                    cleaned_text = cleaned_text[start_idx:]

            if not cleaned_text.endswith("}"):
                # Try to find the end of JSON
                end_idx = cleaned_text.rfind("}")
                if end_idx != -1:
                    cleaned_text = cleaned_text[:end_idx + 1]

            parsed = json.loads(cleaned_text)
        except json.JSONDecodeError as e:
            raise RuntimeError(
                "LLM did not return valid JSON for batch scoring.\n"
                f"Error: {e}\nRaw output:\n{text}"
            )

        items = parsed.get("items", [])

        # ✅ 将本 batch 的 hunk_text 回填到 item，供后续 verify_risks_for_items 使用
        hunk_text_by_id = {h["hunk_id"]: h["hunk_text"] for h in batch}
        line_range_by_id = {
            h["hunk_id"]: {
                "old_start_line": h.get("old_start_line"),
                "old_end_line": h.get("old_end_line"),
                "new_start_line": h.get("new_start_line"),
                "new_end_line": h.get("new_end_line"),
                # keep raw too, some tools prefer start/count
                "old_start": h.get("old_start"),
                "old_count": h.get("old_count"),
                "new_start": h.get("new_start"),
                "new_count": h.get("new_count"),
            }
            for h in batch
        }
        for it in items:
            hid = it.get("hunk_id")
            if hid in hunk_text_by_id:
                it["hunk_text"] = hunk_text_by_id[hid]
            # ✅ Deterministically backfill line ranges so downstream JSON always has them
            if hid in line_range_by_id:
                it.update({k: v for k, v in line_range_by_id[hid].items() if k not in it})

        partial_items = state.get("partial_items", []) + items

        return {
            "batches": remaining,
            "partial_items": partial_items,
            "messages": state.get("messages", []) + [
                HumanMessage(content=f"Scored one batch, got {len(items)} items.")
            ],
        }

    return score_next_batch_node


def should_continue(state: RiskState) -> str:
    """判断是否继续处理批次"""
    return "score_next_batch" if state.get("batches") else "verify_items"


def verify_items_node_factory(llm: ChatOpenAI):
    def verify_items_node(state: RiskState) -> RiskState:
        items = state.get("partial_items", [])
        pr_dir = state.get("pr_dir", "")
        # ✅ 只复核高分项，控制成本
        verified = verify_risks_for_items(llm, items, score_threshold=70, pr_dir=pr_dir)

        return {
            "partial_items": verified,
            "messages": state.get("messages", []) + [
                HumanMessage(content=f"Verified high risk items. total={len(verified)}")
            ],
        }
    return verify_items_node


def finalize_node_factory(llm: ChatOpenAI):
    """完成节点工厂"""
    def finalize_node(state: RiskState) -> RiskState:
        items = state.get("partial_items", [])
        top_n = int(state.get("top_n", 20))

        payload = {
            "top_n": top_n,
            "items": items,
        }

        msgs = [
            SystemMessage(content=FINAL_SYSTEM_PROMPT),
            HumanMessage(content=json.dumps(payload, ensure_ascii=False)),
        ]
        
        # Retry logic for LLM calls with null response handling
        max_retries = 3
        text = None
        for attempt in range(max_retries):
            try:
                resp = llm.invoke(msgs)
                text = resp.content if hasattr(resp, "content") else str(resp)
                if text:
                    break
            except TypeError as e:
                if "null value for `choices`" in str(e):
                    import time
                    time.sleep(1)
                    continue
                raise
            except Exception:
                import time
                time.sleep(1)
                continue
        
        if not text:
            # Fallback: create basic feature structure without LLM
            features = []
            for item in items[:top_n]:
                features.append({
                    "feature_id": f"feature_{item.get('hunk_id', 'unknown')}",
                    "feature_name": f"Change in {item.get('file_path', 'unknown')}",
                    "summary": item.get("functional_change", "Code change"),
                    "hunks": [item],
                    "risk_overview": {
                        "max_risk_score": item.get("risk_score", 50),
                        "dominant_category": item.get("risk_categories", ["unknown"])[0] if item.get("risk_categories") else "unknown"
                    }
                })
            return {
                "final_output": {
                    "features": features,
                    "top_focus": [],
                    "summary": {
                        "total_hunks": len(items),
                        "llm_status": "fallback"
                    }
                },
                "messages": state.get("messages", []) + [
                    HumanMessage(content=f"LLM unavailable, created basic feature structure for {len(features)} items.")
                ],
            }

        try:
            # Clean up the response - remove markdown code blocks if present
            cleaned_text = text.strip()
            if cleaned_text.startswith("```json"):
                cleaned_text = cleaned_text[7:]  # Remove ```json
            elif cleaned_text.startswith("```"):
                cleaned_text = cleaned_text[3:]   # Remove ```

            if cleaned_text.endswith("```"):
                cleaned_text = cleaned_text[:-3]  # Remove trailing ```

            cleaned_text = cleaned_text.strip()

            # Find JSON boundaries
            if not cleaned_text.startswith("{"):
                # Try to find the start of JSON
                start_idx = cleaned_text.find("{")
                if start_idx != -1:
                    cleaned_text = cleaned_text[start_idx:]

            if not cleaned_text.endswith("}"):
                # Try to find the end of JSON
                end_idx = cleaned_text.rfind("}")
                if end_idx != -1:
                    cleaned_text = cleaned_text[:end_idx + 1]

            # First attempt: parse as-is
            try:
                final_output = json.loads(cleaned_text)
            except json.JSONDecodeError as e:
                print(f"Warning: Initial JSON parse failed: {e}")
                print("Attempting to repair common JSON issues...")

                # Try to repair common JSON issues
                try:
                    # Import here to avoid dependency issues
                    import re

                    # Add missing commas before closing braces/brackets (common LLM error)
                    repaired_text = cleaned_text

                    # Fix missing commas between array elements and object properties
                    repaired_text = re.sub(r'(["\d}])\s*({|\[)', r'\1,\2', repaired_text)
                    repaired_text = re.sub(r'(["\d}])\s*(?="[^"]*"\s*:)', r'\1,\2', repaired_text)

                    final_output = json.loads(repaired_text)
                    print("JSON repair successful")

                except json.JSONDecodeError as repair_error:
                    print(f"JSON repair failed: {repair_error}")

                    # Last resort: try to extract partial valid JSON
                    try:
                        # Find the last valid JSON structure
                        bracket_count = 0
                        brace_count = 0
                        last_valid_pos = -1

                        for i, char in enumerate(cleaned_text):
                            if char == '{':
                                brace_count += 1
                            elif char == '}':
                                brace_count -= 1
                            elif char == '[':
                                bracket_count += 1
                            elif char == ']':
                                bracket_count -= 1

                            # When we have a balanced structure, mark this position
                            if brace_count == 0 and bracket_count == 0 and char == '}':
                                last_valid_pos = i

                        if last_valid_pos > 0:
                            truncated_text = cleaned_text[:last_valid_pos + 1]
                            final_output = json.loads(truncated_text)
                            print(f"Successfully parsed truncated JSON (position {last_valid_pos})")
                        else:
                            raise repair_error

                    except Exception:
                        # If all repair attempts fail, raise the original error
                        raise RuntimeError(
                            "LLM did not return valid JSON for final feature grouping.\n"
                            f"Error: {e}\n"
                            f"Repair attempt failed: {repair_error}\n"
                            f"Raw output:\n{text}"
                        )
        except Exception as e:
            if isinstance(e, RuntimeError):
                raise
            else:
                raise RuntimeError(
                    "LLM did not return valid JSON for final feature grouping.\n"
                    f"Error: {e}\nRaw output:\n{text}"
                )

        # --- Deterministic backfill: ensure line ranges are present in final_output ---
        # Some LLM responses may omit these fields even if provided; we enforce them here.
        try:
            item_by_id = {
                it.get("hunk_id"): it
                for it in items
                if isinstance(it, dict) and it.get("hunk_id")
            }

            def _fill_hunk(hd: Dict[str, Any]):
                hid = hd.get("hunk_id")
                src = item_by_id.get(hid) if hid else None
                if not isinstance(src, dict):
                    return
                for k in ("new_start_line", "new_end_line", "old_start_line", "old_end_line"):
                    if hd.get(k) is None and src.get(k) is not None:
                        hd[k] = src.get(k)
                # Keep raw ranges too (useful for tools)
                for k in ("new_start", "new_count", "old_start", "old_count"):
                    if hd.get(k) is None and src.get(k) is not None:
                        hd[k] = src.get(k)

            for feat in final_output.get("features", []) if isinstance(final_output, dict) else []:
                for hd in feat.get("hunks", []) if isinstance(feat, dict) else []:
                    if isinstance(hd, dict):
                        _fill_hunk(hd)
        except Exception:
            # Never fail the workflow because of a best-effort backfill
            pass

        output_path = state["output_path"]
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(final_output, f, ensure_ascii=False, indent=2)

        return {
            "final_output": final_output,
            "messages": state.get("messages", []) + [
                HumanMessage(content=f"Final feature_risk_plan written to: {output_path}")
            ],
        }

    return finalize_node


def build_risk_analysis_workflow(llm: ChatOpenAI):
    """构建风险分析工作流"""
    g = StateGraph(RiskState)
    g.add_node("prepare", prepare_node)
    g.add_node("score_next_batch", score_next_batch_node_factory(llm))
    g.add_node("verify_items", verify_items_node_factory(llm))
    g.add_node("finalize", finalize_node_factory(llm))

    g.add_edge(START, "prepare")
    g.add_edge("prepare", "score_next_batch")
    g.add_conditional_edges("score_next_batch", should_continue, {
        "score_next_batch": "score_next_batch",
        "verify_items": "verify_items",
    })
    g.add_edge("verify_items", "finalize")
    g.add_edge("finalize", END)
    return g.compile()


def verify_risks_for_items(llm: ChatOpenAI, items: List[Dict[str, Any]], score_threshold: int = 70, pr_dir: str = "") -> List[Dict[str, Any]]:
    """
    风险复核函数 - 对高风险项进行二次裁判，使用三态判别

    Args:
        llm: 语言模型
        items: hunk项目列表
        score_threshold: 复核阈值
        pr_dir: PR目录路径，用于上下文收集

    Returns:
        带verified_risk字段的项目列表
    """
    verified = []
    for it in items:
        # 只复核高分片段，控制成本
        if int(it.get("risk_score", 0)) < score_threshold:
            it["verified_risk"] = {
                "status": "NONE",
                "risk_type": "NONE",
                "confidence": 0.9,
                "why": "below_threshold",
                "evidence": [],
                "attack_path": "",
                "missing_context": "",
                "impact_assessment": "Low risk item below threshold"
            }
            verified.append(it)
            continue

        hunk_text = it.get("hunk_text", "")
        file_path = it.get("file_path", "")
        hunk_id = it.get("hunk_id", "")

        # 收集最小化的上下文
        hunk_lines = hunk_text.split('\n') if hunk_text else []
        context = _gather_minimal_context(file_path, hunk_lines, pr_dir) if pr_dir else {}

        # 检测API breakage
        api_breakage = _detect_api_breakage(hunk_text, file_path)

        user = f"""HUNK_ID: {hunk_id}
FILE: {file_path}
DIFF:
{hunk_text}

MINIMAL CONTEXT:
{json.dumps(context, indent=2, ensure_ascii=False)}

API BREAKAGE DETECTION:
{json.dumps(api_breakage, indent=2, ensure_ascii=False)}
"""
        resp = llm.invoke([
            SystemMessage(content=RISK_VERIFY_SYSTEM_PROMPT),
            HumanMessage(content=user),
        ])
        try:
            content = resp.content if hasattr(resp, "content") else str(resp)
            # 找到JSON部分
            start = content.find("{")
            end = content.rfind("}") + 1
            if start != -1 and end > start:
                j = json.loads(content[start:end])
            else:
                raise ValueError("No JSON found in response")
        except Exception as e:
            j = {
                "status": "SUSPECTED",  # 默认为可疑，避免误判
                "risk_type": "LOGIC_DEFECT",
                "confidence": 0.3,
                "why": f"json_parse_failed: {str(e)}",
                "evidence": [],
                "attack_path": "",
                "missing_context": "Failed to parse LLM response",
                "impact_assessment": "Unknown - needs manual review"
            }

        it["verified_risk"] = j

        # 根据三态结果调整风险分数
        status = j.get("status", "SUSPECTED")
        risk_type = j.get("risk_type", "NONE")

        # 如果API breakage检测发现高风险，强制提升为SUSPECTED或更高
        if api_breakage.get("is_api_breakage") and api_breakage.get("risk_level") in ["critical", "high"]:
            if status == "NONE":
                status = "SUSPECTED"
                risk_type = "API_BREAKAGE"
                j["status"] = "SUSPECTED"
                j["risk_type"] = "API_BREAKAGE"
                j["why"] = f"API breakage detected: {api_breakage.get('breakage_type', 'unknown')}"
                j["evidence"] = api_breakage.get("evidence", [])
                j["impact_assessment"] = "Potential API compatibility breakage"

        if status == "NONE":
            # 明确无风险，大幅降低分数
            it["risk_score"] = min(int(it.get("risk_score", 0)), 15)
            it["severity"] = "info"
            it["risk_categories"] = []
        elif status == "CONFIRMED":
            # 确认风险，保持或提高分数
            it["risk_score"] = max(int(it.get("risk_score", 0)), 85)
            if risk_type in ["SQL_INJECTION", "COMMAND_INJECTION", "AUTH_BYPASS"]:
                it["severity"] = "blocker"
            else:
                it["severity"] = "high"
            # 确保有合适的风险分类
            if not it.get("risk_categories"):
                it["risk_categories"] = ["security"]
        elif status == "SUSPECTED":
            # 可疑风险，保持中等分数但不要过度降低
            current_score = int(it.get("risk_score", 0))

            # 根据API breakage检测结果调整分数
            if api_breakage.get("is_api_breakage"):
                if api_breakage.get("risk_level") == "critical":
                    it["risk_score"] = max(current_score, 80)
                    it["severity"] = "high"
                elif api_breakage.get("risk_level") == "high":
                    it["risk_score"] = max(current_score, 70)
                    it["severity"] = "medium"
                elif api_breakage.get("risk_level") == "medium":
                    it["risk_score"] = max(current_score, 55)
                    it["severity"] = "medium"
            else:
                if risk_type in ["API_BREAKAGE", "CONCURRENCY_ISSUE", "CONFIGURATION_RISK"]:
                    # API变更等应该保持较高分数
                    it["risk_score"] = max(current_score, 65)
                    it["severity"] = "medium"
                else:
                    it["risk_score"] = max(current_score, 45)
                    it["severity"] = "medium"

            # 确保有风险分类
            if not it.get("risk_categories"):
                if risk_type == "API_BREAKAGE":
                    it["risk_categories"] = ["other"]
                elif risk_type == "CONCURRENCY_ISSUE":
                    it["risk_categories"] = ["concurrency"]
                elif risk_type == "CONFIGURATION_RISK":
                    it["risk_categories"] = ["config"]
                else:
                    it["risk_categories"] = ["security"]

        verified.append(it)

    return verified


def analyze_risk(pr_dir: str, llm: ChatOpenAI, top_n: int = 20, batch_size: int = 8) -> Dict[str, Any]:
    """
    执行风险分析

    Args:
        pr_dir: PR目录
        llm: 语言模型
        top_n: 重点关注数量
        batch_size: 批次大小

    Returns:
        风险分析结果
    """
    init_state: RiskState = {
        "pr_dir": os.path.abspath(pr_dir),
        "top_n": top_n,
        "batch_size": batch_size,
        "messages": [],
    }

    app = build_risk_analysis_workflow(llm)
    out = app.invoke(init_state)

    return out.get("final_output", {})