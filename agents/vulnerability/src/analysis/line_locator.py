"""Line-level location resolver for LLM-produced findings.

We want robust line positioning for GitHub inline comments.

Approach:
- The prompt shows code with explicit absolute line numbers (RIGHT/new side).
- The model returns both a candidate line number and a short code fragment.
- We verify / correct the line number by matching the fragment against the
  commentable lines (add+context) in the corresponding diff hunk.

This module implements a lightweight Rouge-L (token LCS) similarity.
"""

from __future__ import annotations

import re
from typing import Any, Dict, List, Optional, Tuple


_TOKEN_RE = re.compile(r"[A-Za-z_][A-Za-z_0-9]*|\d+|==|!=|<=|>=|->|::|\S")


def tokenize(s: str) -> List[str]:
    return _TOKEN_RE.findall(s or "")


def lcs_len(a: List[str], b: List[str]) -> int:
    """Length of longest common subsequence (DP, O(n*m))."""
    if not a or not b:
        return 0
    # Keep DP row small.
    if len(a) < len(b):
        short, long = a, b
    else:
        short, long = b, a
    prev = [0] * (len(short) + 1)
    for tok in long:
        cur = [0]
        for j, st in enumerate(short, start=1):
            if tok == st:
                cur.append(prev[j - 1] + 1)
            else:
                cur.append(max(prev[j], cur[-1]))
        prev = cur
    return prev[-1]


def rouge_l_f1(candidate: str, reference: str) -> float:
    """Rouge-L F1 score between two strings."""
    a = tokenize(candidate)
    b = tokenize(reference)
    if not a or not b:
        return 0.0
    lcs = lcs_len(a, b)
    if lcs == 0:
        return 0.0
    return (2.0 * lcs) / (len(a) + len(b))


def resolve_line_by_similarity(
    target_text: str,
    line_candidates: List[Dict[str, Any]],
    min_score: float = 0.45,
) -> Optional[Tuple[int, float]]:
    """Resolve a target code fragment to the best matching candidate line.

    Args:
        target_text: code fragment from LLM (e.g., issue.evidence.diff_snippet)
        line_candidates: entries from build_line_candidates_from_hunk
        min_score: minimum Rouge-L F1 to accept

    Returns:
        (best_line_number, best_score) or None
    """
    best_line: Optional[int] = None
    best_score: float = 0.0

    if not target_text or not line_candidates:
        return None

    for c in line_candidates:
        line_no = c.get("line")
        content = c.get("content", "")
        score = rouge_l_f1(target_text, content)
        # Prefer later line on tie (multi-line snippets often end line is best)
        if score > best_score or (score == best_score and isinstance(line_no, int) and isinstance(best_line, int) and line_no > best_line):
            best_score = score
            best_line = int(line_no) if line_no is not None else None

    if best_line is None or best_score < min_score:
        return None
    return best_line, best_score


def normalize_issue_list(obj: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Handle both {issues:[...]} and legacy {issue:{...}} outputs."""
    if isinstance(obj.get("issues"), list):
        return obj.get("issues") or []
    if isinstance(obj.get("issue"), dict):
        return [obj["issue"]]
    return []


def apply_line_resolution(
    model_obj: Dict[str, Any],
    file_path: str,
    hunk_id: str,
    line_candidates: List[Dict[str, Any]],
) -> Dict[str, Any]:
    """Mutate model_obj in-place: fill/correct issue.location line numbers.

    - If the model already provides location.line_start/line_end, we still try to
      verify/correct using similarity if we have a target snippet.
    - If it doesn't provide line numbers, we attempt to infer from similarity.
    - Only RIGHT-side (new) lines are produced.
    """
    issues = normalize_issue_list(model_obj)
    for issue in issues:
        location = issue.get("location") or {}
        # Best-effort target snippet
        target = (
            issue.get("location_in_code")
            or issue.get("Location in Code")
            or (issue.get("evidence") or {}).get("diff_snippet")
            or ""
        )

        resolved = resolve_line_by_similarity(target, line_candidates) if target else None
        if resolved:
            ln, _ = resolved
            location.setdefault("file", file_path)
            location["line_start"] = ln
            location["line_end"] = ln
            issue["location"] = location

        # Ensure file path always present
        if isinstance(issue.get("location"), dict):
            issue["location"].setdefault("file", file_path)

    # Attach resolved location to _meta for downstream publisher fallback
    model_obj.setdefault("_meta", {})
    if isinstance(model_obj.get("_meta"), dict):
        model_obj["_meta"].setdefault("hunk_id", hunk_id)
        model_obj["_meta"].setdefault("file_path", file_path)
    return model_obj
