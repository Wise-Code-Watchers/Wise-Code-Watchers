"""
ç»Ÿä¸€å·¥ä½œæµç³»ç»Ÿ - æ•´åˆæ‰€æœ‰åŠŸèƒ½çš„å•ä¸€å·¥ä½œæµæ–‡ä»¶
æ”¯æŒå¤šç§åˆ†ææ¨¡å¼ï¼špr_review, security_audit, ai_analysis
"""

import json
import os
import uuid
import logging
import traceback
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, TypedDict, Annotated
from concurrent.futures import ThreadPoolExecutor, as_completed

# ç¯å¢ƒå˜é‡æ§åˆ¶è¯¦ç»†æ—¥å¿—
ENABLE_DETAILED_LOGS = os.getenv("ENABLE_DETAILED_LOGS", "false").lower() == "true"

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode

from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage, ToolMessage
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

# é…ç½®æ—¥å¿— - é¿å…é‡å¤handler
logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)
    logger.propagate = False  # é¿å…ä¼ æ’­åˆ°root loggerå¯¼è‡´é‡å¤

# ä¸ºLogic Agentå’ŒSecurity Agenté…ç½®æ›´è¯¦ç»†çš„æ—¥å¿—çº§åˆ«ï¼ˆå¯æ§åˆ¶ï¼‰
if ENABLE_DETAILED_LOGS:
    print("ğŸ” è¯¦ç»†æ—¥å¿—å·²å¯ç”¨ (è®¾ç½® ENABLE_DETAILED_LOGS=true)")
    for log_name in ['src.agents.logic_agent', 'src.agents.security_agent']:
        sub_logger = logging.getLogger(log_name)
        if not sub_logger.handlers:
            sub_logger.setLevel(logging.DEBUG)
            sub_logger.addHandler(handler)
            sub_logger.propagate = False
else:
    print("ğŸ“ åŸºç¡€æ—¥å¿—æ¨¡å¼ (è®¾ç½® ENABLE_DETAILED_LOGS=true å¯ç”¨è¯¦ç»†LLMäº¤äº’æ—¥å¿—)")

# å¯¼å…¥åˆ†æç»„ä»¶
from .analysis import parse_pr_data, analyze_risk, analyze_vulnerabilities
from .agents.logic_agent import run_logic_agent_for_pr
from .agents.security_agent import run_enhanced_security_agent_for_pr
from .agents.triage_agent import TriageAgent, run_triage_for_pr
from .analysis.cross_file_analyzer import CrossFileAnalyzer, create_cross_file_analyzer, analyze_pr_cross_file_impact
from .scripts.smart_context_builder import SmartContextBuilder
from .scripts.scanning import generate_feature_scan_tasks_and_update_plan, run_parallel_semgrep_tasks
from .scripts.todolist import generate_audit_todolist, create_code_tools_for_pr, execute_audit_todolist
from .scripts.reporting import generate_final_report, save_final_report, generate_precision_first_report
from .scripts.analysis import InitializationEngine


class WorkflowState(TypedDict, total=False):
    """ç»Ÿä¸€å·¥ä½œæµçŠ¶æ€å®šä¹‰"""
    messages: Annotated[List[BaseMessage], add_messages]

    # è¾“å…¥å‚æ•°
    pr_dir: str
    codebase_path: Optional[str]  # ğŸ†• å…‹éš†çš„æºä»£ç è·¯å¾„
    llm: ChatOpenAI
    github_token: Optional[str] = None
    owner: Optional[str] = None
    repo: Optional[str] = None
    pr_number: Optional[int] = None

    # é€šç”¨åˆ†æé˜¶æ®µ
    pr_data: Dict[str, Any]
    diff_ir: Dict[str, Any]
    parse_results: Dict[str, str]
    feature_risk_plan: Dict[str, Any]

    # åˆå§‹åŒ–é˜¶æ®µ
    initialization_result: Dict[str, Any]
    audit_units: List[Dict[str, Any]]

    # å·¥å…·æ‰«æé˜¶æ®µ
    semgrep_results: Dict[str, Any]
    evidence_store: Dict[str, Any]

    # å¹¶è¡ŒAgentåˆ†æé˜¶æ®µ
    logic_review: Dict[str, Any]
    security_review: Dict[str, Any]

    # å¢å¼ºæ¨¡å—é˜¶æ®µ
    triage_summary: Dict[str, Any]
    cross_file_impact: Dict[str, Any]

    # TODOåˆ—è¡¨é˜¶æ®µ
    todo_list: List[Dict[str, Any]]

    # AIæ¼æ´åˆ†æé˜¶æ®µ
    vulnerability_analysis: Dict[str, Any]
    query_plan: Dict[str, Any]

    # é€šç”¨è¾“å‡º
    final_report: Dict[str, Any]
    error: Optional[str]

    # é…ç½®å‚æ•°
    top_n: int
    batch_size: int
    max_workers: int


class WiseCodeWatchersWorkflow:
    """Wise Code Watchers ç»Ÿä¸€å·¥ä½œæµç³»ç»Ÿ"""

    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        self.graph = self._build_workflow()

    def _load_json(self, path: str) -> Dict[str, Any]:
        """å®‰å…¨åŠ è½½JSONæ–‡ä»¶"""
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            raise FileNotFoundError(f"æ— æ³•è¯»å–æ–‡ä»¶ {path}: {e}")

    def _save_json(self, path: str, data: Any) -> None:
        """å®‰å…¨ä¿å­˜JSONæ–‡ä»¶"""
        os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def _log_message(self, message: str) -> List[BaseMessage]:
        """è®°å½•æ—¥å¿—æ¶ˆæ¯"""
        print(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")
        return [HumanMessage(content=message)]

    def _load_pr_data(self, state: WorkflowState) -> Dict[str, Any]:
        """åŠ è½½PRæ•°æ®ï¼ˆæ”¯æŒGitHub APIæˆ–æœ¬åœ°æ–‡ä»¶ï¼‰"""
        if state.get("pr_data"):
            return state["pr_data"]

        # å¦‚æœæœ‰GitHubä¿¡æ¯ï¼Œä»APIè·å–
        if state.get("owner") and state.get("repo") and state.get("pr_number"):
            return self._fetch_github_pr_data(state)

        # å¦åˆ™ä»æœ¬åœ°æ–‡ä»¶åŠ è½½
        metadata_path = os.path.join(state["pr_dir"], "metadata.json")
        return self._load_json(metadata_path)

    def _fetch_github_pr_data(self, state: WorkflowState) -> Dict[str, Any]:
        """ä»GitHub APIè·å–PRæ•°æ®"""
        import requests

        owner = state["owner"]
        repo = state["repo"]
        pr_number = state["pr_number"]
        token = state["github_token"]

        headers = {
            "Authorization": f"Bearer {token}",
            "Accept": "application/vnd.github+json",
            "X-GitHub-Api-Version": "2022-11-28",
            "User-Agent": "wise-code-watchers",
        }

        url = f"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}"
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.json()

    def _load_diff_content(self, state: WorkflowState) -> str:
        """åŠ è½½PR diffå†…å®¹"""
        # å¦‚æœæœ‰GitHubä¿¡æ¯ï¼Œä»APIè·å–diff
        if state.get("owner") and state.get("repo") and state.get("pr_number"):
            return self._fetch_github_diff(state)

        # å¦åˆ™ä»æœ¬åœ°æ–‡ä»¶åŠ è½½
        diff_path = os.path.join(state["pr_dir"], "pr.diff")
        with open(diff_path, "r", encoding="utf-8") as f:
            return f.read()

    def _get_source_code_path(self, state: WorkflowState) -> str:
        """è·å–æºä»£ç è·¯å¾„ - ä¼˜å…ˆä½¿ç”¨ä¼ é€’çš„codebase_pathï¼Œå…¶æ¬¡æŸ¥æ‰¾workspaceï¼Œæœ€åfallbackåˆ°PRç›®å½•"""
        # ğŸ†• 1. ä¼˜å…ˆä½¿ç”¨ä»app.pyä¼ é€’è¿‡æ¥çš„codebase_path
        if state.get("codebase_path"):
            codebase = state["codebase_path"]
            logger.info(f"[æºä»£ç è·¯å¾„] ä½¿ç”¨app.pyä¼ é€’çš„å…‹éš†è·¯å¾„: {codebase}")
            return codebase

        # 2. Fallback: ä»metadata.jsonåæ¨æŸ¥æ‰¾workspaceè·¯å¾„
        pr_dir = state.get("pr_dir", "")
        metadata_path = os.path.join(pr_dir, "metadata.json")
        metadata = self._load_json(metadata_path)
        base_branch = metadata.get("base_branch", "")
        html_url = metadata.get("html_url", "")

        # ä»PR URLæå–repo_full_name
        # https://github.com/Wise-Code-Watchers/sentry-wcw/pull/7 -> Wise-Code-Watchers/sentry-wcw
        repo_full_name = ""
        if html_url:
            parts = html_url.replace('https://github.com/', '').split('/pull/')
            if len(parts) > 0:
                repo_full_name = parts[0]

        # 3. å°è¯•æŸ¥æ‰¾workspaceç›®å½•ä¸­çš„å…‹éš†ä»“åº“ï¼ˆGitClient.clone_for_prä½¿ç”¨çš„è·¯å¾„ï¼‰
        # è·¯å¾„æ¨¡å¼: workspace/{owner_repo}/{base_branch}
        possible_paths = []

        if repo_full_name and base_branch:
            # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆå‘ä¸ŠæŸ¥æ‰¾ï¼‰
            current_dir = os.path.dirname(__file__)
            project_root = current_dir
            for _ in range(5):  # å‘ä¸ŠæŸ¥æ‰¾æœ€å¤š5å±‚
                if os.path.exists(os.path.join(project_root, "app.py")):
                    break
                project_root = os.path.dirname(project_root)

            # æ¨¡å¼1: workspace/{owner_repo}/{base_branch}/ (GitClienté»˜è®¤è·¯å¾„)
            workspace_dir = os.path.join(project_root, "workspace")
            repo_dir = repo_full_name.replace("/", "_")
            possible_paths.append(os.path.join(workspace_dir, repo_dir, base_branch))

            # æ¨¡å¼2: workspace/{owner_repo}/
            possible_paths.append(os.path.join(workspace_dir, repo_dir))

        # 4. æ£€æŸ¥å“ªä¸ªè·¯å¾„å­˜åœ¨ä¸”åŒ…å«ä»£ç ä»“åº“ç‰¹å¾
        for path in possible_paths:
            if os.path.exists(path) and os.path.isdir(path):
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ä»£ç ä»“åº“
                has_git = os.path.exists(os.path.join(path, ".git"))
                has_code = False

                # æ£€æŸ¥æ˜¯å¦æœ‰ä»£ç æ–‡ä»¶
                try:
                    for root, dirs, files in os.walk(path):
                        if any(f.endswith(('.py', '.js', '.ts', '.java', '.go')) for f in files):
                            has_code = True
                            break
                        if len([d for d in dirs if not d.startswith('.')]) > 3:
                            # æœ‰å¤šä¸ªå­ç›®å½•ï¼Œå¯èƒ½æ˜¯ä»£ç ä»“åº“
                            has_code = True
                            break
                        if root != path:  # åªæ£€æŸ¥é¡¶å±‚
                            break
                except Exception:
                    pass

                if has_git or has_code:
                    logger.info(f"[æºä»£ç è·¯å¾„] æ‰¾åˆ°å…‹éš†ä»“åº“: {path}")
                    logger.info(f"[æºä»£ç è·¯å¾„]   åˆ†æ”¯åç§°: {base_branch}")
                    logger.info(f"[æºä»£ç è·¯å¾„]   ä»“åº“: {repo_full_name}")
                    return path

        # 5. Fallback: å°è¯•config.jsonï¼ˆç”¨äºæœ¬åœ°æµ‹è¯•ï¼‰
        config_path = os.path.join(os.path.dirname(__file__), "..", "config.json")
        try:
            config = self._load_json(config_path)
            base_branches_path = config.get("repository", {}).get("base_branches_path", "")

            if base_branches_path and base_branch:
                branch_path = Path(base_branches_path) / base_branch
                if branch_path.exists() and branch_path.is_dir():
                    logger.info(f"[æºä»£ç è·¯å¾„] ä½¿ç”¨config.jsoné…ç½®çš„åˆ†æ”¯ç›®å½•: {branch_path}")
                    logger.info(f"[æºä»£ç è·¯å¾„]   åŸºç¡€è·¯å¾„: {base_branches_path}")
                    logger.info(f"[æºä»£ç è·¯å¾„]   åˆ†æ”¯åç§°: {base_branch}")
                    return str(branch_path)
        except Exception as e:
            logger.debug(f"[æºä»£ç è·¯å¾„] æ— æ³•åŠ è½½config.json: {e}")

        # 6. æœ€ç»ˆfallback: ä½¿ç”¨PRç›®å½•
        logger.warning(f"[æºä»£ç è·¯å¾„] æœªæ‰¾åˆ°æºä»£ç ä»“åº“ï¼Œä½¿ç”¨PRç›®å½•ä½œä¸ºfallback: {pr_dir}")
        logger.warning(f"[æºä»£ç è·¯å¾„]   è¿™å¯èƒ½ä¼šå½±å“ä»£ç åˆ†æåŠŸèƒ½")
        logger.warning(f"[æºä»£ç è·¯å¾„]   æç¤º: ç¡®ä¿app.pyä¸­å·²æ­£ç¡®å…‹éš†ä»£ç åˆ°workspaceç›®å½•")
        return pr_dir

    def _fetch_github_diff(self, state: WorkflowState) -> str:
        """ä»GitHub APIè·å–PR diff"""
        import requests

        owner = state["owner"]
        repo = state["repo"]
        pr_number = state["pr_number"]
        token = state["github_token"]

        headers = {
            "Authorization": f"Bearer {token}",
            "Accept": "application/vnd.github.v3.diff",
            "X-GitHub-Api-Version": "2022-11-28",
            "User-Agent": "wise-code-watchers",
        }

        url = f"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}"
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.text

    # ==================== é€šç”¨èŠ‚ç‚¹ ====================

    def initialization_node(self, state: WorkflowState) -> WorkflowState:
        """åˆå§‹åŒ–èŠ‚ç‚¹ - æ§è¾“å…¥ã€ç«‹è¾¹ç•Œã€é™è¯¯æŠ¥"""
        messages = state.get("messages", [])
        logger.info("[å·¥ä½œæµ] è¿›å…¥åˆå§‹åŒ–èŠ‚ç‚¹")

        # åªåœ¨æœ¬åœ°æ¨¡å¼ä¸‹æ‰§è¡Œåˆå§‹åŒ–
        if not state.get("pr_dir"):
            logger.info("[å·¥ä½œæµ] è·³è¿‡åˆå§‹åŒ–é˜¶æ®µï¼ˆGitHub APIæ¨¡å¼ï¼‰")
            messages.extend(self._log_message("è·³è¿‡åˆå§‹åŒ–é˜¶æ®µï¼ˆGitHub APIæ¨¡å¼ï¼‰"))
            return {**state, "messages": messages, "initialization_result": {"success": True, "audit_units": []}}

        try:
            logger.info(f"[å·¥ä½œæµ] å¼€å§‹æœ¬åœ°åˆå§‹åŒ–ï¼ŒPRç›®å½•: {state['pr_dir']}")
            messages.extend(self._log_message("é˜¶æ®µ0: åˆå§‹åŒ–å®¡è®¡å•å…ƒ"))

            # åˆ›å»ºåˆå§‹åŒ–å¼•æ“
            logger.info("[å·¥ä½œæµ] åˆ›å»ºåˆå§‹åŒ–å¼•æ“")
            init_engine = InitializationEngine(state["llm"])

            # åŠ è½½diffå†…å®¹
            diff_path = os.path.join(state["pr_dir"], "pr.diff")
            logger.info(f"[å·¥ä½œæµ] åŠ è½½diffæ–‡ä»¶: {diff_path}")
            with open(diff_path, "r", encoding="utf-8") as f:
                diff_content = f.read()

            logger.info(f"[å·¥ä½œæµ] diffæ–‡ä»¶å¤§å°: {len(diff_content)} å­—ç¬¦")

            # æ‰§è¡Œåˆå§‹åŒ–
            logger.info("[å·¥ä½œæµ] å¼€å§‹æ‰§è¡Œåˆå§‹åŒ–")
            init_result = init_engine.initialize_audit_units(diff_content, state["pr_dir"])

            if init_result["success"]:
                passed_units = init_result["passed_units"]
                total_units = init_result["total_units"]
                skipped_units = init_result["skipped_units"]
                skipped_reasons = init_result["skipped_reasons"]

                logger.info(f"[å·¥ä½œæµ] åˆå§‹åŒ–æˆåŠŸ: {passed_units}/{total_units} é€šè¿‡, {skipped_units} è·³è¿‡")
                messages.extend(self._log_message(
                    f"åˆå§‹åŒ–å®Œæˆ: {passed_units}/{total_units}ä¸ªå•å…ƒé€šè¿‡, {skipped_units}ä¸ªè·³è¿‡"
                ))

                if skipped_reasons:
                    reasons_str = ", ".join([f"{k}({v})" for k, v in skipped_reasons.items()])
                    logger.info(f"[å·¥ä½œæµ] è·³è¿‡åŸå› ç»Ÿè®¡: {reasons_str}")
                    messages.extend(self._log_message(f"è·³è¿‡åŸå› : {reasons_str}"))

                return {
                    **state,
                    "messages": messages,
                    "initialization_result": init_result,
                    "audit_units": init_result["audit_units"]
                }
            else:
                error_msg = init_result.get("error", "åˆå§‹åŒ–å¤±è´¥")
                logger.error(f"[å·¥ä½œæµ] åˆå§‹åŒ–å¤±è´¥: {error_msg}")
                messages.extend(self._log_message(f"åˆå§‹åŒ–å¤±è´¥: {error_msg}"))
                return {
                    **state,
                    "messages": messages,
                    "initialization_result": init_result,
                    "audit_units": [],
                    "error": error_msg
                }

        except Exception as e:
            error_msg = f"åˆå§‹åŒ–å¼‚å¸¸: {str(e)}"
            logger.error(f"[å·¥ä½œæµ] åˆå§‹åŒ–å¼‚å¸¸: {error_msg}")
            messages.extend(self._log_message(f"é”™è¯¯: {error_msg}"))
            return {
                **state,
                "messages": messages,
                "initialization_result": {"success": False, "error": error_msg},
                "audit_units": [],
                "error": error_msg
            }

    def start_node(self, state: WorkflowState) -> WorkflowState:
        """å¼€å§‹èŠ‚ç‚¹ - åˆå§‹åŒ–å·¥ä½œæµ"""
        messages = state.get("messages", [])

        messages.extend(self._log_message("å¼€å§‹ç»Ÿä¸€å·¥ä½œæµ"))

        # æ”¯æŒæœ¬åœ°æ–‡ä»¶åˆ†ææ¨¡å¼å’ŒGitHub APIæ¨¡å¼
        pr_dir = state.get("pr_dir")

        if not pr_dir and not state.get("github_token"):
            error_msg = "éœ€è¦æä¾›PRç›®å½•å‚æ•°æˆ–GitHub Token"
            return {**state, "messages": messages, "error": error_msg}

        # æœ¬åœ°æ–‡ä»¶åˆ†ææ¨¡å¼
        if pr_dir:
            # æ£€æŸ¥å¿…è¦æ–‡ä»¶
            metadata_path = os.path.join(pr_dir, "metadata.json")
            diff_path = os.path.join(pr_dir, "pr.diff")

            missing_files = []
            if not os.path.exists(metadata_path):
                missing_files.append("metadata.json")
            if not os.path.exists(diff_path):
                missing_files.append("pr.diff")

            if missing_files:
                error_msg = f"ç¼ºå°‘å¿…è¦æ–‡ä»¶: {', '.join(missing_files)}"
                return {**state, "messages": messages, "error": error_msg}

        # GitHub APIæ¨¡å¼ï¼ˆå¦‚æœæœ‰tokenï¼‰
        elif state.get("github_token"):
            if not all([state.get("owner"), state.get("repo"), state.get("pr_number")]):
                error_msg = "GitHubæ¨¡å¼éœ€è¦æä¾› owner, repo, pr_number"
                return {**state, "messages": messages, "error": error_msg}

        return {**state, "messages": messages}

    def data_parsing_node(self, state: WorkflowState) -> WorkflowState:
        """æ•°æ®è§£æèŠ‚ç‚¹ - ç»Ÿä¸€çš„PRæ•°æ®è§£æ"""
        messages = state.get("messages", [])

        try:
            messages.extend(self._log_message("é˜¶æ®µ1: æ•°æ®è§£æ"))

            # åŠ è½½PRæ•°æ®
            pr_data = self._load_pr_data(state)
            diff_text = self._load_diff_content(state)

            # å¦‚æœæ˜¯æœ¬åœ°æ¨¡å¼ï¼Œä½¿ç”¨æ ‡å‡†è§£ææµç¨‹
            if state.get("pr_dir"):
                parse_results = parse_pr_data(state["pr_dir"])
                diff_ir = self._load_json(parse_results["diff_ir"])
            else:
                # GitHub APIæ¨¡å¼ï¼Œåˆ›å»ºç®€åŒ–çš„æ•°æ®ç»“æ„
                diff_ir = {
                    "summary": {
                        "files": pr_data.get("changed_files", 0),
                        "total_additions": pr_data.get("additions", 0),
                        "total_deletions": pr_data.get("deletions", 0)
                    }
                }
                parse_results = {"pr_meta_parsed": "github_api", "diff_ir": "github_api"}

            messages.extend(self._log_message(
                f"è§£æå®Œæˆ: {diff_ir.get('summary', {}).get('files', 0)}ä¸ªæ–‡ä»¶, "
                f"+{diff_ir.get('summary', {}).get('total_additions', 0)}/-{diff_ir.get('summary', {}).get('total_deletions', 0)}è¡Œå˜æ›´"
            ))

            return {
                **state,
                "messages": messages,
                "pr_data": pr_data,
                "diff_ir": diff_ir,
                "parse_results": parse_results
            }

        except Exception as e:
            error_msg = f"æ•°æ®è§£æå¤±è´¥: {str(e)}"
            messages.extend(self._log_message(f"é”™è¯¯: {error_msg}"))
            return {**state, "messages": messages, "error": error_msg}

    def risk_analysis_node(self, state: WorkflowState) -> WorkflowState:
        """é£é™©åˆ†æèŠ‚ç‚¹ - AIé©±åŠ¨çš„é£é™©è¯„ä¼°"""
        messages = state.get("messages", [])
        diff_ir = state["diff_ir"]
        llm = state["llm"]
        top_n = state.get("top_n", 20)
        batch_size = state.get("batch_size", 8)

        try:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ feature_risk_plan.json
            if state.get("pr_dir"):
                pr_dir = state["pr_dir"]
                feature_risk_plan_path = os.path.join(pr_dir, "out", "feature_risk_plan.json")

                if os.path.exists(feature_risk_plan_path):
                    logger.info(f"[Risk Analysis] å‘ç°å·²å­˜åœ¨çš„ feature_risk_plan.jsonï¼Œè·³è¿‡AIé£é™©åˆ†æ")
                    logger.info(f"[Risk Analysis] åŠ è½½æ–‡ä»¶: {feature_risk_plan_path}")

                    try:
                        feature_risk_plan = self._load_json(feature_risk_plan_path)

                        # éªŒè¯æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
                        if feature_risk_plan.get("features"):
                            summary = feature_risk_plan.get("summary", {})
                            features_count = len(feature_risk_plan.get("features", []))
                            total_hunks = summary.get("total_hunks", 0)

                            logger.info(f"[Risk Analysis] æˆåŠŸåŠ è½½å·²æœ‰çš„é£é™©è®¡åˆ’:")
                            logger.info(f"[Risk Analysis]   åŠŸèƒ½ç‰¹æ€§: {features_count}ä¸ª")
                            logger.info(f"[Risk Analysis]   æ€»hunks: {total_hunks}ä¸ª")

                            messages.extend(self._log_message(
                                f"é˜¶æ®µ2: è·³è¿‡AIé£é™©åˆ†æ (ä½¿ç”¨å·²æœ‰æ–‡ä»¶)"
                            ))
                            messages.extend(self._log_message(
                                f"å·²åŠ è½½é£é™©è®¡åˆ’: {features_count}ä¸ªåŠŸèƒ½ç‰¹æ€§, {total_hunks}ä¸ªhunks"
                            ))

                            return {
                                **state,
                                "messages": messages,
                                "feature_risk_plan": feature_risk_plan
                            }
                        else:
                            logger.warning(f"[Risk Analysis] å·²æœ‰çš„ feature_risk_plan.json æ— æœ‰æ•ˆå†…å®¹ï¼Œå°†é‡æ–°ç”Ÿæˆ")

                    except Exception as e:
                        logger.error(f"[Risk Analysis] åŠ è½½å·²æœ‰çš„ feature_risk_plan.json å¤±è´¥: {e}")
                        logger.info(f"[Risk Analysis] å°†é‡æ–°ç”Ÿæˆé£é™©è®¡åˆ’")

            messages.extend(self._log_message("é˜¶æ®µ2: AIé£é™©åˆ†æ"))

            logger.info("[Risk Analysis] å¼€å§‹AIé©±åŠ¨çš„é£é™©è¯„ä¼°")
            logger.info(f"[Risk Analysis] å‚æ•°: top_n={top_n}, batch_size={batch_size}")

            # å¦‚æœæ˜¯æœ¬åœ°æ¨¡å¼ï¼Œä½¿ç”¨æ ‡å‡†é£é™©åˆ†æ
            if state.get("pr_dir"):
                pr_dir = state["pr_dir"]
                logger.info(f"[Risk Analysis] æœ¬åœ°æ¨¡å¼ï¼ŒPRç›®å½•: {pr_dir}")

                # æ˜¾ç¤ºdiff_irçš„åŸºæœ¬ä¿¡æ¯
                files_count = len(diff_ir.get("files", []))
                total_additions = diff_ir.get("summary", {}).get("total_additions", 0)
                total_deletions = diff_ir.get("summary", {}).get("total_deletions", 0)

                logger.info(f"[Risk Analysis] Diffä¿¡æ¯: {files_count}ä¸ªæ–‡ä»¶, +{total_additions}/-{total_deletions}è¡Œ")

                feature_risk_plan = analyze_risk(
                    pr_dir=pr_dir,
                    llm=llm,
                    top_n=top_n,
                    batch_size=batch_size
                )
            else:
                # GitHub APIæ¨¡å¼ï¼Œåˆ›å»ºç®€åŒ–çš„é£é™©è®¡åˆ’
                logger.info("[Risk Analysis] GitHub APIæ¨¡å¼ï¼Œä½¿ç”¨ç®€åŒ–é£é™©è®¡åˆ’")
                feature_risk_plan = {
                    "features": [{
                        "feature_id": "github_pr_review",
                        "feature_name": "PRå®¡æ ¸",
                        "summary": f"Review PR #{state.get('pr_number', 'unknown')}",
                        "hunks": []
                    }],
                    "summary": {"total_hunks": 0}
                }

            summary = feature_risk_plan.get("summary", {})
            features_count = len(feature_risk_plan.get("features", []))
            total_hunks = summary.get("total_hunks", 0)

            logger.info(f"[Risk Analysis] é£é™©åˆ†æå®Œæˆ")
            logger.info(f"[Risk Analysis] åŠŸèƒ½ç‰¹æ€§: {features_count}ä¸ª")
            logger.info(f"[Risk Analysis] æ€»hunks: {total_hunks}ä¸ª")

            # æ˜¾ç¤ºåŠŸèƒ½æ¦‚è§ˆ
            for i, feature in enumerate(feature_risk_plan.get("features", [])[:5], 1):
                feature_name = feature.get("feature_name", f"feature_{i}")
                risk_score = feature.get("risk_overview", {}).get("max_risk_score", 0)
                hunks_count = len(feature.get("hunks", []))
                logger.info(f"[Risk Analysis] [{i}] {feature_name}: {hunks_count}ä¸ªhunks, æœ€é«˜é£é™©åˆ†æ•°{risk_score}")

            messages.extend(self._log_message(
                f"é£é™©åˆ†æå®Œæˆ: {features_count}ä¸ªåŠŸèƒ½ç‰¹æ€§, {total_hunks}ä¸ªhunks"
            ))

            return {
                **state,
                "messages": messages,
                "feature_risk_plan": feature_risk_plan
            }

        except Exception as e:
            error_msg = f"é£é™©åˆ†æå¤±è´¥: {str(e)}"
            logger.error(f"[Risk Analysis] å¼‚å¸¸: {error_msg}")
            logger.error(f"[Risk Analysis] å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            messages.extend(self._log_message(f"é”™è¯¯: {error_msg}"))
            return {**state, "messages": messages, "error": error_msg}

    # ==================== å®‰å…¨å®¡è®¡æ¨¡å¼èŠ‚ç‚¹ ====================

    def scan_task_planning_node(self, state: WorkflowState) -> WorkflowState:
        """æ‰«æä»»åŠ¡è§„åˆ’èŠ‚ç‚¹ - åœ¨Semgrepæ‰§è¡Œå‰ï¼Œç”¨LLMç”Ÿæˆæ¨¡æ¿æ‰«æä»»åŠ¡å¹¶å›å¡«åˆ°feature_risk_plan.json"""

        messages = state.get("messages", [])
        pr_dir = state.get("pr_dir")

        # ä»…æœ¬åœ°PRç›®å½•æ¨¡å¼æ‰§è¡Œï¼ˆéœ€è¦ä»“åº“æ–‡ä»¶ï¼‰
        if not pr_dir:
            messages.extend(self._log_message("è·³è¿‡æ‰«æä»»åŠ¡è§„åˆ’ï¼ˆGitHub APIæ¨¡å¼ï¼‰"))
            return {**state, "messages": messages}

        feature_risk_plan = state.get("feature_risk_plan") or {}
        diff_ir = state.get("diff_ir") or {}

        # å¦‚æœæ²¡æœ‰featureä¿¡æ¯å°±ä¸åš
        if not feature_risk_plan.get("features"):
            messages.extend(self._log_message("è·³è¿‡æ‰«æä»»åŠ¡è§„åˆ’ï¼ˆfeature_risk_plan.featuresä¸ºç©ºï¼‰"))
            return {**state, "messages": messages}

        try:
            messages.extend(self._log_message("é˜¶æ®µ2.5: ç”Ÿæˆæ¨¡æ¿æ‰«æä»»åŠ¡ï¼ˆLLM JSON æŒ‡å¯¼ï¼‰"))

            updated_plan, plan_meta = generate_feature_scan_tasks_and_update_plan(
                llm=state["llm"],
                pr_dir=pr_dir,
                feature_risk_plan=feature_risk_plan,
                diff_ir=diff_ir,
            )

            # å›å¡«state
            return {
                **state,
                "messages": messages + self._log_message(
                    f"æ‰«æä»»åŠ¡è§„åˆ’å®Œæˆ: features={plan_meta.get('features', 0)}, tasks={plan_meta.get('tasks', 0)}"
                ),
                "feature_risk_plan": updated_plan,
            }
        except Exception as e:
            error_msg = f"æ‰«æä»»åŠ¡è§„åˆ’å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            logger.error(traceback.format_exc())
            messages.extend(self._log_message(f"é”™è¯¯: {error_msg}"))
            # ä¸é˜»æ–­åç»­æ‰«æï¼ˆé™çº§ï¼‰
            return {**state, "messages": messages}

    def semgrep_scan_node(self, state: WorkflowState) -> WorkflowState:
        """æ¨¡æ¿è§„åˆ™Semgrepæ‰«æèŠ‚ç‚¹ - ä½¿ç”¨å¹¶è¡Œæ‰«æï¼ˆåŸºäºè‡ªå®šä¹‰æ¨¡æ¿è§„åˆ™çš„è¯­ä¹‰æ‰«æï¼‰"""
        messages = state.get("messages", [])
        pr_dir = state.get("pr_dir")
        diff_ir = state["diff_ir"]
        feature_risk_plan = state.get("feature_risk_plan") or {}

        # å¦‚æœæ²¡æœ‰æœ¬åœ°PRç›®å½•ï¼Œè·³è¿‡Semgrepæ‰«æ
        if not pr_dir:
            messages.extend(self._log_message("è·³è¿‡æ¨¡æ¿Semgrepæ‰«æï¼ˆGitHub APIæ¨¡å¼ï¼‰"))
            return {
                **state,
                "messages": messages,
                "semgrep_results": {"results": []},
                "evidence_store": {"template_findings": [], "semgrep_findings": []},
            }

        try:
            messages.extend(self._log_message("é˜¶æ®µ3: å¹¶è¡Œæ¨¡æ¿è§„åˆ™Semgrepæ‰«æï¼ˆ8çº¿ç¨‹å¹¶è¡Œï¼‰"))

            # è·å–æºä»£ç è·¯å¾„ï¼ˆä¼˜å…ˆä½¿ç”¨app.pyä¼ é€’çš„codebase_pathï¼‰
            source_code_path = self._get_source_code_path(state)

            # æ‰§è¡Œå¹¶è¡Œæ¨¡æ¿æ‰«æï¼Œç”± scan_task_planning_node äº§å‡ºçš„ scan_tasks é©±åŠ¨
            # æ³¨æ„ï¼šoutput_dir æŒ‡å®šè¾“å‡ºåˆ° PR å¯¼å‡ºç›®å½•çš„ out/ å­ç›®å½•ï¼Œè€Œä¸æ˜¯æºä»£ç åˆ†æ”¯ç›®å½•
            template_scan = run_parallel_semgrep_tasks(
                pr_dir=source_code_path,
                feature_risk_plan=feature_risk_plan,
                max_workers=8,
                max_findings=300,
                timeout_seconds=60,  # ä¼šè‡ªåŠ¨æ ¹æ®æ¨¡æ¿å¤æ‚åº¦è°ƒæ•´ï¼ˆ60sæˆ–180sï¼‰
                output_dir=str(Path(pr_dir) / "out")  # è¾“å‡ºåˆ°PRå¯¼å‡ºç›®å½•
            )

            if template_scan.get("success"):
                ts = template_scan.get("summary", {})
                messages.extend(self._log_message(
                    f"å¹¶è¡Œæ¨¡æ¿æ‰«æå®Œæˆ: total_tasks={ts.get('total_tasks', 0)}, "
                    f"successful={ts.get('successful_tasks', 0)}, findings={ts.get('total_findings', 0)}, "
                    f"time={ts.get('execution_time', 0):.1f}s"
                ))
            else:
                messages.extend(self._log_message(
                    f"å¹¶è¡Œæ¨¡æ¿æ‰«æå¤±è´¥: {template_scan.get('error', 'æœªçŸ¥é”™è¯¯')}"
                ))

            # æ„å»ºè¯æ®å­˜å‚¨
            evidence_store = {
                "template_findings": template_scan.get("evidence", []),
                "template_scan_success": template_scan.get("success", False),
                "template_scan_summary": template_scan.get("summary", {}),
            }

            return {
                **state,
                "messages": messages,
                "semgrep_results": {"results": template_scan.get("evidence", [])},
                "evidence_store": evidence_store,
            }

        except Exception as e:
            error_msg = f"æ¨¡æ¿Semgrepæ‰«æå¼‚å¸¸: {str(e)}"
            messages.extend(self._log_message(f"é”™è¯¯: {error_msg}"))
            return {
                **state,
                "messages": messages,
                "semgrep_results": {"results": []},
                "evidence_store": {"template_findings": [], "template_scan_success": False},
                "error": error_msg
            }

    def triage_node(self, state: WorkflowState) -> WorkflowState:
        """Triageé¢„åˆ†ç±»èŠ‚ç‚¹ - å¿«é€Ÿç­›é€‰éœ€è¦å®¡æŸ¥çš„ä»£ç """
        messages = state.get("messages", [])
        pr_dir = state.get("pr_dir")

        if not pr_dir:
            messages.extend(self._log_message("è·³è¿‡Triageé¢„åˆ†ç±»ï¼ˆGitHub APIæ¨¡å¼ï¼‰"))
            return {
                **state,
                "messages": messages,
                "triage_summary": {"total_count": 0, "to_review_count": 0, "skip_count": 0, "to_review": [], "by_priority": {}}
            }

        try:
            messages.extend(self._log_message("é˜¶æ®µ3.6: Triage é¢„åˆ†ç±»ï¼ˆæ™ºèƒ½ç­›é€‰ï¼‰"))

            llm = state["llm"]
            diff_ir = state["diff_ir"]
            feature_risk_plan = state["feature_risk_plan"]

            logger.info("[Triage] å¼€å§‹é¢„åˆ†ç±»")

            # ä½¿ç”¨çº¯è§„åˆ™æ¨¡å¼ï¼ˆå¿«é€Ÿï¼‰
            triage_result = run_triage_for_pr(
                llm=llm,
                diff_ir=diff_ir,
                feature_risk_plan=feature_risk_plan,
                use_llm=False,  # çº¯è§„åˆ™æ¨¡å¼ï¼Œå¿«é€Ÿ
                max_workers=4
            )

            total_count = triage_result.get("total_count", 0)
            to_review_count = triage_result.get("to_review_count", 0)
            skip_count = triage_result.get("skip_count", 0)

            logger.info(f"[Triage] é¢„åˆ†ç±»å®Œæˆ:")
            logger.info(f"[Triage]   æ€»è®¡: {total_count} hunks")
            logger.info(f"[Triage]   éœ€è¦å®¡æŸ¥: {to_review_count} hunks ({to_review_count/total_count*100 if total_count > 0 else 0:.1f}%)")
            logger.info(f"[Triage]   è·³è¿‡: {skip_count} hunks ({skip_count/total_count*100 if total_count > 0 else 0:.1f}%)")

            # æ˜¾ç¤ºä¼˜å…ˆçº§åˆ†å¸ƒ
            by_priority = triage_result.get("by_priority", {})
            for priority in ["P0", "P1", "P2", "P3"]:
                count = len(by_priority.get(priority, []))
                if count > 0:
                    logger.info(f"[Triage]   {priority}: {count} hunks")

            messages.extend(self._log_message(
                f"Triageå®Œæˆ: {to_review_count}ä¸ªéœ€å®¡æŸ¥, {skip_count}ä¸ªå·²è·³è¿‡"
            ))

            return {
                **state,
                "messages": messages,
                "triage_summary": triage_result
            }

        except Exception as e:
            error_msg = f"Triageé¢„åˆ†ç±»å¤±è´¥: {str(e)}"
            logger.error(f"[Triage] {error_msg}")
            logger.error(traceback.format_exc())
            messages.extend(self._log_message(f"é”™è¯¯: {error_msg}"))
            # é™çº§ï¼šè¿”å›ç©ºçš„triageç»“æœ
            return {
                **state,
                "messages": messages,
                "triage_summary": {"total_count": 0, "to_review_count": 0, "skip_count": 0, "to_review": [], "by_priority": {}},
                "error": error_msg
            }

    def cross_file_analysis_node(self, state: WorkflowState) -> WorkflowState:
        """è·¨æ–‡ä»¶åˆ†æèŠ‚ç‚¹ - è¿½è¸ªä»£ç å˜æ›´çš„ä¼ æ’­å½±å“"""
        messages = state.get("messages", [])
        pr_dir = state.get("pr_dir")

        if not pr_dir:
            messages.extend(self._log_message("è·³è¿‡è·¨æ–‡ä»¶åˆ†æï¼ˆGitHub APIæ¨¡å¼ï¼‰"))
            return {
                **state,
                "messages": messages,
                "cross_file_impact": {"files_analyzed": 0, "total_callers": 0, "high_impact_files": []}
            }

        try:
            messages.extend(self._log_message("é˜¶æ®µ3.7: è·¨æ–‡ä»¶å½±å“åˆ†æ"))

            # è·å–æºä»£ç è·¯å¾„ï¼ˆä¼˜å…ˆä½¿ç”¨app.pyä¼ é€’çš„codebase_pathï¼‰
            source_code_path = self._get_source_code_path(state)
            diff_ir = state["diff_ir"]

            logger.info(f"[è·¨æ–‡ä»¶åˆ†æ] å¼€å§‹åˆ†æè·¨æ–‡ä»¶å½±å“")
            logger.info(f"[è·¨æ–‡ä»¶åˆ†æ] æºä»£ç è·¯å¾„: {source_code_path}")

            # æ‰§è¡Œè·¨æ–‡ä»¶å½±å“åˆ†æ
            cross_impact = analyze_pr_cross_file_impact(
                repo_path=source_code_path,
                diff_ir=diff_ir
            )

            files_analyzed = cross_impact.get("files_analyzed", 0)
            total_callers = cross_impact.get("total_callers", 0)
            high_impact_files = cross_impact.get("high_impact_files", [])
            api_changes = cross_impact.get("api_changes", [])
            breaking_changes = cross_impact.get("breaking_changes", [])

            logger.info(f"[è·¨æ–‡ä»¶åˆ†æ] åˆ†æå®Œæˆ:")
            logger.info(f"[è·¨æ–‡ä»¶åˆ†æ]   åˆ†ææ–‡ä»¶: {files_analyzed}")
            logger.info(f"[è·¨æ–‡ä»¶åˆ†æ]   æ€»è°ƒç”¨å…³ç³»: {total_callers}")
            logger.info(f"[è·¨æ–‡ä»¶åˆ†æ]   é«˜å½±å“æ–‡ä»¶: {len(high_impact_files)}")
            logger.info(f"[è·¨æ–‡ä»¶åˆ†æ]   APIå˜æ›´: {len(api_changes)}")
            logger.info(f"[è·¨æ–‡ä»¶åˆ†æ]   ç ´åæ€§å˜æ›´: {len(breaking_changes)}")

            messages.extend(self._log_message(
                f"è·¨æ–‡ä»¶åˆ†æå®Œæˆ: {files_analyzed}ä¸ªæ–‡ä»¶, {total_callers}ä¸ªè°ƒç”¨å…³ç³», {len(high_impact_files)}ä¸ªé«˜å½±å“æ–‡ä»¶"
            ))

            return {
                **state,
                "messages": messages,
                "cross_file_impact": cross_impact
            }

        except Exception as e:
            error_msg = f"è·¨æ–‡ä»¶åˆ†æå¤±è´¥: {str(e)}"
            logger.error(f"[è·¨æ–‡ä»¶åˆ†æ] {error_msg}")
            logger.error(traceback.format_exc())
            messages.extend(self._log_message(f"é”™è¯¯: {error_msg}"))
            # é™çº§ï¼šè¿”å›ç©ºçš„åˆ†æç»“æœ
            return {
                **state,
                "messages": messages,
                "cross_file_impact": {"files_analyzed": 0, "total_callers": 0, "high_impact_files": [], "api_changes": [], "breaking_changes": []},
                "error": error_msg
            }


    # ==================== AIåˆ†ææ¨¡å¼èŠ‚ç‚¹ ====================

    def ai_vulnerability_analysis_node(self, state: WorkflowState) -> WorkflowState:
        """AIæ¼æ´åˆ†æèŠ‚ç‚¹"""
        messages = state.get("messages", [])

        try:
            messages.extend(self._log_message("é˜¶æ®µ6: AIæ¼æ´åˆ†æ"))

            # åˆ›å»ºç©ºçš„æŸ¥è¯¢è®¡åˆ’å’Œç»“æœï¼ˆå·²ç§»é™¤CodeQLï¼‰
            query_plan = {"queries": [], "language": "unknown"}
            codeql_results = {}

            if state.get("pr_dir"):
                # æœ¬åœ°æ¨¡å¼ï¼Œæ‰§è¡Œå®Œæ•´çš„æ¼æ´åˆ†æ
                vulnerability_analysis = analyze_vulnerabilities(
                    pr_dir=state["pr_dir"],
                    feature_risk_plan=state["feature_risk_plan"],
                    query_plan=query_plan,
                    codeql_results=codeql_results,
                    llm=state["llm"],
                    workers=state.get("max_workers", 1),
                    diff_ir=state["diff_ir"]
                )
            else:
                # GitHub APIæ¨¡å¼ï¼Œåˆ›å»ºç®€åŒ–çš„åˆ†æç»“æœ
                vulnerability_analysis = {
                    "analysis_summary": {
                        "total_features_analyzed": 1,
                        "features_with_vulnerabilities": 0,
                        "total_vulnerabilities_found": 0
                    }
                }

            summary = vulnerability_analysis.get("analysis_summary", {})
            total_vulns = summary.get("total_vulnerabilities_found", 0)

            messages.extend(self._log_message(
                f"AIæ¼æ´åˆ†æå®Œæˆ: å‘ç°{total_vulns}ä¸ªæ½œåœ¨é—®é¢˜"
            ))

            return {
                **state,
                "messages": messages,
                "vulnerability_analysis": vulnerability_analysis,
                "query_plan": query_plan
            }

        except Exception as e:
            error_msg = f"AIæ¼æ´åˆ†æå¤±è´¥: {str(e)}"
            messages.extend(self._log_message(f"é”™è¯¯: {error_msg}"))
            return {**state, "messages": messages, "error": error_msg}

    # ==================== PRå®¡æ ¸æ¨¡å¼èŠ‚ç‚¹ ====================

    def parallel_review_node(self, state: WorkflowState) -> WorkflowState:
        """
        å¹¶è¡Œè°ƒåº¦èŠ‚ç‚¹ - åŒæ—¶è¿è¡Œ Logic Agent å’Œ Enhanced Security Agent
        å®ç°å¹¶è¡Œè°ƒåº¦ï¼Œæ»¡è¶³10åˆ†é’Ÿæ‰§è¡Œç›®æ ‡
        """
        messages = state.get("messages", [])
        pr_dir = state.get("pr_dir")

        if not pr_dir:
            # GitHubæ¨¡å¼è·³è¿‡å®¡è®¡
            logger.info("[å¹¶è¡Œè°ƒåº¦] GitHubæ¨¡å¼è·³è¿‡å¹¶è¡Œå®¡è®¡")
            skip_result = {"success": True, "units_analyzed": 0, "issues_found": 0, "issues": []}
            messages.extend(self._log_message("è·³è¿‡å¹¶è¡Œå®¡è®¡ï¼ˆGitHub APIæ¨¡å¼ï¼‰"))
            return {
                **state,
                "messages": messages,
                "logic_review": skip_result,
                "security_review": skip_result
            }

        try:
            start_time = datetime.now()
            messages.extend(self._log_message("é˜¶æ®µ3.5: å¹¶è¡Œè°ƒåº¦ Logic/Security Agents"))
            logger.info("[å¹¶è¡Œè°ƒåº¦] å¼€å§‹å¹¶è¡Œå®¡è®¡")

            # å‡†å¤‡å…±äº«å‚æ•°
            llm = state["llm"]
            diff_ir = state["diff_ir"]
            feature_risk_plan = state["feature_risk_plan"]
            evidence_store = state.get("evidence_store", {})  # ğŸ†• è·å– Semgrep è¯æ®
            template_findings = evidence_store.get("template_findings", [])  # ğŸ†• æå– Semgrep å‘ç°

            # é¢„ç»Ÿè®¡
            features = feature_risk_plan.get("features", [])
            total_hunks = sum(len(f.get("hunks", [])) for f in features)

            logger.info(f"[å¹¶è¡Œè°ƒåº¦] é¢„å¤„ç†: {len(features)}ä¸ªåŠŸèƒ½, {total_hunks}ä¸ªhunks")

            # é¢„ä¼° Logic Agent å·¥ä½œé‡
            logic_relevant_hunks = 0
            for feature in features:
                for hunk in feature.get("hunks", []):
                    risk_score = int(hunk.get("risk_score", 0) or 0)
                    suggested_agents = hunk.get("suggested_agents", []) or []
                    risk_categories = hunk.get("risk_categories", []) or []

                    if (risk_score >= 60 or
                        "logic_agent" in suggested_agents or
                        "logic" in risk_categories):
                        logic_relevant_hunks += 1

            # é¢„ä¼° Security Agent å·¥ä½œé‡
            security_relevant_hunks = 0
            security_keywords = [
                "auth", "jwt", "token", "session", "cookie", "bearer",
                "permission", "rbac", "acl", "role", "admin",
                "sql", "query", "where", "select", "insert", "update", "delete",
                "exec", "eval", "system(", "popen", "runtime.exec", "subprocess",
                "deserialize", "pickle", "yaml.load", "unmarshal", "marshal",
                "http://", "https://", "url", "redirect", "callback",
                "ssrf", "upload", "multipart", "filename", "path", "../",
                "template", "render", "jinja", "freemarker", "thymeleaf",
                "secret", "apikey", "password", "credential", "private_key",
                "cors", "csrf", "xss"
            ]

            for feature in features:
                for hunk in feature.get("hunks", []):
                    risk_score = int(hunk.get("risk_score", 0) or 0)
                    suggested_agents = hunk.get("suggested_agents", []) or []
                    risk_categories = hunk.get("risk_categories", []) or []
                    hunk_text = hunk.get("hunk_text", "") or ""

                    is_security_relevant = (
                        risk_score >= 35 or  # é™ä½é£é™©é˜ˆå€¼
                        "security_agent" in suggested_agents or
                        "security" in risk_categories or
                        any(keyword in hunk_text.lower() for keyword in security_keywords)
                    )

                    if is_security_relevant:
                        security_relevant_hunks += 1

            logger.info(f"[å¹¶è¡Œè°ƒåº¦] å·¥ä½œé‡é¢„ä¼°:")
            logger.info(f"[å¹¶è¡Œè°ƒåº¦]   Logicç›¸å…³: {logic_relevant_hunks} hunks (é˜ˆå€¼:45, æœ€å¤§:12)")
            logger.info(f"[å¹¶è¡Œè°ƒåº¦]   Securityç›¸å…³: {security_relevant_hunks} hunks (é˜ˆå€¼:35, æœ€å¤§:10)")

            # å¹¶è¡Œæ‰§è¡Œä¸¤ä¸ªAgent
            results = {}
            execution_times = {}

            with ThreadPoolExecutor(max_workers=2, thread_name_prefix="Agent") as executor:
                # è·å–codebase_path
                codebase_path = state.get("codebase_path")

                # æäº¤Logic Agentä»»åŠ¡
                logic_future = executor.submit(
                    run_logic_agent_for_pr,
                    pr_dir, diff_ir, feature_risk_plan, llm, 45, 12,
                    template_findings,  # ğŸ†• æ³¨å…¥ Semgrep è¯æ®
                    codebase_path  # ğŸ†• ä¼ é€’codebase_path
                )
                logger.info("[å¹¶è¡Œè°ƒåº¦] Logic Agent ä»»åŠ¡å·²æäº¤")

                # æäº¤Security Agentä»»åŠ¡ï¼ˆé™ä½é£é™©é˜ˆå€¼ä»¥æ•è·æ›´å¤šæ½œåœ¨å®‰å…¨é—®é¢˜ï¼‰
                # ğŸ†• ä¼ é€’ Semgrep template_findings å’Œ codebase_path
                security_future = executor.submit(
                    run_enhanced_security_agent_for_pr,
                    pr_dir, diff_ir, feature_risk_plan, llm, 35, 10,  # ä»55é™åˆ°35
                    template_findings,  # ğŸ†• æ³¨å…¥ Semgrep è¯æ®
                    codebase_path  # ğŸ†• ä¼ é€’codebase_path
                )
                logger.info("[å¹¶è¡Œè°ƒåº¦] Enhanced Security Agent ä»»åŠ¡å·²æäº¤")

                # æ”¶é›†ç»“æœ
                futures = {
                    logic_future: "logic_review",
                    security_future: "security_review"
                }

                for future in as_completed(futures):
                    agent_name = futures[future]
                    try:
                        start_time_agent = datetime.now()
                        result = future.result()
                        end_time_agent = datetime.now()
                        execution_time = (end_time_agent - start_time_agent).total_seconds()

                        results[agent_name] = result
                        execution_times[agent_name] = execution_time

                        units_analyzed = result.get('units_analyzed', 0)
                        issues_found = result.get('issues_found', 0)
                        success = result.get('success', True)

                        if success:
                            logger.info(f"[å¹¶è¡Œè°ƒåº¦] {agent_name} å®Œæˆ: {units_analyzed}å•å…ƒ, {issues_found}é—®é¢˜, è€—æ—¶{execution_time:.1f}s")
                        else:
                            logger.error(f"[å¹¶è¡Œè°ƒåº¦] {agent_name} å¤±è´¥")

                    except Exception as e:
                        error_msg = f"{agent_name}æ‰§è¡Œå¼‚å¸¸: {str(e)}"
                        logger.error(f"[å¹¶è¡Œè°ƒåº¦] {error_msg}")
                        logger.error(f"[å¹¶è¡Œè°ƒåº¦] å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")

                        results[agent_name] = {
                            "success": False,
                            "error": str(e),
                            "units_analyzed": 0,
                            "issues_found": 0,
                            "issues": []
                        }
                        execution_times[agent_name] = 0

            # è®¡ç®—æ€»æ‰§è¡Œæ—¶é—´
            total_time = (datetime.now() - start_time).total_seconds()
            max_agent_time = max(execution_times.values()) if execution_times else 0

            # æ±‡æ€»ç»“æœ
            logic_review = results.get("logic_review", {"success": False, "units_analyzed": 0, "issues_found": 0})
            security_review = results.get("security_review", {"success": False, "units_analyzed": 0, "issues_found": 0})

            total_units = logic_review.get('units_analyzed', 0) + security_review.get('units_analyzed', 0)
            total_issues = logic_review.get('issues_found', 0) + security_review.get('issues_found', 0)

            # æ€§èƒ½ç»Ÿè®¡
            efficiency = (max_agent_time / total_time * 100) if total_time > 0 else 0

            logger.info(f"[å¹¶è¡Œè°ƒåº¦] å¹¶è¡Œå®¡è®¡å®Œæˆ:")
            logger.info(f"[å¹¶è¡Œè°ƒåº¦]   æ€»è€—æ—¶: {total_time:.1f}s (æœ€é•¿Agent: {max_agent_time:.1f}s)")
            logger.info(f"[å¹¶è¡Œè°ƒåº¦]   å¹¶è¡Œæ•ˆç‡: {efficiency:.1f}%")
            logger.info(f"[å¹¶è¡Œè°ƒåº¦]   æ€»åˆ†æ: {total_units}å•å…ƒ, å‘ç°{total_issues}é—®é¢˜")
            logger.info(f"[å¹¶è¡Œè°ƒåº¦]   Logic: {logic_review.get('units_analyzed', 0)}å•å…ƒ, {logic_review.get('issues_found', 0)}é—®é¢˜")
            logger.info(f"[å¹¶è¡Œè°ƒåº¦]   Security: {security_review.get('units_analyzed', 0)}å•å…ƒ, {security_review.get('issues_found', 0)}é—®é¢˜")

            # æ˜¾ç¤ºSecurity Agentçš„å·¥å…·è¯æ®ç»Ÿè®¡
            if security_review.get('success') and security_review.get('tool_evidence_summary'):
                tool_evidence = security_review.get('tool_evidence_summary', {})
                evidence_text = f", å…¥å£ç‚¹:{tool_evidence.get('with_entrypoint', 0)}, è°ƒç”¨é“¾:{tool_evidence.get('with_call_chain', 0)}"
                logger.info(f"[å¹¶è¡Œè°ƒåº¦]   Securityå·¥å…·è¯æ®: {evidence_text}")

            completion_msg = (
                f"å¹¶è¡Œå®¡è®¡å®Œæˆ: Logic {logic_review.get('issues_found', 0)}é—®é¢˜, "
                f"Security {security_review.get('issues_found', 0)}é—®é¢˜, è€—æ—¶{total_time:.1f}s"
            )
            messages.extend(self._log_message(completion_msg))

            return {
                **state,
                "messages": messages,
                "logic_review": logic_review,
                "security_review": security_review
            }

        except Exception as e:
            error_msg = f"å¹¶è¡Œè°ƒåº¦å¼‚å¸¸: {str(e)}"
            logger.error(f"[å¹¶è¡Œè°ƒåº¦] {error_msg}")
            logger.error(f"[å¹¶è¡Œè°ƒåº¦] å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")

            messages.extend(self._log_message(f"é”™è¯¯: {error_msg}"))

            error_result = {"success": False, "error": error_msg, "units_analyzed": 0, "issues_found": 0, "issues": []}
            return {
                **state,
                "messages": messages,
                "logic_review": error_result,
                "security_review": error_result,
                "error": error_msg
            }

    def logic_review_node(self, state: WorkflowState) -> WorkflowState:
        """é€»è¾‘ç¼ºé™·å®¡è®¡èŠ‚ç‚¹ - ä»…åœ¨ PR å®¡æ ¸æ¨¡å¼ä¸‹æ‰§è¡Œï¼ˆå¤‡ç”¨ï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰"""
        if state["mode"] != "pr_review":
            return state

        messages = state.get("messages", [])
        try:
            messages.extend(self._log_message("é˜¶æ®µ3.5: Logic Agent é€»è¾‘ç¼ºé™·å®¡è®¡"))

            pr_dir = state.get("pr_dir")
            if not pr_dir:
                # GitHub æ¨¡å¼å¯å…ˆä¸åšï¼ˆä½ ä¹Ÿå¯ä»¥æ‰©å±•ä¸ºä¸´æ—¶ checkoutï¼‰
                logger.info("[Logic Agent] GitHubæ¨¡å¼è·³è¿‡é€»è¾‘å®¡è®¡")
                return {**state, "messages": messages, "logic_review": {"success": True, "units_analyzed": 0, "issues_found": 0, "issues": []}}

            feature_risk_plan = state["feature_risk_plan"]
            features = feature_risk_plan.get("features", [])

            # ç»Ÿè®¡éœ€è¦åˆ†æçš„hunks
            total_hunks = sum(len(f.get("hunks", [])) for f in features)
            high_risk_hunks = 0
            logic_relevant_hunks = 0

            for feature in features:
                for hunk in feature.get("hunks", []):
                    risk_score = int(hunk.get("risk_score", 0) or 0)
                    suggested_agents = hunk.get("suggested_agents", []) or []
                    risk_categories = hunk.get("risk_categories", []) or []

                    if risk_score >= 60:
                        high_risk_hunks += 1
                    if "logic_agent" in suggested_agents or "logic" in risk_categories:
                        logic_relevant_hunks += 1

            logger.info(f"[Logic Agent] å¼€å§‹é€»è¾‘ç¼ºé™·å®¡è®¡")
            logger.info(f"[Logic Agent] æ€»åŠŸèƒ½æ•°: {len(features)}")
            logger.info(f"[Logic Agent] æ€»hunks: {total_hunks}")
            logger.info(f"[Logic Agent] é«˜é£é™©hunks (â‰¥60): {high_risk_hunks}")
            logger.info(f"[Logic Agent] é€»è¾‘ç›¸å…³hunks: {logic_relevant_hunks}")
            logger.info(f"[Logic Agent] ä½¿ç”¨é£é™©é˜ˆå€¼: 60, æœ€å¤§åˆ†æå•å…ƒ: 12")

            logic_review = run_logic_agent_for_pr(
                pr_dir=pr_dir,
                diff_ir=state["diff_ir"],
                feature_risk_plan=state["feature_risk_plan"],
                llm=state["llm"],
                risk_threshold=60,
                max_units=12
            )

            # è¯¦ç»†è®°å½•åˆ†æç»“æœ
            units_analyzed = logic_review.get('units_analyzed', 0)
            issues_found = logic_review.get('issues_found', 0)
            errors = logic_review.get('errors', [])
            success = logic_review.get('success', True)

            if success:
                logger.info(f"[Logic Agent] é€»è¾‘å®¡è®¡å®Œæˆ")
                logger.info(f"[Logic Agent] å®é™…åˆ†æå•å…ƒ: {units_analyzed}")
                logger.info(f"[Logic Agent] å‘ç°é€»è¾‘é—®é¢˜: {issues_found}")

                # æ˜¾ç¤ºé—®é¢˜åˆ†å¸ƒ
                issues = logic_review.get('issues', [])
                if issues:
                    severity_counts = {}
                    for issue in issues:
                        if issue.get('result') == 'ISSUE' and issue.get('issue'):
                            severity = issue.get('issue', {}).get('severity', 'unknown')
                            severity_counts[severity] = severity_counts.get(severity, 0) + 1

                    if severity_counts:
                        logger.info(f"[Logic Agent] é—®é¢˜ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ: {severity_counts}")

                    # æ˜¾ç¤ºå‰å‡ ä¸ªé—®é¢˜çš„æ ‡é¢˜
                    for i, issue in enumerate(issues[:3], 1):
                        if issue.get('result') == 'ISSUE' and issue.get('issue'):
                            title = issue.get('issue', {}).get('title', 'æœªçŸ¥é—®é¢˜')
                            file_path = issue.get('_meta', {}).get('file_path', 'æœªçŸ¥æ–‡ä»¶')
                            logger.info(f"[Logic Agent] é—®é¢˜{i}: {title} (æ–‡ä»¶: {file_path})")

                if errors:
                    logger.warning(f"[Logic Agent] æ‰§è¡Œé”™è¯¯: {len(errors)}ä¸ª")
                    for error in errors[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                        logger.warning(f"[Logic Agent] é”™è¯¯: {error}")

                messages.extend(self._log_message(
                    f"Logic Agent å®Œæˆ: åˆ†æ{units_analyzed}ä¸ªç‰‡æ®µï¼Œå‘ç°{issues_found}ä¸ªé€»è¾‘é—®é¢˜"
                ))
            else:
                error_msg = logic_review.get('error', 'æœªçŸ¥é”™è¯¯')
                logger.error(f"[Logic Agent] æ‰§è¡Œå¤±è´¥: {error_msg}")
                messages.extend(self._log_message(f"Logic Agent å¤±è´¥: {error_msg}"))

            return {**state, "messages": messages, "logic_review": logic_review}

        except Exception as e:
            err = f"Logic Agent å¤±è´¥: {str(e)}"
            logger.error(f"[Logic Agent] å¼‚å¸¸: {err}")
            logger.error(f"[Logic Agent] å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            messages.extend(self._log_message(f"é”™è¯¯: {err}"))
            return {**state, "messages": messages, "logic_review": {"success": False, "error": err}}

    def security_review_node(self, state: WorkflowState) -> WorkflowState:
        """å®‰å…¨æ¼æ´å®¡è®¡èŠ‚ç‚¹ - ä»…åœ¨ PR å®¡æ ¸æ¨¡å¼ä¸‹æ‰§è¡Œï¼Œä½¿ç”¨å·¥å…·å¢å¼ºç‰ˆ"""
        if state["mode"] != "pr_review":
            return state

        messages = state.get("messages", [])
        try:
            messages.extend(self._log_message("é˜¶æ®µ3.6: Enhanced Security Agent å·¥å…·å¢å¼ºå®‰å…¨æ¼æ´å®¡è®¡"))

            pr_dir = state.get("pr_dir")
            if not pr_dir:
                # GitHub æ¨¡å¼å¯å…ˆä¸åšï¼ˆä½ ä¹Ÿå¯ä»¥æ‰©å±•ä¸ºä¸´æ—¶ checkoutï¼‰
                logger.info("[Enhanced Security Agent] GitHubæ¨¡å¼è·³è¿‡å®‰å…¨å®¡è®¡")
                return {**state, "messages": messages, "security_review": {"success": True, "units_analyzed": 0, "issues_found": 0, "issues": []}}

            feature_risk_plan = state["feature_risk_plan"]
            features = feature_risk_plan.get("features", [])

            # ç»Ÿè®¡éœ€è¦åˆ†æçš„hunks
            total_hunks = sum(len(f.get("hunks", [])) for f in features)
            security_relevant_hunks = 0
            security_keyword_hunks = 0
            symbol_named_hunks = 0

            for feature in features:
                for h in feature.get("hunks", []):
                    risk_score = int(h.get("risk_score", 0) or 0)
                    suggested_agents = h.get("suggested_agents", []) or []
                    risk_categories = h.get("risk_categories", []) or []
                    symbol_name = h.get("symbol_name", "")

                    is_security_relevant = (
                        risk_score >= 35 or  # é™ä½é£é™©é˜ˆå€¼
                        "security_agent" in suggested_agents or
                        "security" in risk_categories
                    )

                    if is_security_relevant:
                        security_relevant_hunks += 1

                    # æ£€æŸ¥å®‰å…¨å…³é”®å­—ï¼ˆä¸é£é™©åˆ†æ•°æ¡ä»¶åˆ†å¼€è®¡ç®—ï¼‰
                    hunk_text = h.get("hunk_text", "") or ""
                    has_security_keywords = any(keyword in hunk_text.lower() for keyword in [
                        "auth", "jwt", "token", "session", "cookie", "bearer",
                        "permission", "rbac", "acl", "role", "admin",
                        "sql", "query", "where", "select", "insert", "update", "delete",
                        "exec", "eval", "system(", "popen", "runtime.exec", "subprocess",
                        "deserialize", "pickle", "yaml.load", "unmarshal", "marshal",
                        "http://", "https://", "url", "redirect", "callback",
                        "ssrf", "upload", "multipart", "filename", "path", "../",
                        "template", "render", "jinja", "freemarker", "thymeleaf",
                        "secret", "apikey", "password", "credential", "private_key",
                        "cors", "csrf", "xss"
                    ])

                    if has_security_keywords:
                        security_keyword_hunks += 1

                    # æ£€æŸ¥æ˜¯å¦æœ‰ç¬¦å·åï¼ˆå·¥å…·å¢å¼ºç‰ˆçš„å…³é”®ç‰¹å¾ï¼‰
                    if symbol_name:
                        symbol_named_hunks += 1

            logger.info(f"[Enhanced Security Agent] å¼€å§‹å·¥å…·å¢å¼ºå®‰å…¨æ¼æ´å®¡è®¡")
            logger.info(f"[Enhanced Security Agent] æ€»åŠŸèƒ½æ•°: {len(features)}")
            logger.info(f"[Enhanced Security Agent] æ€»hunks: {total_hunks}")
            logger.info(f"[Enhanced Security Agent] å®‰å…¨ç›¸å…³hunks: {security_relevant_hunks}")
            logger.info(f"[Enhanced Security Agent] å…³é”®å­—å‘½ä¸­hunks: {security_keyword_hunks}")
            logger.info(f"[Enhanced Security Agent] æœ‰ç¬¦å·åhunks: {symbol_named_hunks}")
            logger.info(f"[Enhanced Security Agent] ä½¿ç”¨é£é™©é˜ˆå€¼: 35, æœ€å¤§åˆ†æå•å…ƒ: 10")

            # ğŸš€ ä½¿ç”¨å·¥å…·å¢å¼ºç‰ˆSecurity Agentï¼ˆé™ä½é£é™©é˜ˆå€¼ï¼‰
            security_review = run_enhanced_security_agent_for_pr(
                pr_dir=pr_dir,
                diff_ir=state["diff_ir"],
                feature_risk_plan=state["feature_risk_plan"],
                llm=state["llm"],
                risk_threshold=35,  # ä»55é™åˆ°35
                max_units=10
            )

            # è¯¦ç»†è®°å½•åˆ†æç»“æœ
            units_analyzed = security_review.get('units_analyzed', 0)
            issues_found = security_review.get('issues_found', 0)
            errors = security_review.get('errors', [])
            success = security_review.get('success', True)
            tool_evidence_summary = security_review.get('tool_evidence_summary', {})

            if success:
                logger.info(f"[Enhanced Security Agent] å·¥å…·å¢å¼ºå®‰å…¨å®¡è®¡å®Œæˆ")
                logger.info(f"[Enhanced Security Agent] å®é™…åˆ†æå•å…ƒ: {units_analyzed}")
                logger.info(f"[Enhanced Security Agent] å‘ç°å®‰å…¨æ¼æ´: {issues_found}")

                # ğŸš€ æ˜¾ç¤ºå·¥å…·è¯æ®ç»Ÿè®¡
                if tool_evidence_summary:
                    logger.info(f"[Enhanced Security Agent] å·¥å…·è¯æ®ç»Ÿè®¡:")
                    logger.info(f"[Enhanced Security Agent]   æ€»å•å…ƒ: {tool_evidence_summary.get('total_units', 0)}")
                    logger.info(f"[Enhanced Security Agent]   æœ‰å…¥å£ç‚¹: {tool_evidence_summary.get('with_entrypoint', 0)}")
                    logger.info(f"[Enhanced Security Agent]   æœ‰è°ƒç”¨é“¾: {tool_evidence_summary.get('with_call_chain', 0)}")
                    logger.info(f"[Enhanced Security Agent]   æœ‰æ¡†æ¶è·¯ç”±: {tool_evidence_summary.get('with_framework_routes', 0)}")
                    logger.info(f"[Enhanced Security Agent]   æœ‰ä¸Šä¸‹æ–‡: {tool_evidence_summary.get('with_context', 0)}")

                # æ˜¾ç¤ºæ¼æ´åˆ†å¸ƒ
                issues = security_review.get('issues', [])
                if issues:
                    severity_counts = {}
                    cwe_counts = {}
                    confidence_scores = []

                    for issue in issues:
                        issue_data = issue.get('issue', {})
                        if issue_data:
                            severity = issue_data.get('severity', 'unknown')
                            severity_counts[severity] = severity_counts.get(severity, 0) + 1

                            cwe_list = issue_data.get('cwe', [])
                            for cwe in cwe_list:
                                cwe_counts[cwe] = cwe_counts.get(cwe, 0) + 1

                        # ğŸš€ ç»Ÿè®¡å·¥å…·è¯æ®ç½®ä¿¡åº¦
                        tool_evidence = issue.get('tool_evidence', {})
                        confidence_score = tool_evidence.get('summary', {}).get('confidence_score', 0)
                        if confidence_score > 0:
                            confidence_scores.append(confidence_score)

                    if severity_counts:
                        logger.info(f"[Enhanced Security Agent] æ¼æ´ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ: {severity_counts}")
                    if cwe_counts:
                        logger.info(f"[Enhanced Security Agent] CWEç±»å‹åˆ†å¸ƒ: {cwe_counts}")
                    if confidence_scores:
                        avg_confidence = sum(confidence_scores) / len(confidence_scores)
                        logger.info(f"[Enhanced Security Agent] å¹³å‡å·¥å…·è¯æ®ç½®ä¿¡åº¦: {avg_confidence:.1f}/100")

                    # æ˜¾ç¤ºå‰å‡ ä¸ªæ¼æ´çš„æ ‡é¢˜ï¼ˆåŒ…å«ç½®ä¿¡åº¦ï¼‰
                    for i, issue in enumerate(issues[:3], 1):
                        issue_data = issue.get('issue', {})
                        if issue_data:
                            title = issue_data.get('title', 'æœªçŸ¥å®‰å…¨æ¼æ´')
                            file_path = issue.get('_meta', {}).get('file_path', 'æœªçŸ¥æ–‡ä»¶')
                            symbol_name = issue.get('_meta', {}).get('symbol_name', '')
                            tool_evidence = issue.get('tool_evidence', {})
                            confidence = tool_evidence.get('summary', {}).get('confidence_score', 0)

                            display_info = f"æ¼æ´{i}: {title} (æ–‡ä»¶: {file_path}"
                            if symbol_name:
                                display_info += f", ç¬¦å·: {symbol_name}"
                            if confidence > 0:
                                display_info += f", ç½®ä¿¡åº¦: {confidence}/100"
                            display_info += ")"

                            logger.info(f"[Enhanced Security Agent] {display_info}")

                if errors:
                    logger.warning(f"[Enhanced Security Agent] æ‰§è¡Œé”™è¯¯: {len(errors)}ä¸ª")
                    for error in errors[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                        logger.warning(f"[Enhanced Security Agent] é”™è¯¯: {error}")

                evidence_summary_text = ""
                if tool_evidence_summary.get('with_entrypoint', 0) > 0:
                    evidence_summary_text += f", å…¥å£ç‚¹è¯æ®: {tool_evidence_summary.get('with_entrypoint', 0)}"
                if tool_evidence_summary.get('with_call_chain', 0) > 0:
                    evidence_summary_text += f", è°ƒç”¨é“¾è¯æ®: {tool_evidence_summary.get('with_call_chain', 0)}"

                messages.extend(self._log_message(
                    f"Enhanced Security Agent å®Œæˆ: åˆ†æ{units_analyzed}ä¸ªç‰‡æ®µï¼Œå‘ç°{issues_found}ä¸ªå®‰å…¨æ¼æ´{evidence_summary_text}"
                ))
            else:
                error_msg = security_review.get('error', 'æœªçŸ¥é”™è¯¯')
                logger.error(f"[Enhanced Security Agent] æ‰§è¡Œå¤±è´¥: {error_msg}")
                messages.extend(self._log_message(f"Enhanced Security Agent å¤±è´¥: {error_msg}"))

            return {**state, "messages": messages, "security_review": security_review}

        except Exception as e:
            err = f"Enhanced Security Agent å¤±è´¥: {str(e)}"
            logger.error(f"[Enhanced Security Agent] å¼‚å¸¸: {err}")
            logger.error(f"[Enhanced Security Agent] å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            messages.extend(self._log_message(f"é”™è¯¯: {err}"))
            return {**state, "messages": messages, "security_review": {"success": False, "error": err}}

    def generate_todolist_node(self, state: WorkflowState) -> WorkflowState:
        """TODOåˆ—è¡¨ç”ŸæˆèŠ‚ç‚¹"""
        messages = state.get("messages", [])

        try:
            messages.extend(self._log_message("é˜¶æ®µ5: ç”Ÿæˆå®¡æ ¸TODOåˆ—è¡¨"))

            # åŸºäºé£é™©åˆ†æç”Ÿæˆç®€åŒ–çš„TODOåˆ—è¡¨
            feature_risk_plan = state["feature_risk_plan"]
            todo_list = []

            for i, feature in enumerate(feature_risk_plan.get("features", []), 1):
                todo_list.append({
                    "id": str(i),
                    "name": f"å®¡æ ¸ {feature.get('feature_name', 'åŠŸèƒ½')}",
                    "desc": f"å®¡æ ¸åŠŸèƒ½: {feature.get('summary', '')}",
                    "status": "pending",
                    "depends_on": [],
                    "inputs": {},
                    "outputs": {}
                })

            messages.extend(self._log_message(f"ç”Ÿæˆ{len(todo_list)}ä¸ªå®¡æ ¸ä»»åŠ¡"))

            return {
                **state,
                "messages": messages,
                "todo_list": todo_list,
                "planned": True
            }

        except Exception as e:
            error_msg = f"TODOåˆ—è¡¨ç”Ÿæˆå¤±è´¥: {str(e)}"
            messages.extend(self._log_message(f"é”™è¯¯: {error_msg}"))
            return {**state, "messages": messages, "error": error_msg}

    # ==================== æœ€ç»ˆæŠ¥å‘ŠèŠ‚ç‚¹ ====================

    def final_report_node(self, state: WorkflowState) -> WorkflowState:
        """æœ€ç»ˆæŠ¥å‘ŠèŠ‚ç‚¹"""
        messages = state.get("messages", [])

        try:
            messages.extend(self._log_message("æœ€ç»ˆé˜¶æ®µ: ç”ŸæˆæŠ¥å‘Š"))

            # ç»Ÿä¸€æŠ¥å‘Šæ ¼å¼ï¼ŒåŒ…å«æ‰€æœ‰åˆ†æç»“æœ
            logic_review = state.get("logic_review", {})
            logic_issues = (logic_review.get("issues") or []) if logic_review.get("success") else []

            security_review = state.get("security_review", {})
            security_issues = (security_review.get("issues") or []) if security_review.get("success") else []

            # åˆå¹¶æ‰€æœ‰åˆ†æç»“æœ
            final_report = {
                "workflow_summary": {
                    "execution_time": datetime.now().isoformat(),
                    "total_tasks": len(state.get("todo_list", [])),
                    "completed_tasks": len([t for t in state.get("todo_list", []) if t.get("status") == "completed"])
                },
                "pr_info": {
                    "title": state.get("pr_data", {}).get("title", "Unknown PR"),
                    "author": state.get("pr_data", {}).get("user", {}).get("login", "Unknown"),
                    "state": state.get("pr_data", {}).get("state", "Unknown"),
                    "additions": state.get("pr_data", {}).get("additions", 0),
                    "deletions": state.get("pr_data", {}).get("deletions", 0)
                },
                "semgrep_summary": {
                    "files_scanned": state.get("semgrep_results", {}).get("summary", {}).get("files_scanned", 0),
                    "findings": state.get("semgrep_results", {}).get("summary", {}).get("filtered_findings", 0)
                },
                # ğŸ†• Triageç­›é€‰ç»“æœ
                "triage_summary": state.get("triage_summary", {
                    "total_count": 0,
                    "to_review_count": 0,
                    "skip_count": 0,
                    "by_priority": {}
                }),
                # ğŸ†• è·¨æ–‡ä»¶åˆ†æç»“æœ
                "cross_file_impact": state.get("cross_file_impact", {
                    "files_analyzed": 0,
                    "total_callers": 0,
                    "high_impact_files": [],
                    "api_changes": [],
                    "breaking_changes": []
                }),
                "vulnerability_analysis": state.get("vulnerability_analysis", {
                    "analysis_summary": {"total_vulnerabilities_found": 0}
                }),
                "review_summary": {
                    "total_tasks": len(state.get("todo_list", [])),
                    "completed_tasks": len([t for t in state.get("todo_list", []) if t.get("status") == "completed"]),
                    "recommendation": "approved" if len([t for t in state.get("todo_list", []) if t.get("status") == "pending"]) == 0 else "needs_review"
                },
                "logic_review": {
                    "success": logic_review.get("success", True),
                    "units_analyzed": logic_review.get("units_analyzed", 0),
                    "issues_found": logic_review.get("issues_found", 0),
                    "issues": logic_issues[:20],  # é˜²æ­¢å¤ªé•¿
                    "errors": logic_review.get("errors", [])[:10]
                },
                "security_review": {
                    "success": security_review.get("success", True),
                    "units_analyzed": security_review.get("units_analyzed", 0),
                    "issues_found": security_review.get("issues_found", 0),
                    "issues": security_issues[:20],  # é˜²æ­¢å¤ªé•¿
                    "errors": security_review.get("errors", [])[:10]
                }
            }

            # ä¿å­˜æŠ¥å‘Š
            if state.get("pr_dir"):
                output_path = os.path.join(state["pr_dir"], "out", "final_report.json")
                self._save_json(output_path, final_report)
                messages.extend(self._log_message(f"æŠ¥å‘Šå·²ä¿å­˜: {output_path}"))

            return {
                **state,
                "messages": messages,
                "final_report": final_report
            }

        except Exception as e:
            error_msg = f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"
            messages.extend(self._log_message(f"é”™è¯¯: {error_msg}"))
            return {**state, "messages": messages, "error": error_msg}

    def _should_continue_execution(self, state: WorkflowState) -> str:
        """å†³å®šæ‰§è¡Œè·¯å¾„ - ç»Ÿä¸€å·¥ä½œæµåºåˆ—"""
        if state.get("error"):
            return "end"

        if "final_report" in state:
            return "end"

        # ç»Ÿä¸€å·¥ä½œæµåºåˆ—: START â†’ start_node â†’ initialization_node â†’ data_parsing_node
        # â†’ risk_analysis_node â†’ semgrep_scan_node â†’ triage_node â†’ cross_file_analysis_node
        # â†’ parallel_review_node â†’ generate_todolist_node â†’ ai_vulnerability_analysis_node â†’ final_report_node â†’ END

        # åˆå§‹åŒ–é˜¶æ®µï¼ˆä»…æœ¬åœ°æ¨¡å¼ï¼‰
        if state.get("pr_dir") and "initialization_result" not in state:
            return "initialization"
        # æ•°æ®è§£æé˜¶æ®µ
        elif "parse_results" not in state:
            return "data_parsing"
        # é£é™©åˆ†æé˜¶æ®µ
        elif "feature_risk_plan" not in state:
            return "risk_analysis"
        # Semgrepæ‰«æé˜¶æ®µ
        elif "semgrep_results" not in state:
            # åœ¨æ‰§è¡ŒSemgrepä¹‹å‰ï¼Œå…ˆç”¨LLMç”Ÿæˆæ¨¡æ¿æ‰«æä»»åŠ¡ï¼ˆä»…æ‰§è¡Œä¸€æ¬¡ï¼‰
            plan = state.get("feature_risk_plan") or {}
            planner_meta = plan.get("scan_task_planner", {}) if isinstance(plan, dict) else {}
            if plan.get("features") and not planner_meta.get("generated", False):
                return "scan_task_planning"
            return "semgrep_scan"
        # ğŸ†• Triageé¢„åˆ†ç±»é˜¶æ®µ
        elif "triage_summary" not in state:
            return "triage"
        # ğŸ†• è·¨æ–‡ä»¶åˆ†æé˜¶æ®µ
        elif "cross_file_impact" not in state:
            return "cross_file_analysis"
        # å¹¶è¡ŒAgentåˆ†æé˜¶æ®µ
        elif "logic_review" not in state and "security_review" not in state:
            return "parallel_review"
        # TODOåˆ—è¡¨ç”Ÿæˆé˜¶æ®µ
        elif "todo_list" not in state:
            return "generate_todolist"
        # AIæ¼æ´åˆ†æé˜¶æ®µ
        elif "vulnerability_analysis" not in state:
            return "ai_vulnerability_analysis"
        else:
            return "final_report"

    def _build_workflow(self) -> StateGraph:
        """æ„å»ºç»Ÿä¸€å·¥ä½œæµå›¾"""
        builder = StateGraph(WorkflowState)

        # æ·»åŠ èŠ‚ç‚¹
        builder.add_node("start", self.start_node)
        builder.add_node("initialization", self.initialization_node)
        builder.add_node("data_parsing", self.data_parsing_node)
        builder.add_node("risk_analysis", self.risk_analysis_node)
        builder.add_node("scan_task_planning", self.scan_task_planning_node)
        builder.add_node("semgrep_scan", self.semgrep_scan_node)
        builder.add_node("triage", self.triage_node)  # ğŸ†•
        builder.add_node("cross_file_analysis", self.cross_file_analysis_node)  # ğŸ†•
        builder.add_node("parallel_review", self.parallel_review_node)
        builder.add_node("generate_todolist", self.generate_todolist_node)
        builder.add_node("ai_vulnerability_analysis", self.ai_vulnerability_analysis_node)
        builder.add_node("final_report", self.final_report_node)

        # æ·»åŠ è¾¹
        builder.add_edge(START, "start")

        # æ·»åŠ æ¡ä»¶è¾¹ - æŒ‰ç…§ç»Ÿä¸€å·¥ä½œæµåºåˆ—
        builder.add_conditional_edges(
            "start",
            self._should_continue_execution,
            {
                "initialization": "initialization",
                "data_parsing": "data_parsing",
                "end": END
            }
        )

        builder.add_conditional_edges(
            "initialization",
            self._should_continue_execution,
            {
                "data_parsing": "data_parsing",
                "end": END
            }
        )

        builder.add_conditional_edges(
            "data_parsing",
            self._should_continue_execution,
            {
                "risk_analysis": "risk_analysis",
                "end": END
            }
        )

        builder.add_conditional_edges(
            "risk_analysis",
            self._should_continue_execution,
            {
                "scan_task_planning": "scan_task_planning",
                "semgrep_scan": "semgrep_scan",
                "end": END
            }
        )

        builder.add_conditional_edges(
            "scan_task_planning",
            self._should_continue_execution,
            {
                "semgrep_scan": "semgrep_scan",
                "end": END
            }
        )

        builder.add_conditional_edges(
            "semgrep_scan",
            self._should_continue_execution,
            {
                "triage": "triage",  # ğŸ†•
                "parallel_review": "parallel_review",
                "end": END
            }
        )

        # ğŸ†• Triageåçš„æ¡ä»¶è¾¹
        builder.add_conditional_edges(
            "triage",
            self._should_continue_execution,
            {
                "cross_file_analysis": "cross_file_analysis",
                "parallel_review": "parallel_review",
                "end": END
            }
        )

        # ğŸ†• è·¨æ–‡ä»¶åˆ†æåçš„æ¡ä»¶è¾¹
        builder.add_conditional_edges(
            "cross_file_analysis",
            self._should_continue_execution,
            {
                "parallel_review": "parallel_review",
                "end": END
            }
        )

        builder.add_conditional_edges(
            "parallel_review",
            self._should_continue_execution,
            {
                "generate_todolist": "generate_todolist",
                "end": END
            }
        )

        builder.add_conditional_edges(
            "generate_todolist",
            self._should_continue_execution,
            {
                "ai_vulnerability_analysis": "ai_vulnerability_analysis",
                "end": END
            }
        )

        builder.add_conditional_edges(
            "ai_vulnerability_analysis",
            self._should_continue_execution,
            {
                "final_report": "final_report",
                "end": END
            }
        )

        builder.add_edge("final_report", END)

        return builder.compile()

    def run(
        self,
        pr_dir: Optional[str] = None,
        codebase_path: Optional[str] = None,  # ğŸ†• å…‹éš†çš„æºä»£ç è·¯å¾„
        github_token: Optional[str] = None,
        owner: Optional[str] = None,
        repo: Optional[str] = None,
        pr_number: Optional[int] = None,
        top_n: int = 20,
        batch_size: int = 8,
        max_workers: int = 1
    ) -> Dict[str, Any]:
        """
        è¿è¡Œç»Ÿä¸€å·¥ä½œæµ

        Args:
            pr_dir: æœ¬åœ°PRæ•°æ®ç›®å½•ï¼ˆæœ¬åœ°æ¨¡å¼å¿…éœ€ï¼‰
            codebase_path: å…‹éš†çš„æºä»£ç è·¯å¾„ï¼ˆæ¨èä»app.pyä¼ é€’ï¼‰
            github_token: GitHub API tokenï¼ˆGitHubæ¨¡å¼å¿…éœ€ï¼‰
            owner: GitHubä»“åº“æ‰€æœ‰è€…ï¼ˆGitHubæ¨¡å¼å¿…éœ€ï¼‰
            repo: GitHubä»“åº“åï¼ˆGitHubæ¨¡å¼å¿…éœ€ï¼‰
            pr_number: PRç¼–å·ï¼ˆGitHubæ¨¡å¼å¿…éœ€ï¼‰
            top_n: é‡ç‚¹å…³æ³¨æ•°é‡
            batch_size: æ‰¹æ¬¡å¤§å°
            max_workers: å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°

        Returns:
            æœ€ç»ˆåˆ†ææŠ¥å‘Š
        """
        print("=" * 60)
        print("ğŸš€ Wise Code Watchers - ç»Ÿä¸€å·¥ä½œæµç³»ç»Ÿ")
        print("=" * 60)

        if not pr_dir and not github_token:
            print("âŒ éœ€è¦æä¾› pr_dirï¼ˆæœ¬åœ°æ¨¡å¼ï¼‰æˆ– github_tokenï¼ˆGitHubæ¨¡å¼ï¼‰å‚æ•°")
            return {"error": "éœ€è¦æä¾› pr_dirï¼ˆæœ¬åœ°æ¨¡å¼ï¼‰æˆ– github_tokenï¼ˆGitHubæ¨¡å¼ï¼‰å‚æ•°"}

        # åˆå§‹åŒ–çŠ¶æ€
        init_state: WorkflowState = {
            "messages": [HumanMessage(content="å¼€å§‹ç»Ÿä¸€å·¥ä½œæµ")],
            "pr_dir": pr_dir,
            "codebase_path": codebase_path,  # ğŸ†• ä¼ é€’codebase_path
            "llm": self.llm,
            "github_token": github_token,
            "owner": owner,
            "repo": repo,
            "pr_number": pr_number,
            "top_n": top_n,
            "batch_size": batch_size,
            "max_workers": max_workers,
        }

        try:
            final_state = self.graph.invoke(init_state, config={"recursion_limit": 50})

            if final_state.get("error"):
                print(f"\nâŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {final_state['error']}")
                return final_state.get("final_report", {"error": final_state['error']})

            print("\nâœ… å·¥ä½œæµæ‰§è¡ŒæˆåŠŸå®Œæˆï¼")

            # æ˜¾ç¤ºæ‘˜è¦
            final_report = final_state.get("final_report", {})
            self._print_summary(final_report)

            return final_report

        except Exception as e:
            print(f"\nğŸ’¥ å·¥ä½œæµæ‰§è¡Œå¼‚å¸¸: {str(e)}")
            return {"error": str(e)}

    def _print_summary(self, final_report: Dict[str, Any]) -> None:
        """æ‰“å°æ‰§è¡Œæ‘˜è¦ - ç»Ÿä¸€æŠ¥å‘Šæ ¼å¼"""
        print("\nğŸ“Š æ‰§è¡Œæ‘˜è¦:")
        print("-" * 40)

        # PRä¿¡æ¯
        pr_info = final_report.get("pr_info", {})
        if pr_info.get("title"):
            print(f"âœ… PRæ ‡é¢˜: {pr_info.get('title', 'N/A')}")
            print(f"ğŸ“Š å˜æ›´: +{pr_info.get('additions', 0)}/-{pr_info.get('deletions', 0)}")

        # Semgrepæ‰«æç»“æœ
        semgrep_summary = final_report.get("semgrep_summary", {})
        print(f"ğŸ” Semgrepæ‰«æ: {semgrep_summary.get('files_scanned', 0)}ä¸ªæ–‡ä»¶, {semgrep_summary.get('findings', 0)}ä¸ªå‘ç°")

        # ğŸ†• Triageç­›é€‰ç»“æœ
        triage_summary = final_report.get("triage_summary", {})
        if triage_summary.get("total_count", 0) > 0:
            print(f"ğŸ¯ Triageç­›é€‰: {triage_summary.get('total_count', 0)}ä¸ªæ€»è®¡, {triage_summary.get('to_review_count', 0)}ä¸ªéœ€å®¡æŸ¥, {triage_summary.get('skip_count', 0)}ä¸ªå·²è·³è¿‡")

        # ğŸ†• è·¨æ–‡ä»¶åˆ†æç»“æœ
        cross_impact = final_report.get("cross_file_impact", {})
        if cross_impact.get("files_analyzed", 0) > 0:
            print(f"ğŸ”— è·¨æ–‡ä»¶åˆ†æ: {cross_impact.get('files_analyzed', 0)}ä¸ªæ–‡ä»¶, {cross_impact.get('total_callers', 0)}ä¸ªè°ƒç”¨å…³ç³», {len(cross_impact.get('high_impact_files', []))}ä¸ªé«˜å½±å“æ–‡ä»¶")

        # å¹¶è¡ŒAgentåˆ†æç»“æœ
        logic_review = final_report.get("logic_review", {})
        security_review = final_report.get("security_review", {})
        print(f"ğŸ§  é€»è¾‘å®¡è®¡: {logic_review.get('units_analyzed', 0)}ä¸ªç‰‡æ®µï¼Œ{logic_review.get('issues_found', 0)}ä¸ªé—®é¢˜")
        print(f"ğŸ”’ å®‰å…¨å®¡è®¡: {security_review.get('units_analyzed', 0)}ä¸ªç‰‡æ®µï¼Œ{security_review.get('issues_found', 0)}ä¸ªæ¼æ´")

        # AIæ¼æ´åˆ†æç»“æœ
        vuln_analysis = final_report.get("vulnerability_analysis", {})
        analysis_summary = vuln_analysis.get("analysis_summary", {})
        print(f"ğŸ¤– AIæ¼æ´åˆ†æ: {analysis_summary.get('total_features_analyzed', 0)}ä¸ªåŠŸèƒ½, {analysis_summary.get('total_vulnerabilities_found', 0)}ä¸ªæ¼æ´")

        # TODOåˆ—è¡¨ç»“æœ
        review_summary = final_report.get("review_summary", {})
        if review_summary.get("total_tasks", 0) > 0:
            print(f"ğŸ“‹ å®¡æ ¸ä»»åŠ¡: {review_summary.get('total_tasks', 0)}ä¸ª, {review_summary.get('completed_tasks', 0)}ä¸ªå®Œæˆ")
            print(f"ğŸ¯ å®¡æ ¸å»ºè®®: {review_summary.get('recommendation', 'N/A')}")

        print("-" * 40)