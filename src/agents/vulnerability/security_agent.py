"""
Security Analysis Agent - Tool-enhanced security analysis with Semgrep evidence.

Supports two modes:
- Legacy mode: Uses feature_points + changed_files (original behavior)
- Enhanced mode: Uses diff_ir + feature_risk_plan (tool-enhanced analysis)
"""

import asyncio
import json
import logging
import os
import re
from typing import Optional

from langchain_openai import ChatOpenAI

from src.agents.base import BaseAgent, AgentResult
from src.config import Config
from src.knowledge.vulnerability_kb import VulnerabilityKB
from src.tools.security_scanner import SecurityScannerTool
from src.output.models import Bug, BugType, Severity, SecurityAnalysis, FeaturePoint

logger = logging.getLogger(__name__)


class SecurityAnalysisAgent(BaseAgent):
    """Security analysis agent with dual-mode support (legacy + enhanced)."""

    name = "security_analysis_agent"
    description = "Tool-enhanced security analysis with Semgrep evidence"

    def __init__(
        self,
        llm: Optional[ChatOpenAI] = None,
        vulnerability_kb: Optional[VulnerabilityKB] = None,
        security_scanner: Optional[SecurityScannerTool] = None,
        verbose: bool = False,
        risk_threshold: Optional[int] = None,
        max_units: Optional[int] = None,
    ):
        super().__init__(llm, verbose)
        self.vulnerability_kb = vulnerability_kb or VulnerabilityKB()
        self.security_scanner = security_scanner or SecurityScannerTool()
        self.add_knowledge_base(self.vulnerability_kb)
        self.add_tool(self.security_scanner)

        self.risk_threshold = risk_threshold or Config.VULN_RISK_THRESHOLD_SECURITY
        self.max_units = max_units or Config.VULN_MAX_UNITS_SECURITY

        self.prompt = self._create_prompt(
            system_message="""You are a security analysis expert. Analyze the provided code for:
1. Injection vulnerabilities (SQL, command, XSS)
2. Authentication and authorization issues
3. Sensitive data exposure (hardcoded secrets, logging PII)
4. Insecure cryptography
5. Input validation issues
6. Insecure deserialization
7. Security misconfigurations

Focus on the feature points provided and analyze security implications of each change.

Respond in JSON format:
{{
    "issues": [
        {{
            "title": "short title",
            "description": "detailed description",
            "file": "filename",
            "line": line_number,
            "severity": "low|medium|high|critical",
            "feature_point_id": "id of related feature point",
            "cwe": "CWE ID if applicable",
            "suggestion": "how to fix the vulnerability"
        }}
    ],
    "summary": "overall security assessment",
    "vulnerabilities": [
        {{
            "type": "vulnerability type",
            "count": number,
            "severity": "severity level"
        }}
    ]
}}""",
            human_message="""Analyze the following code for security vulnerabilities:

Feature Points to analyze:
{feature_points}

Code for each feature point:
{code_snippets}

Security scanner results:
{scanner_results}

Known vulnerabilities from KB:
{kb_vulnerabilities}

Provide your security analysis.""",
        )

    async def analyze(
        self,
        codebase_path: str,
        feature_points: list[FeaturePoint] = None,
        changed_files: list[str] = None,
        diff_ir: dict = None,
        feature_risk_plan: dict = None,
        pr_dir: str = None,
        semgrep_findings: list = None,
        **kwargs,
    ) -> AgentResult:
        """
        Unified analyze method supporting both legacy and enhanced modes.

        Args:
            codebase_path: Path to the codebase
            feature_points: Feature points for legacy mode
            changed_files: Changed files for legacy mode
            diff_ir: Diff IR for enhanced mode
            feature_risk_plan: Risk plan for enhanced mode
            pr_dir: PR directory for enhanced mode
            semgrep_findings: Semgrep evidence for enhanced mode

        Returns:
            AgentResult with analysis results
        """
        try:
            if diff_ir and feature_risk_plan and pr_dir:
                return await self._run_enhanced_analysis(
                    pr_dir=pr_dir,
                    diff_ir=diff_ir,
                    feature_risk_plan=feature_risk_plan,
                    semgrep_findings=semgrep_findings or [],
                )

            return await self._run_legacy_analysis(
                codebase_path=codebase_path,
                feature_points=feature_points,
                changed_files=changed_files,
            )

        except Exception as e:
            logger.error(f"[{self.name}] Analysis failed: {e}")
            return AgentResult(
                agent_name=self.name,
                success=False,
                error=str(e),
            )

    async def _run_enhanced_analysis(
        self,
        pr_dir: str,
        diff_ir: dict,
        feature_risk_plan: dict,
        semgrep_findings: list = None,
    ) -> AgentResult:
        """Tool-enhanced security analysis using the advanced engine."""
        logger.info(f"[{self.name}] Running enhanced analysis with risk threshold {self.risk_threshold}")

        try:
            from src.agents.vulnerability.src.analysis.hunk_index import (
                build_hunk_index,
                select_security_targets,
                build_audit_unit_from_hunk,
            )
            from src.agents.vulnerability.src.prompts.prompt import (
                SECURITY_AGENT_SYSTEM_V2,
                format_security_agent_prompt_v2,
            )
            from src.agents.vulnerability.src.scripts.scanning.security_tooling import (
                create_security_tooling,
            )
            from src.agents.vulnerability.src.analysis.security_validator import (
                validate_security_issues_batch,
            )
        except ImportError as e:
            logger.warning(f"[{self.name}] Enhanced engine not available: {e}, falling back to legacy")
            return await self._run_legacy_analysis(
                codebase_path=pr_dir,
                feature_points=None,
                changed_files=None,
            )

        hunk_index = build_hunk_index(diff_ir)
        logger.info(f"[{self.name}] Built hunk index with {len(hunk_index)} entries")

        targets = select_security_targets(
            feature_risk_plan=feature_risk_plan,
            risk_threshold=self.risk_threshold,
            max_units=self.max_units,
        )

        if not targets:
            logger.info(f"[{self.name}] No security targets found")
            return AgentResult(
                agent_name=self.name,
                success=True,
                issues=[],
                summary="No security targets found for analysis",
                metrics={
                    "units_analyzed": 0,
                    "tool_evidence_summary": {
                        "total_units": 0,
                        "with_entrypoint": 0,
                        "with_call_chain": 0,
                    },
                },
            )

        logger.info(f"[{self.name}] Selected {len(targets)} targets for analysis")

        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            self._analyze_targets_sync,
            targets,
            hunk_index,
            pr_dir,
            feature_risk_plan,
            diff_ir,
            semgrep_findings or [],
            SECURITY_AGENT_SYSTEM_V2,
            format_security_agent_prompt_v2,
            create_security_tooling,
            validate_security_issues_batch,
        )

        issues = self._convert_enhanced_issues(result.get("issues", []))

        return AgentResult(
            agent_name=self.name,
            success=result.get("success", True),
            issues=[self._bug_to_dict(b) for b in issues],
            summary=f"Analyzed {result.get('units_analyzed', 0)} units, found {len(issues)} security issues",
            metrics={
                "units_analyzed": result.get("units_analyzed", 0),
                "issues_found": len(issues),
                "errors": result.get("errors", []),
                "vulnerabilities": [],
                "tool_evidence_summary": result.get("tool_evidence_summary", {}),
                "validation_summary": result.get("validation_summary", {}),
            },
        )

    def _analyze_targets_sync(
        self,
        targets: list,
        hunk_index: dict,
        pr_dir: str,
        feature_risk_plan: dict,
        diff_ir: dict,
        semgrep_findings: list,
        system_prompt: str,
        format_prompt_fn,
        create_security_tooling_fn,
        validate_issues_fn,
    ) -> dict:
        """Synchronous target analysis with tool evidence collection (runs in executor)."""
        from langchain_core.messages import SystemMessage, HumanMessage

        security_tooling = create_security_tooling_fn(pr_dir, feature_risk_plan)

        audit_units = []
        for target_info in targets:
            hunk_id = target_info.get("hunk_id")
            if not hunk_id:
                continue

            hunk_detail = hunk_index.get(hunk_id)
            if not hunk_detail:
                continue

            try:
                from src.agents.vulnerability.src.analysis.hunk_index import build_audit_unit_from_hunk
                audit_unit = build_audit_unit_from_hunk(
                    hunk_id=hunk_id,
                    hunk=hunk_detail,
                    code_tools=security_tooling.code_tools,
                    target_info=target_info,
                )
                audit_units.append(audit_unit)
            except Exception as e:
                logger.warning(f"[{self.name}] Failed to build audit unit for {hunk_id}: {e}")

        if not audit_units:
            return {
                "success": True,
                "units_analyzed": 0,
                "issues": [],
                "errors": [],
                "tool_evidence_summary": {"total_units": 0},
            }

        results = []
        errors = []
        tool_evidence_stats = {
            "total_units": len(audit_units),
            "with_entrypoint": 0,
            "with_call_chain": 0,
            "with_framework_routes": 0,
            "with_context": 0,
        }

        for unit in audit_units:
            hunk_id = unit.get("hunk_id", "unknown")
            file_path = unit.get("file_path", "unknown")
            risk_score = unit.get("risk_score", 0)
            symbol_name = unit.get("symbol_name", "")
            modified_lines = unit.get("code_context", {}).get("modified_lines", [])

            try:
                tool_evidence = security_tooling.system_collect_evidence(
                    file_path=file_path,
                    symbol_name=symbol_name,
                    modified_lines=modified_lines,
                )

                if tool_evidence.get("summary", {}).get("has_entrypoint"):
                    tool_evidence_stats["with_entrypoint"] += 1
                if tool_evidence.get("summary", {}).get("has_call_chain"):
                    tool_evidence_stats["with_call_chain"] += 1
                if tool_evidence.get("summary", {}).get("has_framework_routes"):
                    tool_evidence_stats["with_framework_routes"] += 1
                if tool_evidence.get("summary", {}).get("related_context"):
                    tool_evidence_stats["with_context"] += 1

                unit_semgrep_findings = self._match_semgrep_to_unit(unit, semgrep_findings)

                prompt = format_prompt_fn(unit, tool_evidence, semgrep_findings=unit_semgrep_findings)
                messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=prompt),
                ]

                resp = self.llm.invoke(messages)
                text = resp.content if hasattr(resp, "content") else str(resp)

                obj = self._safe_json_loads(text)

                if obj.get("result") not in ("NO_ISSUE", "ISSUE"):
                    obj = {"result": "NO_ISSUE", "issue": None}

                elif obj.get("result") == "ISSUE":
                    issue = obj.get("issue", {})
                    required_fields = ["entrypoint", "data_flow", "impact", "sink", "evidence"]
                    missing_fields = [f for f in required_fields if not issue.get(f)]

                    tool_summary = tool_evidence.get("summary", {})
                    has_sufficient_evidence = (
                        tool_summary.get("has_entrypoint", False) and
                        tool_summary.get("has_call_chain", False) and
                        tool_summary.get("confidence_score", 0) >= 50
                    )

                    if missing_fields or not has_sufficient_evidence:
                        obj = {"result": "NO_ISSUE", "issue": None, "need_context": ["insufficient_evidence"]}

                obj["_meta"] = {
                    "hunk_id": hunk_id,
                    "file_path": file_path,
                    "risk_score": risk_score,
                    "symbol_name": symbol_name,
                }
                obj["tool_evidence"] = tool_evidence
                results.append(obj)

            except Exception as e:
                errors.append(f"hunk_id={hunk_id} error={str(e)}")
                logger.error(f"[{self.name}] Analysis failed for {hunk_id}: {e}")

        raw_issues = [r for r in results if r.get("result") == "ISSUE"]
        issues_for_validation = [r.get("issue", {}) for r in raw_issues]

        validation_result = validate_issues_fn(
            issues=issues_for_validation,
            hunk_index=hunk_index,
        )

        final_issues = []
        for valid_issue in validation_result.get("valid_issues", []):
            for result in results:
                if result.get("result") == "ISSUE":
                    result_hunk_id = result.get("_meta", {}).get("hunk_id")
                    valid_hunk_id = valid_issue.get("diff_anchor", {}).get("hunk_id")
                    if result_hunk_id == valid_hunk_id:
                        result["issue"] = valid_issue
                        final_issues.append(result)
                        break

        return {
            "success": True,
            "units_analyzed": len(audit_units),
            "issues_found": len(final_issues),
            "issues": final_issues,
            "raw_results": results,
            "errors": errors,
            "tool_evidence_summary": tool_evidence_stats,
            "validation_summary": validation_result.get("validation_summary", {}),
        }

    def _match_semgrep_to_unit(self, audit_unit: dict, semgrep_findings: list) -> list:
        """Match Semgrep findings to the audit unit by file path and line range."""
        if not semgrep_findings:
            return []

        unit_file = audit_unit.get("file_path", "")
        modified_lines = audit_unit.get("code_context", {}).get("modified_lines", [])

        if not modified_lines:
            return []

        unit_line_min = min(modified_lines)
        unit_line_max = max(modified_lines)

        matched = []
        for finding in semgrep_findings:
            if finding.get("file_path", "") != unit_file:
                continue

            finding_start = finding.get("line_start", 0)
            finding_end = finding.get("line_end", finding_start)

            if not (finding_end < unit_line_min or finding_start > unit_line_max):
                matched.append(finding)

        return matched

    def _safe_json_loads(self, text: str) -> dict:
        """Extract JSON from potentially messy LLM output."""
        text = text.strip()
        if text.startswith("```json"):
            start = text.find("{")
            end = text.rfind("}")
            if start != -1 and end != -1 and end > start:
                return json.loads(text[start:end + 1])
        elif text.startswith("{") and text.endswith("}"):
            return json.loads(text)
        else:
            left = text.find("{")
            right = text.rfind("}")
            if left != -1 and right != -1 and right > left:
                return json.loads(text[left:right + 1])
        raise json.JSONDecodeError("no json object", text, 0)

    def _convert_enhanced_issues(self, issues: list) -> list[Bug]:
        """Convert enhanced analysis issues to Bug objects."""
        bugs = []
        for issue_data in issues:
            issue = issue_data.get("issue", {})
            meta = issue_data.get("_meta", {})

            if not issue:
                continue

            cwe_list = issue.get("cwe", [])
            cwe_str = ", ".join(str(c) for c in cwe_list) if cwe_list else None

            bugs.append(Bug(
                id=f"sec-{len(bugs)}",
                type=BugType.SECURITY_VULNERABILITY,
                severity=self._map_severity(issue.get("severity", "medium")),
                title=issue.get("title", "Security issue"),
                description=self._build_security_description(issue),
                file=meta.get("file_path", issue.get("file", "")),
                line=issue.get("line", 0),
                suggestion=issue.get("fix_suggestion", issue.get("suggestion", "")),
                feature_point_id=meta.get("hunk_id"),
            ))

        return bugs

    def _build_security_description(self, issue: dict) -> str:
        """Build a detailed description from the security issue."""
        parts = []

        if issue.get("description"):
            parts.append(issue["description"])

        if issue.get("entrypoint"):
            parts.append(f"Entry point: {issue['entrypoint']}")

        if issue.get("data_flow"):
            parts.append(f"Data flow: {issue['data_flow']}")

        if issue.get("impact"):
            parts.append(f"Impact: {issue['impact']}")

        if issue.get("cwe"):
            parts.append(f"CWE: {', '.join(str(c) for c in issue['cwe'])}")

        return "\n".join(parts) if parts else "Security vulnerability detected"

    async def _run_legacy_analysis(
        self,
        codebase_path: str,
        feature_points: list[FeaturePoint] = None,
        changed_files: list[str] = None,
    ) -> AgentResult:
        """Original analysis logic for backward compatibility."""
        try:
            scanner_result = await self.security_scanner.run(
                codebase_path,
                changed_files=changed_files,
            )
            scanner_issues = scanner_result.issues if scanner_result.success else []

            kb_vulns = self._query_knowledge_bases("injection authentication security vulnerability")

            fp_info = self._format_feature_points(feature_points or [])
            code_snippets = self._extract_security_relevant_code(codebase_path, feature_points or [])

            chain = self.prompt | self.llm
            response = await chain.ainvoke({
                "feature_points": fp_info,
                "code_snippets": code_snippets,
                "scanner_results": json.dumps(scanner_issues[:15], indent=2),
                "kb_vulnerabilities": json.dumps(kb_vulns[:5], indent=2),
            })

            result = self._parse_response(response.content, scanner_issues)

            return AgentResult(
                agent_name=self.name,
                success=True,
                issues=[self._bug_to_dict(bug) for bug in result.issues],
                summary=result.summary,
                metrics={"vulnerabilities": result.vulnerabilities},
            )

        except Exception as e:
            return AgentResult(
                agent_name=self.name,
                success=False,
                error=str(e),
            )

    def _format_feature_points(self, feature_points: list[FeaturePoint]) -> str:
        if not feature_points:
            return "No specific feature points provided - analyzing entire codebase"

        lines = []
        for fp in feature_points:
            lines.append(f"- [{fp.id}] {fp.name}: {fp.description}")
            lines.append(f"  Files: {', '.join(fp.files)}")
        return "\n".join(lines)

    def _extract_security_relevant_code(self, codebase_path: str, feature_points: list[FeaturePoint]) -> str:
        snippets = []

        security_patterns = [
            r"password", r"secret", r"token", r"api[_-]?key", r"auth",
            r"login", r"execute", r"eval", r"exec", r"subprocess",
            r"os\.system", r"pickle", r"yaml\.load", r"request\.", r"input\s*\(",
        ]

        files_to_check = []
        if feature_points:
            for fp in feature_points:
                for filepath in fp.files:
                    full_path = os.path.join(codebase_path, filepath)
                    if os.path.exists(full_path):
                        files_to_check.append(full_path)
        else:
            for root, _, files in os.walk(codebase_path):
                for file in files:
                    if file.endswith((".py", ".js", ".ts", ".java", ".go")):
                        files_to_check.append(os.path.join(root, file))

        for filepath in files_to_check[:10]:
            try:
                with open(filepath, "r") as f:
                    lines = f.readlines()

                relevant_sections = []
                for i, line in enumerate(lines):
                    for pattern in security_patterns:
                        if re.search(pattern, line, re.IGNORECASE):
                            start = max(0, i - 2)
                            end = min(len(lines), i + 3)
                            context = "".join(lines[start:end])
                            relevant_sections.append(f"Lines {start + 1}-{end}:\n{context}")
                            break

                if relevant_sections:
                    snippets.append(f"=== {filepath} ===\n" + "\n---\n".join(relevant_sections[:5]))

            except Exception:
                pass

        return "\n\n".join(snippets[:5]) or "No security-relevant code patterns found"

    def _parse_response(self, content: str, scanner_issues: list[dict]) -> SecurityAnalysis:
        issues = []
        vulnerabilities = []

        for si in scanner_issues:
            issues.append(Bug(
                id=f"scan-{si.get('line', 0)}",
                type=BugType.SECURITY_VULNERABILITY,
                severity=self._map_severity(si.get("severity", "medium")),
                title=si.get("message", "Security issue")[:50],
                description=si.get("message", ""),
                file=si.get("file", ""),
                line=si.get("line", 0),
                suggestion=f"CWE: {si.get('cwe', 'N/A')}",
            ))

        try:
            json_match = re.search(r"\{.*\}", content, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
            else:
                result = json.loads(content)

            for issue_data in result.get("issues", []):
                issues.append(Bug(
                    id=f"sec-{len(issues)}",
                    type=BugType.SECURITY_VULNERABILITY,
                    severity=self._map_severity(issue_data.get("severity", "medium")),
                    title=issue_data.get("title", ""),
                    description=issue_data.get("description", ""),
                    file=issue_data.get("file", ""),
                    line=issue_data.get("line", 0),
                    suggestion=issue_data.get("suggestion"),
                    feature_point_id=issue_data.get("feature_point_id"),
                ))

            vulnerabilities = result.get("vulnerabilities", [])

            return SecurityAnalysis(
                issues=issues,
                summary=result.get("summary", ""),
                vulnerabilities=vulnerabilities,
            )

        except json.JSONDecodeError:
            return SecurityAnalysis(
                issues=issues,
                summary="Security analysis completed with scanner results",
                vulnerabilities=[],
            )

    def _map_severity(self, severity: str) -> Severity:
        mapping = {
            "low": Severity.LOW,
            "medium": Severity.MEDIUM,
            "high": Severity.HIGH,
            "critical": Severity.CRITICAL,
        }
        return mapping.get(severity.lower(), Severity.MEDIUM)

    def _bug_to_dict(self, bug: Bug) -> dict:
        return {
            "id": bug.id,
            "type": bug.type.value,
            "severity": bug.severity.value,
            "title": bug.title,
            "description": bug.description,
            "file": bug.file,
            "line": bug.line,
            "suggestion": bug.suggestion,
            "feature_point_id": bug.feature_point_id,
        }
