"""
智能上下文构建器 (Smart Context Builder)
根据代码结构和审查需求动态构建最优上下文，最大化Token利用效率
"""

import json
import logging
import os
import re
from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass, field, asdict
from pathlib import Path
from enum import Enum

logger = logging.getLogger(__name__)


class ContextPriority(Enum):
    """上下文优先级"""
    CRITICAL = 1    # 必须包含
    HIGH = 2        # 高优先级
    MEDIUM = 3      # 中等优先级
    LOW = 4         # 低优先级
    OPTIONAL = 5    # 可选


@dataclass
class ContextBlock:
    """上下文块"""
    content: str
    source: str  # diff, definition, caller, callee, test, doc
    priority: ContextPriority
    token_count: int
    file_path: Optional[str] = None
    line_range: Optional[Tuple[int, int]] = None
    relevance_score: float = 1.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "source": self.source,
            "priority": self.priority.name,
            "token_count": self.token_count,
            "file_path": self.file_path,
            "line_range": self.line_range,
            "relevance_score": self.relevance_score
        }


@dataclass
class BuiltContext:
    """构建完成的上下文"""
    blocks: List[ContextBlock]
    total_tokens: int
    budget_remaining: int
    summary: str
    
    def get_full_context(self) -> str:
        """获取完整上下文文本"""
        parts = []
        for block in self.blocks:
            parts.append(f"--- {block.source.upper()} ---")
            if block.file_path:
                parts.append(f"File: {block.file_path}")
            if block.line_range:
                parts.append(f"Lines: {block.line_range[0]}-{block.line_range[1]}")
            parts.append(block.content)
            parts.append("")
        return "\n".join(parts)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "total_tokens": self.total_tokens,
            "budget_remaining": self.budget_remaining,
            "blocks_count": len(self.blocks),
            "blocks": [b.to_dict() for b in self.blocks],
            "summary": self.summary
        }


class SmartContextBuilder:
    """
    智能上下文构建器
    
    功能：
    1. 根据Token预算动态分配上下文空间
    2. 按优先级选择最相关的上下文
    3. 支持多种上下文来源（diff、定义、调用者、测试等）
    4. 自动截断和压缩长内容
    """
    
    # 默认Token预算分配比例
    DEFAULT_BUDGET_ALLOCATION = {
        "diff": 0.30,           # Diff变更本身
        "definition": 0.25,     # 函数/类完整定义
        "caller": 0.15,         # 调用者上下文
        "callee": 0.10,         # 被调用者上下文
        "test": 0.10,           # 相关测试
        "doc": 0.05,            # 文档/注释
        "history": 0.05,        # 历史问题
    }
    
    # Token估算比例（字符数 / Token数）
    TOKEN_RATIO = {
        "english": 4.0,
        "chinese": 1.5,
        "code": 3.5,
        "mixed": 3.0,
    }
    
    def __init__(self, 
                 max_tokens: int = 8000,
                 budget_allocation: Optional[Dict[str, float]] = None,
                 code_tools = None):
        """
        初始化智能上下文构建器
        
        Args:
            max_tokens: 最大Token数
            budget_allocation: 自定义预算分配
            code_tools: 代码工具实例（用于读取文件）
        """
        self.max_tokens = max_tokens
        self.budget_allocation = budget_allocation or self.DEFAULT_BUDGET_ALLOCATION
        self.code_tools = code_tools
        
        # 验证预算分配
        total = sum(self.budget_allocation.values())
        if abs(total - 1.0) > 0.01:
            logger.warning(f"[SmartContext] 预算分配总和不为1: {total}，将自动归一化")
            for key in self.budget_allocation:
                self.budget_allocation[key] /= total
    
    def build_context(self,
                      audit_unit: Dict[str, Any],
                      cross_file_context: Optional[Dict[str, Any]] = None,
                      historical_issues: Optional[List[Dict]] = None,
                      review_type: str = "both") -> BuiltContext:
        """
        构建审查上下文
        
        Args:
            audit_unit: 审计单元
            cross_file_context: 跨文件上下文
            historical_issues: 历史问题
            review_type: 审查类型 ("logic", "security", "both")
        
        Returns:
            构建完成的上下文
        """
        logger.info(f"[SmartContext] 开始构建上下文, 预算: {self.max_tokens} tokens")
        
        # 收集所有可用的上下文块
        all_blocks = []
        
        # 1. Diff变更（最高优先级）
        diff_blocks = self._extract_diff_blocks(audit_unit)
        all_blocks.extend(diff_blocks)
        
        # 2. 代码定义
        definition_blocks = self._extract_definition_blocks(audit_unit)
        all_blocks.extend(definition_blocks)
        
        # 3. 跨文件上下文
        if cross_file_context:
            cross_blocks = self._extract_cross_file_blocks(cross_file_context, review_type)
            all_blocks.extend(cross_blocks)
        
        # 4. 历史问题
        if historical_issues:
            history_blocks = self._extract_history_blocks(historical_issues)
            all_blocks.extend(history_blocks)
        
        # 按优先级和相关性排序
        all_blocks.sort(key=lambda b: (b.priority.value, -b.relevance_score))
        
        # 选择最终的上下文块
        selected_blocks = self._select_blocks(all_blocks)
        
        # 计算总Token数
        total_tokens = sum(b.token_count for b in selected_blocks)
        
        # 生成摘要
        summary = self._generate_summary(selected_blocks, audit_unit)
        
        result = BuiltContext(
            blocks=selected_blocks,
            total_tokens=total_tokens,
            budget_remaining=self.max_tokens - total_tokens,
            summary=summary
        )
        
        logger.info(f"[SmartContext] 构建完成: {len(selected_blocks)} 块, "
                   f"{total_tokens}/{self.max_tokens} tokens")
        
        return result
    
    def _estimate_tokens(self, text: str) -> int:
        """估算文本的Token数"""
        if not text:
            return 0
        
        # 检测内容类型
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        total_chars = len(text)
        
        if chinese_chars > total_chars * 0.3:
            # 中文为主
            ratio = self.TOKEN_RATIO["chinese"]
        elif re.search(r'[{}\[\]();=]', text):
            # 代码
            ratio = self.TOKEN_RATIO["code"]
        else:
            # 英文或混合
            ratio = self.TOKEN_RATIO["mixed"]
        
        return int(total_chars / ratio)
    
    def _extract_diff_blocks(self, audit_unit: Dict[str, Any]) -> List[ContextBlock]:
        """提取Diff相关的上下文块"""
        blocks = []
        
        # 主Diff内容
        diff_hunk = audit_unit.get("diff_hunk", "")
        if diff_hunk:
            blocks.append(ContextBlock(
                content=diff_hunk,
                source="diff",
                priority=ContextPriority.CRITICAL,
                token_count=self._estimate_tokens(diff_hunk),
                file_path=audit_unit.get("file_path"),
                line_range=tuple(audit_unit.get("code_context", {}).get("new_line_range", [None, None])),
                relevance_score=1.0,
                metadata={"type": "main_diff"}
            ))
        
        return blocks
    
    def _extract_definition_blocks(self, audit_unit: Dict[str, Any]) -> List[ContextBlock]:
        """提取函数/类定义的上下文块"""
        blocks = []
        
        code_context = audit_unit.get("code_context", {})
        snippet = code_context.get("snippet", "")
        
        if snippet:
            # 尝试提取函数定义
            function_def = self._extract_function_definition(snippet)
            
            if function_def:
                blocks.append(ContextBlock(
                    content=function_def,
                    source="definition",
                    priority=ContextPriority.HIGH,
                    token_count=self._estimate_tokens(function_def),
                    file_path=audit_unit.get("file_path"),
                    relevance_score=0.9,
                    metadata={"type": "function_definition"}
                ))
            
            # 添加完整代码上下文（如果函数定义不够完整）
            if len(function_def or "") < len(snippet) * 0.5:
                blocks.append(ContextBlock(
                    content=snippet,
                    source="definition",
                    priority=ContextPriority.MEDIUM,
                    token_count=self._estimate_tokens(snippet),
                    file_path=audit_unit.get("file_path"),
                    line_range=tuple(code_context.get("new_line_range", [None, None])),
                    relevance_score=0.7,
                    metadata={"type": "code_context"}
                ))
        
        return blocks
    
    def _extract_function_definition(self, code: str) -> str:
        """从代码中提取函数定义"""
        lines = code.split('\n')
        
        # 查找函数定义的起始
        func_patterns = [
            r'^\s*def\s+\w+',           # Python
            r'^\s*async\s+def\s+\w+',   # Python async
            r'^\s*function\s+\w+',      # JavaScript
            r'^\s*const\s+\w+\s*=',     # JavaScript arrow
            r'^\s*func\s+\w+',          # Go
            r'^\s*(?:public|private|protected)?\s*\w+\s+\w+\s*\(',  # Java
        ]
        
        start_line = None
        for i, line in enumerate(lines):
            for pattern in func_patterns:
                if re.match(pattern, line):
                    start_line = i
                    break
            if start_line is not None:
                break
        
        if start_line is None:
            return ""
        
        # 查找函数结束（简化实现）
        # 使用缩进判断（适用于Python等）
        base_indent = len(lines[start_line]) - len(lines[start_line].lstrip())
        end_line = start_line
        
        for i in range(start_line + 1, len(lines)):
            line = lines[i]
            if line.strip() == "":
                continue
            
            current_indent = len(line) - len(line.lstrip())
            if current_indent <= base_indent and line.strip():
                break
            end_line = i
        
        # 限制函数长度
        max_lines = 50
        if end_line - start_line > max_lines:
            end_line = start_line + max_lines
        
        return '\n'.join(lines[start_line:end_line + 1])
    
    def _extract_cross_file_blocks(self, 
                                   cross_file_context: Dict[str, Any],
                                   review_type: str) -> List[ContextBlock]:
        """提取跨文件上下文块"""
        blocks = []
        
        # 调用者
        callers = cross_file_context.get("callers", [])
        for i, caller in enumerate(callers[:5]):
            priority = ContextPriority.HIGH if i < 2 else ContextPriority.MEDIUM
            
            content = self._format_caller_context(caller)
            blocks.append(ContextBlock(
                content=content,
                source="caller",
                priority=priority,
                token_count=self._estimate_tokens(content),
                file_path=caller.get("file"),
                line_range=(caller.get("line"), caller.get("line")),
                relevance_score=0.8 - i * 0.1,
                metadata={"function": caller.get("function")}
            ))
        
        # 被调用者
        callees = cross_file_context.get("callees", [])
        for i, callee in enumerate(callees[:3]):
            content = self._format_callee_context(callee)
            blocks.append(ContextBlock(
                content=content,
                source="callee",
                priority=ContextPriority.MEDIUM,
                token_count=self._estimate_tokens(content),
                file_path=callee.get("file"),
                relevance_score=0.6 - i * 0.1,
                metadata={"function": callee.get("function")}
            ))
        
        # 安全相关上下文（仅安全审查时）
        if review_type in ["security", "both"]:
            # API路由
            api_routes = cross_file_context.get("api_routes", [])
            if api_routes:
                content = "API Routes:\n" + "\n".join(f"  - {r}" for r in api_routes)
                blocks.append(ContextBlock(
                    content=content,
                    source="security_context",
                    priority=ContextPriority.HIGH,
                    token_count=self._estimate_tokens(content),
                    relevance_score=0.85,
                    metadata={"type": "api_routes"}
                ))
            
            # 认证检查
            auth_checks = cross_file_context.get("auth_checks", [])
            if auth_checks:
                content = "Auth Checks:\n" + "\n".join(f"  - {c}" for c in auth_checks)
                blocks.append(ContextBlock(
                    content=content,
                    source="security_context",
                    priority=ContextPriority.HIGH,
                    token_count=self._estimate_tokens(content),
                    relevance_score=0.85,
                    metadata={"type": "auth_checks"}
                ))
        
        # 相关代码片段
        related_snippets = cross_file_context.get("related_snippets", [])
        for i, snippet in enumerate(related_snippets[:3]):
            content = snippet.get("code", "")
            blocks.append(ContextBlock(
                content=content,
                source="related",
                priority=ContextPriority.LOW,
                token_count=self._estimate_tokens(content),
                file_path=snippet.get("file"),
                line_range=(snippet.get("line_start"), snippet.get("line_end")),
                relevance_score=0.5 - i * 0.1,
                metadata={"relation": snippet.get("relation")}
            ))
        
        return blocks
    
    def _format_caller_context(self, caller: Dict[str, Any]) -> str:
        """格式化调用者上下文"""
        parts = [
            f"Caller: {caller.get('function', 'unknown')}",
            f"Location: {caller.get('file', '')}:{caller.get('line', '')}",
        ]
        
        if caller.get("code"):
            parts.append(f"Code: {caller['code']}")
        
        return "\n".join(parts)
    
    def _format_callee_context(self, callee: Dict[str, Any]) -> str:
        """格式化被调用者上下文"""
        parts = [
            f"Callee: {callee.get('function', 'unknown')}",
        ]
        
        if callee.get("file"):
            parts.append(f"From: {callee['file']}")
        
        if callee.get("code"):
            parts.append(f"Call: {callee['code']}")
        
        return "\n".join(parts)
    
    def _extract_history_blocks(self, 
                               historical_issues: List[Dict]) -> List[ContextBlock]:
        """提取历史问题上下文块"""
        blocks = []
        
        if not historical_issues:
            return blocks
        
        # 格式化历史问题
        history_lines = ["Historical Issues in this area:"]
        for issue in historical_issues[:5]:
            category = issue.get("category", "unknown")
            description = issue.get("description", "")[:100]
            history_lines.append(f"  - [{category}] {description}")
        
        content = "\n".join(history_lines)
        
        blocks.append(ContextBlock(
            content=content,
            source="history",
            priority=ContextPriority.LOW,
            token_count=self._estimate_tokens(content),
            relevance_score=0.4,
            metadata={"issue_count": len(historical_issues)}
        ))
        
        return blocks
    
    def _select_blocks(self, all_blocks: List[ContextBlock]) -> List[ContextBlock]:
        """选择最终的上下文块"""
        selected = []
        used_tokens = 0
        
        # 首先添加所有CRITICAL优先级的块
        for block in all_blocks:
            if block.priority == ContextPriority.CRITICAL:
                if used_tokens + block.token_count <= self.max_tokens:
                    selected.append(block)
                    used_tokens += block.token_count
                else:
                    # 尝试截断
                    truncated = self._truncate_block(block, self.max_tokens - used_tokens)
                    if truncated:
                        selected.append(truncated)
                        used_tokens += truncated.token_count
        
        # 按预算分配选择其他块
        remaining_budget = self.max_tokens - used_tokens
        
        # 按来源分组
        by_source = {}
        for block in all_blocks:
            if block not in selected:
                source = block.source
                if source not in by_source:
                    by_source[source] = []
                by_source[source].append(block)
        
        # 按预算分配选择
        for source, budget_ratio in self.budget_allocation.items():
            source_budget = int(remaining_budget * budget_ratio)
            source_blocks = by_source.get(source, [])
            
            source_used = 0
            for block in source_blocks:
                if source_used + block.token_count <= source_budget:
                    selected.append(block)
                    source_used += block.token_count
                    used_tokens += block.token_count
                elif source_budget - source_used > 100:
                    # 尝试截断
                    truncated = self._truncate_block(block, source_budget - source_used)
                    if truncated:
                        selected.append(truncated)
                        source_used += truncated.token_count
                        used_tokens += truncated.token_count
        
        return selected
    
    def _truncate_block(self, block: ContextBlock, 
                        max_tokens: int) -> Optional[ContextBlock]:
        """截断上下文块以适应Token限制"""
        if max_tokens < 50:  # 太短不值得保留
            return None
        
        # 估算需要保留的字符数
        target_chars = int(max_tokens * self.TOKEN_RATIO["code"])
        
        content = block.content
        if len(content) <= target_chars:
            return block
        
        # 智能截断：保留开头和结尾
        head_size = int(target_chars * 0.6)
        tail_size = int(target_chars * 0.3)
        
        head = content[:head_size]
        tail = content[-tail_size:]
        
        truncated_content = f"{head}\n... [truncated {len(content) - head_size - tail_size} chars] ...\n{tail}"
        
        return ContextBlock(
            content=truncated_content,
            source=block.source,
            priority=block.priority,
            token_count=self._estimate_tokens(truncated_content),
            file_path=block.file_path,
            line_range=block.line_range,
            relevance_score=block.relevance_score * 0.8,  # 降低相关性分数
            metadata={**block.metadata, "truncated": True}
        )
    
    def _generate_summary(self, blocks: List[ContextBlock], 
                         audit_unit: Dict[str, Any]) -> str:
        """生成上下文摘要"""
        sources = {}
        for block in blocks:
            source = block.source
            if source not in sources:
                sources[source] = 0
            sources[source] += 1
        
        parts = [
            f"File: {audit_unit.get('file_path', 'unknown')}",
            f"Context blocks: {len(blocks)}",
            f"Sources: {', '.join(f'{k}({v})' for k, v in sources.items())}"
        ]
        
        return " | ".join(parts)


class ContextWindowManager:
    """
    上下文窗口管理器
    管理多个审计单元的上下文构建
    """
    
    def __init__(self, 
                 max_tokens_per_unit: int = 8000,
                 code_tools = None):
        """
        初始化上下文窗口管理器
        
        Args:
            max_tokens_per_unit: 每个审计单元的最大Token数
            code_tools: 代码工具实例
        """
        self.max_tokens_per_unit = max_tokens_per_unit
        self.code_tools = code_tools
        self.builder = SmartContextBuilder(
            max_tokens=max_tokens_per_unit,
            code_tools=code_tools
        )
    
    def build_contexts_for_units(self,
                                audit_units: List[Dict[str, Any]],
                                cross_file_analyzer = None,
                                review_type: str = "both") -> Dict[str, BuiltContext]:
        """
        为多个审计单元构建上下文
        
        Args:
            audit_units: 审计单元列表
            cross_file_analyzer: 跨文件分析器实例
            review_type: 审查类型
        
        Returns:
            审计单元ID到构建上下文的映射
        """
        contexts = {}
        
        for unit in audit_units:
            unit_id = unit.get("hunk_id", "unknown")
            
            # 获取跨文件上下文
            cross_file_context = None
            if cross_file_analyzer:
                try:
                    file_path = unit.get("file_path", "")
                    line_range = tuple(unit.get("code_context", {}).get("new_line_range", [1, 1]))
                    language = unit.get("language", "python")
                    
                    cross_file_context = cross_file_analyzer.analyze_cross_file_context(
                        file_path=file_path,
                        line_range=line_range,
                        language=language
                    ).to_dict()
                except Exception as e:
                    logger.warning(f"[ContextManager] 跨文件分析失败: {unit_id}, {e}")
            
            # 构建上下文
            context = self.builder.build_context(
                audit_unit=unit,
                cross_file_context=cross_file_context,
                review_type=review_type
            )
            
            contexts[unit_id] = context
        
        return contexts
    
    def adjust_budget_for_complexity(self, 
                                    audit_unit: Dict[str, Any]) -> int:
        """
        根据审计单元复杂度调整Token预算
        
        Args:
            audit_unit: 审计单元
        
        Returns:
            调整后的Token预算
        """
        base_budget = self.max_tokens_per_unit
        
        # 根据diff大小调整
        diff_hunk = audit_unit.get("diff_hunk", "")
        diff_lines = len(diff_hunk.split('\n'))
        
        if diff_lines > 100:
            # 大变更需要更多上下文
            base_budget = int(base_budget * 1.2)
        elif diff_lines < 20:
            # 小变更可以减少上下文
            base_budget = int(base_budget * 0.8)
        
        # 根据风险分数调整
        risk_score = audit_unit.get("risk_score", 50)
        if risk_score >= 80:
            base_budget = int(base_budget * 1.3)
        elif risk_score <= 30:
            base_budget = int(base_budget * 0.7)
        
        return min(base_budget, 16000)  # 硬上限


def create_smart_context_builder(max_tokens: int = 8000,
                                 code_tools = None) -> SmartContextBuilder:
    """创建智能上下文构建器实例"""
    return SmartContextBuilder(max_tokens=max_tokens, code_tools=code_tools)


def build_review_context(audit_unit: Dict[str, Any],
                        cross_file_context: Optional[Dict] = None,
                        max_tokens: int = 8000) -> Dict[str, Any]:
    """
    构建审查上下文（便捷函数）
    
    Args:
        audit_unit: 审计单元
        cross_file_context: 跨文件上下文
        max_tokens: 最大Token数
    
    Returns:
        构建结果字典
    """
    builder = SmartContextBuilder(max_tokens=max_tokens)
    result = builder.build_context(
        audit_unit=audit_unit,
        cross_file_context=cross_file_context
    )
    
    return {
        "full_context": result.get_full_context(),
        "total_tokens": result.total_tokens,
        "budget_remaining": result.budget_remaining,
        "summary": result.summary,
        "blocks": [b.to_dict() for b in result.blocks]
    }
