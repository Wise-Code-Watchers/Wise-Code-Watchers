"""
TODO Listæ‰§è¡Œå™¨ - æ‰§è¡Œä»£ç å®¡è®¡ä»»åŠ¡
"""

import json
import os
from typing import Any, Dict, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime

from langchain_core.messages import SystemMessage, HumanMessage
from langchain_openai import ChatOpenAI

from ..core.code_tools import CodeTools


def create_audit_task_prompt(task: Dict[str, Any], context_info: Dict[str, Any]) -> str:
    """åˆ›å»ºå•ä¸ªå®¡è®¡ä»»åŠ¡çš„LLMæç¤ºè¯"""

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç å®‰å…¨å®¡è®¡å¸ˆã€‚è¯·æ‰§è¡Œä»¥ä¸‹å…·ä½“çš„ä»£ç å®¡è®¡ä»»åŠ¡ï¼š

## ä»»åŠ¡ä¿¡æ¯:
- **ä»»åŠ¡ID**: {task.get("task_id")}
- **ä»»åŠ¡æ ‡é¢˜**: {task.get("task_title")}
- **åŠŸèƒ½åç§°**: {task.get("feature_name")}
- **ä¼˜å…ˆçº§**: {task.get("priority")}
- **é¢„è®¡å·¥æ—¶**: {task.get("estimated_hours")}å°æ—¶

## å®¡è®¡ç›®æ ‡:
{task.get("audit_objective", "")}

## å®¡è®¡æ–¹æ³•:
{task.get("audit_method", "")}

## æ£€æŸ¥ç‚¹æ¸…å•:
{chr(10).join([f"- {cp}" for cp in task.get("check_points", [])])}

## ç›®æ ‡æ–‡ä»¶:
{chr(10).join([f"- {f}" for f in task.get("target_files", [])])}

## ç›®æ ‡å‡½æ•°:
{chr(10).join([f"- {f}" for f in task.get("target_functions", [])])}

## é£Žé™©ç±»åˆ«:
{', '.join(task.get("risk_categories", []))}

## å¯ç”¨å·¥å…·ç»“æžœ:
{json.dumps(context_info, ensure_ascii=False, indent=2)}

## åˆ†æžè¦æ±‚:

è¯·ä»”ç»†åˆ†æžä¸Šè¿°ä»£ç ï¼Œé‡ç‚¹æ£€æŸ¥ä»¥ä¸‹æ–¹é¢ï¼š

### å®‰å…¨æ€§æ£€æŸ¥:
1. **è¾“å…¥éªŒè¯** - æ˜¯å¦å­˜åœ¨æœªç»éªŒè¯çš„ç”¨æˆ·è¾“å…¥
2. **æƒé™æŽ§åˆ¶** - æ˜¯å¦æœ‰é€‚å½“çš„è®¿é—®æŽ§åˆ¶æ£€æŸ¥
3. **æ•°æ®å¤„ç†** - æ•æ„Ÿæ•°æ®å¤„ç†æ˜¯å¦å®‰å…¨
4. **æ³¨å…¥é˜²æŠ¤** - æ˜¯å¦å­˜åœ¨SQLæ³¨å…¥ã€å‘½ä»¤æ³¨å…¥ç­‰é£Žé™©

### é€»è¾‘æ­£ç¡®æ€§:
1. **ä¸šåŠ¡é€»è¾‘** - æ˜¯å¦ç¬¦åˆé¢„æœŸä¸šåŠ¡æµç¨‹
2. **é”™è¯¯å¤„ç†** - å¼‚å¸¸æƒ…å†µå¤„ç†æ˜¯å¦å®Œå–„
3. **çŠ¶æ€ç®¡ç†** - çŠ¶æ€è½¬æ¢æ˜¯å¦æ­£ç¡®
4. **è¾¹ç•Œæ¡ä»¶** - è¾¹ç•Œå€¼å¤„ç†æ˜¯å¦æ­£ç¡®

### ä»£ç è´¨é‡:
1. **ä»£ç è§„èŒƒ** - æ˜¯å¦éµå¾ªç¼–ç è§„èŒƒ
2. **æ€§èƒ½è€ƒè™‘** - æ˜¯å¦å­˜åœ¨æ€§èƒ½é—®é¢˜
3. **å¯ç»´æŠ¤æ€§** - ä»£ç æ˜¯å¦æ˜“äºŽç†è§£å’Œç»´æŠ¤

## è¾“å‡ºæ ¼å¼è¦æ±‚:

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºå®¡è®¡ç»“æžœï¼š

{{
  "task_id": "{task.get("task_id")}",
  "execution_summary": {{
    "status": "completed|partial|failed",
    "files_analyzed": ["file1.py", "file2.py"],
    "functions_reviewed": ["func1", "func2"],
    "lines_of_code_reviewed": 150,
    "execution_time_minutes": 30
  }},
  "findings": [
    {{
      "id": "finding_001",
      "type": "security|logic|performance|quality",
      "severity": "critical|high|medium|low|info",
      "category": "å…·ä½“é—®é¢˜ç±»åˆ«",
      "title": "é—®é¢˜æ ‡é¢˜",
      "description": "è¯¦ç»†é—®é¢˜æè¿°",
      "file_path": "æ–‡ä»¶è·¯å¾„",
      "line_numbers": [10, 15, 20],
      "code_snippet": "é—®é¢˜ä»£ç ç‰‡æ®µ",
      "risk_impact": "é£Žé™©å½±å“è¯´æ˜Ž",
      "recommendation": "ä¿®å¤å»ºè®®",
      "confidence": "high|medium|low",
      "cwe_id": "CWE-IDï¼ˆå¦‚é€‚ç”¨ï¼‰"
    }}
  ],
  "security_assessment": {{
    "overall_security_level": "secure|moderate|risk|critical",
    "security_score": 8.5,
    "critical_findings_count": 0,
    "high_findings_count": 1,
    "medium_findings_count": 3,
    "low_findings_count": 2
  }},
  "recommendations": [
    {{
      "priority": "high|medium|low",
      "action": "å…·ä½“ä¿®å¤åŠ¨ä½œ",
      "description": "ä¿®å¤è¯´æ˜Ž",
      "estimated_effort": "ä¿®å¤å·¥ä½œé‡ä¼°ç®—"
    }}
  ],
  "conclusion": "å®¡è®¡ç»“è®ºæ€»ç»“"
}}

## é‡è¦çº¦æŸ:
- å¿…é¡»åŸºäºŽå®žé™…ä»£ç å†…å®¹è¿›è¡Œåˆ†æž
- æ¯ä¸ªå‘çŽ°å¿…é¡»æœ‰æ˜Žç¡®çš„æ–‡ä»¶ä½ç½®å’Œè¡Œå·
- æä¾›å…·ä½“çš„ä¿®å¤å»ºè®®ï¼Œä¸ä»…ä»…æ˜¯æŒ‡å‡ºé—®é¢˜
- ç»™å‡ºå®¢è§‚çš„é£Žé™©è¯„ä¼°å’Œç½®ä¿¡åº¦
- ç»“è®ºè¦å…·ä½“æ˜Žç¡®ï¼Œä¸è¦å«ç³Šå…¶è¾ž
"""

    return prompt


def execute_single_audit_task(
    task: Dict[str, Any],
    code_tools: CodeTools,
    llm: ChatOpenAI,
    context_info: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    æ‰§è¡Œå•ä¸ªå®¡è®¡ä»»åŠ¡

    Args:
        task: å®¡è®¡ä»»åŠ¡
        code_tools: ä»£ç å·¥å…·å®žä¾‹
        llm: è¯­è¨€æ¨¡åž‹
        context_info: ä¸Šä¸‹æ–‡ä¿¡æ¯

    Returns:
        å®¡è®¡ç»“æžœ
    """
    task_id = task.get("task_id", "unknown")
    start_time = datetime.now()

    try:
        # å‡†å¤‡ä¸Šä¸‹æ–‡ä¿¡æ¯
        task_context = context_info or {}
        task_context["task_info"] = task

        # æ”¶é›†ç›®æ ‡æ–‡ä»¶çš„ä»£ç å†…å®¹
        target_files = task.get("target_files", [])
        file_contents = {}
        file_analysis = {}

        for file_path in target_files[:5]:  # é™åˆ¶æ–‡ä»¶æ•°é‡
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹
                file_result = code_tools.read_file(file_path)
                if file_result["success"]:
                    file_contents[file_path] = file_result["content"]

                    # åˆ†æžæ–‡ä»¶å˜æ›´ï¼ˆå¦‚æžœæœ‰hunkä¿¡æ¯ï¼‰
                    if "diff_hunks" in task:
                        file_analysis_result = code_tools.analyze_file_changes(
                            file_path, task.get("diff_hunks", [])
                        )
                        if file_analysis_result["success"]:
                            file_analysis[file_path] = file_analysis_result["analysis"]

                # æœç´¢ç›¸å…³å‡½æ•°
                target_functions = task.get("target_functions", [])
                for func_name in target_functions[:3]:  # é™åˆ¶å‡½æ•°æ•°é‡
                    func_result = code_tools.find_function(func_name, "*.py")
                    if func_result["success"]:
                        file_contents[f"function_{func_name}"] = func_result

            except Exception as e:
                file_contents[f"error_{file_path}"] = f"æ–‡ä»¶åˆ†æžå¤±è´¥: {str(e)}"

        task_context.update({
            "file_contents": file_contents,
            "file_analysis": file_analysis,
            "collected_at": datetime.now().isoformat()
        })

        # ç”Ÿæˆå®¡è®¡æç¤ºè¯
        prompt = create_audit_task_prompt(task, task_context)

        # è°ƒç”¨LLMè¿›è¡Œåˆ†æž
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç å®‰å…¨å®¡è®¡å¸ˆï¼Œå…·æœ‰ä¸°å¯Œçš„ä»£ç å®¡æŸ¥å’Œæ¼æ´žå‘çŽ°ç»éªŒã€‚"),
            HumanMessage(content=prompt)
        ]

        response = llm.invoke(messages)
        response_text = response.content if hasattr(response, "content") else str(response)

        # è§£æžå“åº”
        try:
            # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œæå–JSON
            json_start = response_text.find("{")
            json_end = response_text.rfind("}") + 1
            if json_start != -1 and json_end > json_start:
                json_text = response_text[json_start:json_end]
                result = json.loads(json_text)
            else:
                result = json.loads(response_text)
        except json.JSONDecodeError:
            # JSONè§£æžå¤±è´¥ï¼Œåˆ›å»ºåŸºæœ¬ç»“æžœç»“æž„
            result = {
                "task_id": task_id,
                "execution_summary": {
                    "status": "completed",
                    "files_analyzed": target_files,
                    "functions_reviewed": task.get("target_functions", []),
                    "lines_of_code_reviewed": 0,
                    "execution_time_minutes": (datetime.now() - start_time).total_seconds() / 60
                },
                "findings": [],
                "security_assessment": {
                    "overall_security_level": "unknown",
                    "security_score": 0,
                    "critical_findings_count": 0,
                    "high_findings_count": 0,
                    "medium_findings_count": 0,
                    "low_findings_count": 0
                },
                "recommendations": [],
                "conclusion": "å®¡è®¡å®Œæˆï¼Œä½†å“åº”è§£æžå¤±è´¥",
                "parsing_error": True,
                "raw_response": response_text[:1000]
            }

        # æ·»åŠ æ‰§è¡Œå…ƒæ•°æ®
        result["execution_metadata"] = {
            "task_id": task_id,
            "start_time": start_time.isoformat(),
            "end_time": datetime.now().isoformat(),
            "execution_duration_minutes": (datetime.now() - start_time).total_seconds() / 60,
            "files_attempted": len(target_files),
            "context_size": len(str(task_context))
        }

        result["success"] = True
        result["task_id"] = task_id

        return result

    except Exception as e:
        return {
            "task_id": task_id,
            "success": False,
            "error": f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}",
            "execution_metadata": {
                "task_id": task_id,
                "start_time": start_time.isoformat(),
                "end_time": datetime.now().isoformat(),
                "error": str(e)
            },
            "findings": [],
            "conclusion": f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}"
        }


def execute_audit_todolist(
    todolist: Dict[str, Any],
    code_tools: CodeTools,
    llm: ChatOpenAI,
    max_workers: int = 2,
    execution_plan: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    æ‰§è¡Œå®¡è®¡TODOåˆ—è¡¨

    Args:
        todolist: TODOåˆ—è¡¨
        code_tools: ä»£ç å·¥å…·å®žä¾‹
        llm: è¯­è¨€æ¨¡åž‹
        max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
        execution_plan: æ‰§è¡Œè®¡åˆ’ï¼ˆå¯é€‰ï¼‰

    Returns:
        æ‰§è¡Œç»“æžœæ±‡æ€»
    """
    start_time = datetime.now()
    audit_tasks = todolist.get("audit_tasks", [])

    if not audit_tasks:
        return {
            "success": False,
            "error": "æ²¡æœ‰æ‰¾åˆ°å®¡è®¡ä»»åŠ¡",
            "execution_results": []
        }

    print(f"ðŸš€ å¼€å§‹æ‰§è¡Œå®¡è®¡TODOåˆ—è¡¨ï¼Œå…± {len(audit_tasks)} ä¸ªä»»åŠ¡")
    print(f"ðŸ“Š ä½¿ç”¨ {max_workers} ä¸ªå¹¶è¡Œå·¥ä½œçº¿ç¨‹")

    # åˆå§‹åŒ–ä»£ç å·¥å…·è°ƒç”¨ç»Ÿè®¡
    initial_stats = code_tools.get_tool_call_stats()
    print(f"ðŸ› ï¸  åˆå§‹ä»£ç å·¥å…·è°ƒç”¨ç»Ÿè®¡: {initial_stats['statistics']['total_calls']}æ¬¡è°ƒç”¨")

    # å‡†å¤‡ä»»åŠ¡æ‰§è¡Œ
    all_results = []
    completed_tasks = 0
    failed_tasks = 0

    if max_workers == 1:
        # é¡ºåºæ‰§è¡Œ
        print("ðŸ“ ä½¿ç”¨é¡ºåºæ‰§è¡Œæ¨¡å¼")
        for i, task in enumerate(audit_tasks, 1):
            print(f"â³ æ‰§è¡Œä»»åŠ¡ {i}/{len(audit_tasks)}: {task.get('task_title', task.get('task_id'))}")

            result = execute_single_audit_task(task, code_tools, llm)
            all_results.append(result)

            if result.get("success"):
                completed_tasks += 1
                print(f"âœ… ä»»åŠ¡ {task.get('task_id')} å®Œæˆ")
            else:
                failed_tasks += 1
                print(f"âŒ ä»»åŠ¡ {task.get('task_id')} å¤±è´¥: {result.get('error')}")

    else:
        # å¹¶è¡Œæ‰§è¡Œ
        print(f"âš¡ ä½¿ç”¨å¹¶è¡Œæ‰§è¡Œæ¨¡å¼ï¼Œ{max_workers} ä¸ªçº¿ç¨‹")
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {
                executor.submit(execute_single_audit_task, task, code_tools, llm): task
                for task in audit_tasks
            }

            # æ”¶é›†ç»“æžœ
            for future in as_completed(future_to_task):
                task = future_to_task[future]
                try:
                    result = future.result()
                    all_results.append(result)

                    if result.get("success"):
                        completed_tasks += 1
                        print(f"âœ… ä»»åŠ¡ {task.get('task_id')} å®Œæˆ")
                    else:
                        failed_tasks += 1
                        print(f"âŒ ä»»åŠ¡ {task.get('task_id')} å¤±è´¥: {result.get('error')}")

                except Exception as e:
                    failed_tasks += 1
                    error_result = {
                        "task_id": task.get("task_id"),
                        "success": False,
                        "error": f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {str(e)}",
                        "findings": []
                    }
                    all_results.append(error_result)
                    print(f"ðŸ’¥ ä»»åŠ¡ {task.get('task_id')} å¼‚å¸¸: {str(e)}")

                completed_tasks = len([r for r in all_results if r.get("success")])

    # ç”Ÿæˆæ‰§è¡Œæ±‡æ€»
    end_time = datetime.now()
    total_duration = (end_time - start_time).total_seconds() / 60

    # æ‰“å°ä»£ç å·¥å…·è°ƒç”¨æ±‡æ€»
    final_stats = code_tools.get_tool_call_stats()
    tool_calls_made = final_stats['statistics']['total_calls'] - initial_stats['statistics']['total_calls']
    print(f"\nðŸ› ï¸  æ‰§è¡ŒæœŸé—´ä»£ç å·¥å…·è°ƒç”¨ç»Ÿè®¡:")
    print(f"  æ€»è°ƒç”¨æ¬¡æ•°: {tool_calls_made}")
    print(f"  å¹³å‡è€—æ—¶: {final_stats['average_call_time']:.3f}ç§’")
    print(f"  æˆåŠŸçŽ‡: {final_stats['success_rate']:.1f}%")

    # æ‰“å°è¯¦ç»†çš„å·¥å…·è°ƒç”¨æ±‡æ€»
    code_tools.print_tool_call_summary()

    # æ±‡æ€»æ‰€æœ‰å‘çŽ°çš„é—®é¢˜
    all_findings = []
    total_findings = 0
    critical_findings = 0
    high_findings = 0
    medium_findings = 0
    low_findings = 0

    for result in all_results:
        if result.get("success"):
            findings = result.get("findings", [])
            all_findings.extend(findings)
            total_findings += len(findings)

            # ç»Ÿè®¡å„ä¸¥é‡çº§åˆ«é—®é¢˜æ•°é‡
            for finding in findings:
                severity = finding.get("severity", "").lower()
                if severity == "critical":
                    critical_findings += 1
                elif severity == "high":
                    high_findings += 1
                elif severity == "medium":
                    medium_findings += 1
                elif severity == "low":
                    low_findings += 1

    execution_summary = {
        "execution_start_time": start_time.isoformat(),
        "execution_end_time": end_time.isoformat(),
        "total_duration_minutes": total_duration,
        "total_tasks": len(audit_tasks),
        "completed_tasks": completed_tasks,
        "failed_tasks": failed_tasks,
        "success_rate": (completed_tasks / len(audit_tasks)) * 100 if audit_tasks else 0,
        "total_findings": total_findings,
        "findings_by_severity": {
            "critical": critical_findings,
            "high": high_findings,
            "medium": medium_findings,
            "low": low_findings
        },
        "parallel_execution": max_workers > 1,
        "max_workers_used": max_workers
    }

    return {
        "success": True,
        "execution_summary": execution_summary,
        "all_findings": all_findings,
        "execution_results": all_results,
        "audit_summary": {
            "tasks_completed": completed_tasks,
            "tasks_failed": failed_tasks,
            "security_issues_found": total_findings,
            "critical_issues": critical_findings,
            "high_issues": high_findings,
            "overall_risk_level": "high" if critical_findings > 0 else "medium" if high_findings > 0 else "low"
        }
    }