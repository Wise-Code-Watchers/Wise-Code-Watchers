"""
Security Issue 校验器 - 防止误报的硬性过滤器
基于证据链完整性校验，只让符合严格条件的issue通过
"""

import re
from dataclasses import dataclass
from typing import Any, Dict, List, Tuple, Optional

ALLOWED_ENTRYPOINT_KINDS = {"HTTP", "RPC", "MQ", "ENV", "FILE_UPLOAD", "CRON", "CLI"}

ALLOWED_CATEGORIES = {
    "SQLI", "RCE", "SSRF", "XSS", "IDOR", "PATH_TRAVERSAL", "DESERIALIZATION",
    "AUTH_BYPASS", "CSRF", "OPEN_REDIRECT", "INSECURE_CRYPTO", "SECRETS_EXPOSURE"
}

LOW_QUALITY_WHY_PATTERNS = [
    r"\b可能\b", r"\b疑似\b", r"\b也许\b", r"\b建议关注\b", r"\b存在风险\b",
    r"\bpotential\b", r"\bmaybe\b", r"\bpossible\b", r"\bcould\b", r"\bmight\b"
]

def _norm_text(s: str) -> str:
    """标准化文本：去空白、统一大小写"""
    s = s or ""
    s = re.sub(r"\s+", " ", s).strip().lower()
    return s

def _snippet_in_hunk(snippet: str, hunk_text: str) -> bool:
    """检查snippet是否在hunk中能匹配到（宽松匹配）"""
    sn = _norm_text(snippet)
    ht = _norm_text(hunk_text)
    if not sn or not ht:
        return False

    # 直接包含
    if sn in ht:
        return True

    # 按token overlap匹配
    sn_tokens = set(sn.split())
    ht_tokens = set(ht.split())
    if len(sn_tokens) >= 6:
        overlap = len(sn_tokens & ht_tokens) / max(1, len(sn_tokens))
        return overlap >= 0.6

    return False

def _find_hunk_in_diff_ir(diff_ir: Dict[str, Any], file_path: str,
                          hunk_id: Optional[str] = None,
                          new_start: Optional[int] = None) -> Optional[Dict[str, Any]]:
    """在diff_ir中查找指定的hunk"""
    for file in diff_ir.get("files", []):
        if file.get("file_path") != file_path:
            continue

        for hunk in file.get("hunks", []):
            current_hunk_id = f"{diff_ir.get('files', []).index(file)}:{file.get('hunks', []).index(hunk)}"

            # 优先使用hunk_id匹配
            if hunk_id and current_hunk_id == hunk_id:
                return hunk

            # 兜底：使用new_start匹配
            if new_start is not None and hunk.get("new_start") == new_start:
                return hunk

    return None

def _find_hunk_in_hunk_index(hunk_index: Dict[str, Dict[str, Any]],
                            file_path: str,
                            hunk_id: Optional[str] = None,
                            new_start: Optional[int] = None) -> Optional[Dict[str, Any]]:
    """在hunk_index中查找指定的hunk"""
    if hunk_id and hunk_id in hunk_index:
        hunk = hunk_index[hunk_id]
        if hunk.get("file_path") == file_path:
            return hunk

    # 如果hunk_id没找到，按文件路径和new_start查找
    for hid, hunk in hunk_index.items():
        if hunk.get("file_path") == file_path:
            hunk_data = hunk.get("hunk", {})
            if new_start is not None and hunk_data.get("new_start") == new_start:
                return hunk

    return None

def _validate_entrypoint(entrypoint: Dict[str, Any]) -> List[str]:
    """校验entrypoint字段"""
    reasons = []

    if not isinstance(entrypoint, dict):
        reasons.append("entrypoint missing or not a dict")
        return reasons

    kind = entrypoint.get("kind")
    if kind not in ALLOWED_ENTRYPOINT_KINDS:
        reasons.append(f"entrypoint.kind invalid: {kind}")

    file_path = entrypoint.get("file_path")
    symbol = entrypoint.get("symbol")
    if not file_path or not symbol:
        reasons.append("entrypoint.file_path or entrypoint.symbol missing")

    # 可选但推荐：route_or_topic 或 param 至少一个
    if not (entrypoint.get("route_or_topic") or entrypoint.get("param")):
        reasons.append("entrypoint.route_or_topic/param missing (need at least one)")

    return reasons

def _validate_call_chain(call_chain: List[Dict[str, Any]]) -> List[str]:
    """校验call_chain字段"""
    reasons = []

    if not isinstance(call_chain, list):
        reasons.append("call_chain missing or not a list")
        return reasons

    if len(call_chain) < 2:
        reasons.append("call_chain too short (<2)")
        return reasons

    for i, segment in enumerate(call_chain):
        if not isinstance(segment, dict):
            reasons.append(f"call_chain[{i}] not a dict")
            continue

        if not segment.get("file_path") or not segment.get("symbol"):
            reasons.append(f"call_chain[{i}] file_path/symbol missing")

        line_range = segment.get("line_range")
        if line_range is not None:
            if not isinstance(line_range, (list, tuple)) or len(line_range) != 2:
                reasons.append(f"call_chain[{i}] line_range invalid")

    return reasons

def _validate_impact(category: str, impact: Dict[str, Any]) -> List[str]:
    """校验impact字段"""
    reasons = []

    if category not in ALLOWED_CATEGORIES:
        reasons.append(f"category invalid: {category}")

    if not isinstance(impact, dict):
        reasons.append("impact missing or not a dict")
        return reasons

    what = (impact.get("what") or "").strip()
    why = (impact.get("why") or "").strip()

    if not what:
        reasons.append("impact.what missing")

    if not why or len(why) < 20:
        reasons.append("impact.why missing or too short (<20 chars)")
    else:
        # 检查低质量表述
        for pat in LOW_QUALITY_WHY_PATTERNS:
            if re.search(pat, why, flags=re.IGNORECASE):
                reasons.append("impact.why looks low-quality/hedged")
                break

    return reasons

def _validate_diff_linking(issue: Dict[str, Any], hunk_text: str) -> List[str]:
    """校验diff链接字段"""
    reasons = []

    anchor = issue.get("diff_anchor") or {}
    if not isinstance(anchor, dict):
        reasons.append("diff_anchor missing or not a dict")
        return reasons

    file_path = anchor.get("file_path")
    hunk_id = anchor.get("hunk_id")
    new_start = anchor.get("new_start")

    if not file_path:
        reasons.append("diff_anchor.file_path missing")

    if not (hunk_id or new_start is not None):
        reasons.append("diff_anchor.hunk_id or new_start missing")

    # 检查evidence_snippets
    evidence_snippets = issue.get("evidence_snippets") or []
    if not isinstance(evidence_snippets, list):
        reasons.append("evidence_snippets missing or not a list")
        return reasons

    diff_evidences = [e for e in evidence_snippets if e.get("source") == "diff"]
    if not diff_evidences:
        reasons.append("no evidence_snippets from diff")

    # 如果有hunk文本，检查diff evidence是否能匹配
    if hunk_text and diff_evidences:
        matched = any(_snippet_in_hunk(e.get("text", ""), hunk_text) for e in diff_evidences)
        if not matched:
            reasons.append("diff evidence snippet cannot be found/matched in target hunk")

    return reasons

def normalize_issue_format(issue: Dict[str, Any]) -> Dict[str, Any]:
    """将不同Agent输出的issue格式标准化为统一schema"""
    normalized = {
        "type": "SECURITY",
        "category": None,
        "severity": "MEDIUM",
        "entrypoint": {},
        "call_chain": [],
        "diff_anchor": {},
        "evidence_snippets": [],
        "impact": {},
        "original_issue": issue  # 保留原始数据用于调试
    }

    # 从原始issue中提取和映射字段
    if isinstance(issue, dict):
        # 尝试从各种可能的字段名中提取信息
        normalized["category"] = (
            issue.get("category") or
            issue.get("risk_type") or
            issue.get("vulnerability_type") or
            "UNKNOWN"
        ).upper()

        normalized["severity"] = (
            issue.get("severity") or
            issue.get("risk_level") or
            "MEDIUM"
        ).upper()

        # 尝试提取entrypoint信息
        entrypoint_data = (
            issue.get("entrypoint") or
            issue.get("attack_surface") or
            {}
        )
        if isinstance(entrypoint_data, dict):
            normalized["entrypoint"] = entrypoint_data

        # 尝试提取call chain信息
        call_chain_data = (
            issue.get("call_chain") or
            issue.get("data_flow") or
            issue.get("call_graph") or
            []
        )
        if isinstance(call_chain_data, list):
            normalized["call_chain"] = call_chain_data

        # 尝试提取impact信息
        impact_data = (
            issue.get("impact") or
            issue.get("analysis", {}).get("impact") or
            {}
        )
        if isinstance(impact_data, dict):
            normalized["impact"] = impact_data

        # 尝试提取diff anchor信息
        if "hunk_id" in issue:
            normalized["diff_anchor"]["hunk_id"] = issue["hunk_id"]
        if "file_path" in issue:
            normalized["diff_anchor"]["file_path"] = issue["file_path"]

        # 尝试提取evidence snippets
        evidence_data = (
            issue.get("evidence_snippets") or
            issue.get("evidence") or
            []
        )
        if isinstance(evidence_data, list):
            normalized["evidence_snippets"] = evidence_data

    return normalized

def validate_security_issue(issue: Dict[str, Any],
                          diff_ir: Optional[Dict[str, Any]] = None,
                          hunk_index: Optional[Dict[str, Dict[str, Any]]] = None) -> Tuple[bool, Dict[str, Any], List[str]]:
    """
    校验安全issue的证据链完整性

    Args:
        issue: Agent输出的安全issue
        diff_ir: diff数据结构（可选）
        hunk_index: hunk索引映射（可选，推荐使用）

    Returns:
        Tuple[is_valid, normalized_issue, reasons]
    """
    reasons = []

    # 标准化issue格式
    normalized = normalize_issue_format(issue)

    # A) 校验entrypoint
    entrypoint_reasons = _validate_entrypoint(normalized["entrypoint"])
    reasons.extend(entrypoint_reasons)

    # B) 校验call_chain
    call_chain_reasons = _validate_call_chain(normalized["call_chain"])
    reasons.extend(call_chain_reasons)

    # C) 校验impact
    impact_reasons = _validate_impact(normalized["category"], normalized["impact"])
    reasons.extend(impact_reasons)

    # D) 校验diff链接
    hunk_text = ""

    # 优先使用hunk_index查找hunk文本
    if hunk_index:
        anchor = normalized["diff_anchor"]
        hunk_id = anchor.get("hunk_id")
        file_path = anchor.get("file_path")
        if hunk_id and file_path:
            hunk_detail = hunk_index.get(hunk_id)
            if hunk_detail and hunk_detail.get("file_path") == file_path:
                hunk_obj = hunk_detail.get("hunk", {})
                hunk_text = "\n".join([
                    line.get("content", "") for line in hunk_obj.get("lines", [])
                    if line.get("type") in ("add", "context")
                ])

    # 兜底：使用diff_ir查找
    elif diff_ir:
        anchor = normalized["diff_anchor"]
        hunk = _find_hunk_in_diff_ir(
            diff_ir,
            anchor.get("file_path"),
            anchor.get("hunk_id"),
            anchor.get("new_start")
        )
        if hunk:
            hunk_text = "\n".join([
                line.get("content", "") for line in hunk.get("lines", [])
                if line.get("type") in ("add", "context")
            ])

    diff_reasons = _validate_diff_linking(normalized, hunk_text)
    reasons.extend(diff_reasons)

    # 最终判定
    is_valid = len(reasons) == 0

    if not is_valid:
        # 强制降级
        normalized["final_verdict"] = "NO_ISSUE"
        normalized["dropped_by_validator"] = True
        normalized["validator_reasons"] = reasons
        normalized["validation_status"] = "REJECTED"
    else:
        normalized["final_verdict"] = "ISSUE"
        normalized["validation_status"] = "PASSED"

    return is_valid, normalized, reasons

def validate_security_issues_batch(issues: List[Dict[str, Any]],
                                 diff_ir: Optional[Dict[str, Any]] = None,
                                 hunk_index: Optional[Dict[str, Dict[str, Any]]] = None) -> Dict[str, Any]:
    """
    批量校验安全issues

    Returns:
        Dict with:
        - valid_issues: 通过校验的issues
        - rejected_issues: 被拒绝的issues
        - validation_summary: 校验汇总统计
    """
    valid_issues = []
    rejected_issues = []
    rejection_reasons_count = {}

    for i, issue in enumerate(issues):
        is_valid, normalized, reasons = validate_security_issue(
            issue, diff_ir, hunk_index
        )

        # 统计拒绝原因
        if not is_valid:
            rejected_issues.append({
                "index": i,
                "original_issue": issue,
                "normalized_issue": normalized,
                "reasons": reasons
            })

            # 统计各类拒绝原因
            for reason in reasons:
                rejection_reasons_count[reason] = rejection_reasons_count.get(reason, 0) + 1
        else:
            valid_issues.append(normalized)

    return {
        "valid_issues": valid_issues,
        "rejected_issues": rejected_issues,
        "validation_summary": {
            "total_input": len(issues),
            "valid_count": len(valid_issues),
            "rejected_count": len(rejected_issues),
            "rejection_rate": len(rejected_issues) / max(1, len(issues)) * 100,
            "rejection_reasons_count": rejection_reasons_count
        }
    }