"""
Logic Analysis Agent - Detects logic errors with risk-based prioritization.

Supports two modes:
- Legacy mode: Uses feature_points + changed_files (original behavior)
- Enhanced mode: Uses diff_ir + feature_risk_plan (risk-based analysis)
"""

import asyncio
import json
import logging
import os
import re
from typing import Optional

from langchain_openai import ChatOpenAI

from src.agents.base import BaseAgent, AgentResult
from src.config import Config
from src.knowledge.vulnerability_kb import VulnerabilityKB
from src.knowledge.best_practices_kb import BestPracticesKB
from src.tools.static_analyzer import StaticAnalyzerTool
from src.output.models import Bug, BugType, Severity, LogicAnalysis, FeaturePoint

logger = logging.getLogger(__name__)


class LogicAnalysisAgent(BaseAgent):
    """Logic analysis agent with dual-mode support (legacy + enhanced)."""

    name = "logic_analysis_agent"
    description = "Detects logic errors, edge cases, and algorithmic bugs with risk-based prioritization"

    def __init__(
        self,
        llm: Optional[ChatOpenAI] = None,
        vulnerability_kb: Optional[VulnerabilityKB] = None,
        best_practices_kb: Optional[BestPracticesKB] = None,
        static_analyzer: Optional[StaticAnalyzerTool] = None,
        verbose: bool = False,
        risk_threshold: Optional[int] = None,
        max_units: Optional[int] = None,
    ):
        super().__init__(llm, verbose)
        self.vulnerability_kb = vulnerability_kb or VulnerabilityKB()
        self.best_practices_kb = best_practices_kb or BestPracticesKB()
        self.static_analyzer = static_analyzer or StaticAnalyzerTool()
        self.add_knowledge_base(self.vulnerability_kb)
        self.add_knowledge_base(self.best_practices_kb)
        self.add_tool(self.static_analyzer)

        self.risk_threshold = risk_threshold or Config.VULN_RISK_THRESHOLD_LOGIC
        self.max_units = max_units or Config.VULN_MAX_UNITS_LOGIC

        self.prompt = self._create_prompt(
            system_message="""You are a logic analysis expert. Analyze the provided code for:
1. Logic errors and bugs (off-by-one, null pointer, type errors)
2. Edge cases not handled (empty inputs, boundary conditions)
3. Race conditions and concurrency issues
4. Error handling gaps
5. Algorithm correctness issues

Focus on the feature points provided and analyze each one separately.

Respond in JSON format:
{{
    "issues": [
        {{
            "title": "short title",
            "description": "detailed description with explanation",
            "file": "filename",
            "line": line_number,
            "severity": "low|medium|high|critical",
            "feature_point_id": "id of related feature point",
            "suggestion": "how to fix the issue"
        }}
    ],
    "summary": "overall assessment",
    "edge_cases": ["list of unhandled edge cases found"]
}}""",
            human_message="""Analyze the following code for logic errors:

Feature Points to analyze:
{feature_points}

Code for each feature point:
{code_snippets}

Static analysis results:
{static_results}

Knowledge base references:
{kb_references}

Provide your logic analysis.""",
        )

    async def analyze(
        self,
        codebase_path: str,
        feature_points: list[FeaturePoint] = None,
        changed_files: list[str] = None,
        diff_ir: dict = None,
        feature_risk_plan: dict = None,
        pr_dir: str = None,
        **kwargs,
    ) -> AgentResult:
        """
        Unified analyze method supporting both legacy and enhanced modes.

        Args:
            codebase_path: Path to the codebase
            feature_points: Feature points for legacy mode
            changed_files: Changed files for legacy mode
            diff_ir: Diff IR for enhanced mode
            feature_risk_plan: Risk plan for enhanced mode
            pr_dir: PR directory for enhanced mode

        Returns:
            AgentResult with analysis results
        """
        try:
            if diff_ir and feature_risk_plan and pr_dir:
                return await self._run_enhanced_analysis(
                    pr_dir=pr_dir,
                    diff_ir=diff_ir,
                    feature_risk_plan=feature_risk_plan,
                )

            return await self._run_legacy_analysis(
                codebase_path=codebase_path,
                feature_points=feature_points,
                changed_files=changed_files,
            )

        except Exception as e:
            logger.error(f"[{self.name}] Analysis failed: {e}")
            return AgentResult(
                agent_name=self.name,
                success=False,
                error=str(e),
            )

    async def _run_enhanced_analysis(
        self,
        pr_dir: str,
        diff_ir: dict,
        feature_risk_plan: dict,
    ) -> AgentResult:
        """Risk-based logic analysis using enhanced engine."""
        logger.info(f"[{self.name}] Running enhanced analysis with risk threshold {self.risk_threshold}")

        try:
            from src.agents.vulnerability.src.analysis.hunk_index import (
                build_hunk_index,
                select_logic_targets,
                build_audit_unit_from_hunk,
            )
            from src.agents.vulnerability.src.prompts.prompt import (
                LOGIC_AGENT_SYSTEM,
                format_logic_agent_prompt,
            )
            from src.agents.vulnerability.src.scripts.todolist.todolist_generator import (
                create_code_tools_for_pr,
            )
        except ImportError as e:
            logger.warning(f"[{self.name}] Enhanced engine not available: {e}, falling back to legacy")
            return await self._run_legacy_analysis(
                codebase_path=pr_dir,
                feature_points=None,
                changed_files=None,
            )

        hunk_index = build_hunk_index(diff_ir)
        logger.info(f"[{self.name}] Built hunk index with {len(hunk_index)} entries")

        targets = select_logic_targets(
            feature_risk_plan=feature_risk_plan,
            risk_threshold=self.risk_threshold,
            max_units=self.max_units,
        )

        if not targets:
            logger.info(f"[{self.name}] No high-risk logic targets found")
            return AgentResult(
                agent_name=self.name,
                success=True,
                issues=[],
                summary="No high-risk logic targets found for analysis",
                metrics={"units_analyzed": 0, "targets_selected": 0},
            )

        logger.info(f"[{self.name}] Selected {len(targets)} targets for analysis")

        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            self._analyze_targets_sync,
            targets,
            hunk_index,
            pr_dir,
            feature_risk_plan,
            LOGIC_AGENT_SYSTEM,
            format_logic_agent_prompt,
            create_code_tools_for_pr,
        )

        issues = self._convert_enhanced_issues(result.get("issues", []))

        return AgentResult(
            agent_name=self.name,
            success=result.get("success", True),
            issues=[self._bug_to_dict(b) for b in issues],
            summary=f"Analyzed {result.get('units_analyzed', 0)} units, found {len(issues)} logic issues",
            metrics={
                "units_analyzed": result.get("units_analyzed", 0),
                "issues_found": len(issues),
                "errors": result.get("errors", []),
                "edge_cases": [],
            },
        )

    def _analyze_targets_sync(
        self,
        targets: list,
        hunk_index: dict,
        pr_dir: str,
        feature_risk_plan: dict,
        system_prompt: str,
        format_prompt_fn,
        create_code_tools_fn,
    ) -> dict:
        """Synchronous target analysis (runs in executor)."""
        from langchain_core.messages import SystemMessage, HumanMessage

        tools_bundle = create_code_tools_fn(pr_dir, feature_risk_plan)
        code_tools = tools_bundle["code_tools"]

        audit_units = []
        for target_info in targets:
            hunk_id = target_info.get("hunk_id")
            if not hunk_id:
                continue

            hunk_detail = hunk_index.get(hunk_id)
            if not hunk_detail:
                continue

            try:
                from src.agents.vulnerability.src.analysis.hunk_index import build_audit_unit_from_hunk
                audit_unit = build_audit_unit_from_hunk(
                    hunk_id=hunk_id,
                    hunk=hunk_detail,
                    code_tools=code_tools,
                    target_info=target_info,
                )
                audit_units.append(audit_unit)
            except Exception as e:
                logger.warning(f"[{self.name}] Failed to build audit unit for {hunk_id}: {e}")

        if not audit_units:
            return {"success": True, "units_analyzed": 0, "issues": [], "errors": []}

        results = []
        errors = []

        for unit in audit_units:
            hunk_id = unit.get("hunk_id", "unknown")
            file_path = unit.get("file_path", "unknown")
            risk_score = unit.get("risk_score", 0)

            try:
                prompt = format_prompt_fn(unit)
                messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=prompt),
                ]

                resp = self.llm.invoke(messages)
                text = resp.content if hasattr(resp, "content") else str(resp)

                obj = self._safe_json_loads(text)

                if obj.get("result") not in ("NO_ISSUE", "ISSUE"):
                    obj = {"result": "NO_ISSUE", "issue": None}

                obj["_meta"] = {
                    "hunk_id": hunk_id,
                    "file_path": file_path,
                    "risk_score": risk_score,
                }
                results.append(obj)

            except Exception as e:
                errors.append(f"hunk_id={hunk_id} error={str(e)}")
                logger.error(f"[{self.name}] Analysis failed for {hunk_id}: {e}")

        issues = [r for r in results if r.get("result") == "ISSUE"]

        return {
            "success": True,
            "units_analyzed": len(audit_units),
            "issues_found": len(issues),
            "issues": issues,
            "raw_results": results,
            "errors": errors,
        }

    def _safe_json_loads(self, text: str) -> dict:
        """Extract JSON from potentially messy LLM output."""
        text = text.strip()
        if text.startswith("{") and text.endswith("}"):
            return json.loads(text)
        left = text.find("{")
        right = text.rfind("}")
        if left != -1 and right != -1 and right > left:
            return json.loads(text[left:right + 1])
        raise json.JSONDecodeError("no json object", text, 0)

    def _convert_enhanced_issues(self, issues: list) -> list[Bug]:
        """Convert enhanced analysis issues to Bug objects."""
        bugs = []
        for issue_data in issues:
            issue = issue_data.get("issue", {})
            meta = issue_data.get("_meta", {})

            if not issue:
                continue

            bugs.append(Bug(
                id=f"logic-{len(bugs)}",
                type=BugType.LOGIC_ERROR,
                severity=self._map_severity(issue.get("severity", "medium")),
                title=issue.get("title", "Logic issue"),
                description=issue.get("description", issue.get("rationale", "")),
                file=meta.get("file_path", issue.get("file", "")),
                line=issue.get("line", 0),
                suggestion=issue.get("suggestion", issue.get("fix_suggestion", "")),
                feature_point_id=meta.get("hunk_id"),
            ))

        return bugs

    async def _run_legacy_analysis(
        self,
        codebase_path: str,
        feature_points: list[FeaturePoint] = None,
        changed_files: list[str] = None,
    ) -> AgentResult:
        """Original analysis logic for backward compatibility."""
        try:
            if changed_files:
                static_issues = []
                for f in changed_files:
                    full_path = os.path.join(codebase_path, f)
                    if os.path.exists(full_path):
                        result = await self.static_analyzer.run(full_path)
                        if result.success:
                            static_issues.extend(result.issues)
            else:
                static_result = await self.static_analyzer.run(codebase_path)
                static_issues = static_result.issues if static_result.success else []

            kb_refs = self._query_knowledge_bases("logic error edge case null boundary")

            fp_info = self._format_feature_points(feature_points or [])
            code_snippets = self._extract_feature_code(codebase_path, feature_points or [])

            chain = self.prompt | self.llm
            response = await chain.ainvoke({
                "feature_points": fp_info,
                "code_snippets": code_snippets,
                "static_results": json.dumps(static_issues[:15], indent=2),
                "kb_references": json.dumps(kb_refs[:5], indent=2),
            })

            result = self._parse_response(response.content, static_issues)

            return AgentResult(
                agent_name=self.name,
                success=True,
                issues=[self._bug_to_dict(bug) for bug in result.issues],
                summary=result.summary,
                metrics={"edge_cases": result.edge_cases},
            )

        except Exception as e:
            return AgentResult(
                agent_name=self.name,
                success=False,
                error=str(e),
            )

    def _format_feature_points(self, feature_points: list[FeaturePoint]) -> str:
        if not feature_points:
            return "No specific feature points provided"

        lines = []
        for fp in feature_points:
            lines.append(f"- [{fp.id}] {fp.name}: {fp.description}")
            lines.append(f"  Files: {', '.join(fp.files)}")
        return "\n".join(lines)

    def _extract_feature_code(self, codebase_path: str, feature_points: list[FeaturePoint]) -> str:
        snippets = []

        if not feature_points:
            return self._extract_general_code(codebase_path)

        for fp in feature_points:
            for filepath in fp.files[:3]:
                full_path = os.path.join(codebase_path, filepath)
                if os.path.exists(full_path):
                    try:
                        with open(full_path, "r") as f:
                            content = f.read()
                            lines = content.split("\n")[:100]
                            snippets.append(f"=== [{fp.id}] {filepath} ===\n" + "\n".join(lines))
                    except Exception:
                        pass

        return "\n\n".join(snippets[:5]) or "No code extracted"

    def _extract_general_code(self, codebase_path: str) -> str:
        snippets = []
        extensions = {".py", ".js", ".ts"}

        for root, _, files in os.walk(codebase_path):
            for file in files:
                if any(file.endswith(ext) for ext in extensions):
                    filepath = os.path.join(root, file)
                    try:
                        with open(filepath, "r") as f:
                            content = f.read()
                            lines = content.split("\n")[:50]
                            snippets.append(f"=== {filepath} ===\n" + "\n".join(lines))
                            if len(snippets) >= 3:
                                break
                    except Exception:
                        pass
            if len(snippets) >= 3:
                break

        return "\n\n".join(snippets) or "No code found"

    def _parse_response(self, content: str, static_issues: list[dict]) -> LogicAnalysis:
        issues = []
        edge_cases = []

        for si in static_issues:
            if si.get("severity") in ["error", "high", "critical"]:
                issues.append(Bug(
                    id=f"static-{si.get('line', 0)}",
                    type=BugType.LOGIC_ERROR,
                    severity=self._map_severity(si.get("severity", "medium")),
                    title=si.get("rule", "Static analysis issue"),
                    description=si.get("message", ""),
                    file=si.get("file", ""),
                    line=si.get("line", 0),
                ))

        try:
            json_match = re.search(r"\{.*\}", content, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
            else:
                result = json.loads(content)

            for issue_data in result.get("issues", []):
                issues.append(Bug(
                    id=f"logic-{len(issues)}",
                    type=BugType.LOGIC_ERROR,
                    severity=self._map_severity(issue_data.get("severity", "medium")),
                    title=issue_data.get("title", ""),
                    description=issue_data.get("description", ""),
                    file=issue_data.get("file", ""),
                    line=issue_data.get("line", 0),
                    suggestion=issue_data.get("suggestion"),
                    feature_point_id=issue_data.get("feature_point_id"),
                ))

            edge_cases = result.get("edge_cases", [])

            return LogicAnalysis(
                issues=issues,
                summary=result.get("summary", ""),
                edge_cases=edge_cases,
            )

        except json.JSONDecodeError:
            return LogicAnalysis(
                issues=issues,
                summary="Analysis completed with static analysis results",
                edge_cases=[],
            )

    def _map_severity(self, severity: str) -> Severity:
        mapping = {
            "low": Severity.LOW,
            "medium": Severity.MEDIUM,
            "high": Severity.HIGH,
            "critical": Severity.CRITICAL,
            "error": Severity.HIGH,
            "warning": Severity.MEDIUM,
        }
        return mapping.get(severity.lower(), Severity.MEDIUM)

    def _bug_to_dict(self, bug: Bug) -> dict:
        return {
            "id": bug.id,
            "type": bug.type.value,
            "severity": bug.severity.value,
            "title": bug.title,
            "description": bug.description,
            "file": bug.file,
            "line": bug.line,
            "suggestion": bug.suggestion,
            "feature_point_id": bug.feature_point_id,
        }
